{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d568f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../cyx＆xy/bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "f=open('../cyx＆xy/bert/THUCNews/data/all_data_with_label1.txt','r',encoding='utf-8')\n",
    "data=f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2790585",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,test_set=train_test_split(data,test_size=0.4)\n",
    "dev_set,test_set=train_test_split(test_set,test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('../cyx＆xy/bert/THUCNews/data/train.txt','w',encoding='utf-8')\n",
    "for i in train_set:\n",
    "    f.write(i)\n",
    "f.close()\n",
    "\n",
    "f=open('../cyx＆xy/bert/THUCNews/data/dev.txt','w',encoding='utf-8')\n",
    "for i in dev_set:\n",
    "    f.write(i)\n",
    "f.close()\n",
    "\n",
    "f=open('../cyx＆xy/bert/THUCNews/data/test.txt','w',encoding='utf-8')\n",
    "for i in test_set:\n",
    "    f.write(i)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3519f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from pytorch_pretrained_bert import BertModel, BertTokenizer\n",
    "from pytorch_pretrained import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "\n",
    "    \"\"\"配置参数\"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.model_name = 'bert'\n",
    "        self.train_path = dataset + '/data/train.txt'                                # 训练集\n",
    "        self.dev_path = dataset + '/data/dev.txt'                                    # 验证集\n",
    "        self.test_path = dataset + '/data/test.txt'                                  # 测试集\n",
    "        self.class_list = [x.strip() for x in open(\n",
    "            dataset + '/data/class_my.txt').readlines()]                                # 类别名单\n",
    "        self.save_path = dataset + '/saved_dict/' + self.model_name + '_without_my.ckpt'        # 模型训练结果\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n",
    "\n",
    "        self.require_improvement = 10000                                 # 若超过1000batch效果还没提升，则提前结束训练\n",
    "        self.num_classes = len(self.class_list)                         # 类别数\n",
    "        #self.num_epochs = 3                                             # epoch数\n",
    "        self.num_epochs = 30\n",
    "        #self.batch_size = 128\n",
    "        self.batch_size = 8                                           # mini-batch大小\n",
    "        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)\n",
    "        self.learning_rate = 4e-6                                       # 学习率\n",
    "        self.bert_path = 'bert-base-chinese'\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n",
    "        self.hidden_size = 768\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = x[0]  # 输入的句子\n",
    "        mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n",
    "        _, pooled = self.bert(context, attention_mask=mask, output_all_encoded_layers=False)\n",
    "        out = self.fc(pooled)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3da643ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from utils import get_time_dif\n",
    "from pytorch_pretrained.optimization import BertAdam\n",
    "\n",
    "\n",
    "# 权重初始化，默认xavier\n",
    "def init_network(model, method='xavier', exclude='embedding', seed=123):\n",
    "    for name, w in model.named_parameters():\n",
    "        if exclude not in name:\n",
    "            if len(w.size()) < 2:\n",
    "                continue\n",
    "            if 'weight' in name:\n",
    "                if method == 'xavier':\n",
    "                    nn.init.xavier_normal_(w)\n",
    "                elif method == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(w)\n",
    "                else:\n",
    "                    nn.init.normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(w, 0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "\n",
    "def train(config, model, train_iter, dev_iter, test_iter):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=config.learning_rate,\n",
    "                         warmup=0.05,\n",
    "                         t_total=len(train_iter) * config.num_epochs)\n",
    "    total_batch = 0  # 记录进行到多少batch\n",
    "    dev_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    flag = False  # 记录是否很久没有效果提升\n",
    "    model.train()\n",
    "    train_log=[]\n",
    "    dev_log=[]\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        loss_temp = 0\n",
    "        dev_temp = []\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n",
    "        for i, (trains, labels) in enumerate(train_iter):\n",
    "            outputs = model(trains)\n",
    "            model.zero_grad()\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss_temp += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #if total_batch % 100 == 0:\n",
    "            if total_batch % 10 == 0:\n",
    "                # 每多少轮输出在训练集和验证集上的效果\n",
    "                true = labels.data.cpu()\n",
    "                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n",
    "                if dev_loss < dev_best_loss:\n",
    "                    dev_best_loss = dev_loss\n",
    "                    torch.save(model.state_dict(), config.save_path)\n",
    "                    improve = '*'\n",
    "                    last_improve = total_batch\n",
    "                else:\n",
    "                    improve = ''\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n",
    "                dev_temp.append(dev_loss.item())\n",
    "                model.train()\n",
    "            total_batch += 1\n",
    "            if total_batch - last_improve > config.require_improvement:\n",
    "                # 验证集loss超过1000batch没下降，结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break\n",
    "        print('--------------')\n",
    "        print(dev_temp)   \n",
    "        dev_acc, dev_loss = evaluate(config, model, dev_iter)\n",
    "        dev_log.append(dev_temp)\n",
    "        train_log.append(loss_temp)\n",
    "        if flag:\n",
    "            break\n",
    "    test(config, model, test_iter)\n",
    "    return dev_log,train_log\n",
    "\n",
    "\n",
    "def test(config, model, test_iter):\n",
    "    # test\n",
    "    model.load_state_dict(torch.load(config.save_path))\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    test_acc, test_loss, test_report, test_confusion = evaluate(config, model, test_iter, test=True)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n",
    "    print(msg.format(test_loss, test_acc))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)\n",
    "    print(\"Confusion Matrix...\")\n",
    "    print(test_confusion)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "\n",
    "def evaluate(config, model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_iter:\n",
    "            outputs = model(texts)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=4)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "    return acc, loss_total / len(data_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "099b86c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "432it [00:00, 22231.29it/s]\n",
      "173it [00:00, 22192.08it/s]\n",
      "116it [00:00, 21722.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time usage: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]\n",
      "Iter:      0,  Train Loss:   2.0,  Train Acc: 25.00%,  Val Loss:   1.9,  Val Acc: 18.50%,  Time: 0:00:01 *\n",
      "Iter:     10,  Train Loss:   1.8,  Train Acc: 25.00%,  Val Loss:   1.8,  Val Acc: 19.65%,  Time: 0:00:03 *\n",
      "Iter:     20,  Train Loss:   1.6,  Train Acc: 25.00%,  Val Loss:   1.5,  Val Acc: 34.10%,  Time: 0:00:05 *\n",
      "Iter:     30,  Train Loss:   1.2,  Train Acc: 50.00%,  Val Loss:   1.4,  Val Acc: 49.13%,  Time: 0:00:07 *\n",
      "Iter:     40,  Train Loss:   1.3,  Train Acc: 62.50%,  Val Loss:   1.2,  Val Acc: 53.76%,  Time: 0:00:09 *\n",
      "Iter:     50,  Train Loss:   1.1,  Train Acc: 62.50%,  Val Loss:   1.1,  Val Acc: 61.27%,  Time: 0:00:11 *\n",
      "--------------\n",
      "[1.9003894329071045, 1.786452293395996, 1.5156646966934204, 1.3957135677337646, 1.223368525505066, 1.1003735065460205]\n",
      "Epoch [2/30]\n",
      "Iter:     60,  Train Loss:   0.9,  Train Acc: 50.00%,  Val Loss:  0.97,  Val Acc: 58.96%,  Time: 0:00:13 *\n",
      "Iter:     70,  Train Loss:  0.76,  Train Acc: 62.50%,  Val Loss:  0.87,  Val Acc: 64.74%,  Time: 0:00:15 *\n",
      "Iter:     80,  Train Loss:  0.85,  Train Acc: 62.50%,  Val Loss:  0.71,  Val Acc: 71.68%,  Time: 0:00:17 *\n",
      "Iter:     90,  Train Loss:  0.32,  Train Acc: 100.00%,  Val Loss:  0.62,  Val Acc: 78.03%,  Time: 0:00:18 *\n",
      "Iter:    100,  Train Loss:  0.35,  Train Acc: 87.50%,  Val Loss:  0.57,  Val Acc: 83.24%,  Time: 0:00:20 *\n",
      "--------------\n",
      "[0.9662672877311707, 0.8683924674987793, 0.7127153873443604, 0.6193370223045349, 0.5723140239715576]\n",
      "Epoch [3/30]\n",
      "Iter:    110,  Train Loss:  0.24,  Train Acc: 87.50%,  Val Loss:  0.63,  Val Acc: 73.41%,  Time: 0:00:22 \n",
      "Iter:    120,  Train Loss:  0.44,  Train Acc: 87.50%,  Val Loss:  0.53,  Val Acc: 80.35%,  Time: 0:00:24 *\n",
      "Iter:    130,  Train Loss:   0.8,  Train Acc: 75.00%,  Val Loss:  0.46,  Val Acc: 84.97%,  Time: 0:00:26 *\n",
      "Iter:    140,  Train Loss:  0.33,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 83.82%,  Time: 0:00:28 *\n",
      "Iter:    150,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 84.39%,  Time: 0:00:29 \n",
      "Iter:    160,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.42,  Val Acc: 83.82%,  Time: 0:00:31 *\n",
      "--------------\n",
      "[0.6266615390777588, 0.5312105417251587, 0.4628494679927826, 0.45678943395614624, 0.4600417912006378, 0.4243442118167877]\n",
      "Epoch [4/30]\n",
      "Iter:    170,  Train Loss:  0.16,  Train Acc: 87.50%,  Val Loss:  0.42,  Val Acc: 85.55%,  Time: 0:00:33 *\n",
      "Iter:    180,  Train Loss: 0.061,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 87.86%,  Time: 0:00:35 *\n",
      "Iter:    190,  Train Loss:  0.21,  Train Acc: 100.00%,  Val Loss:  0.35,  Val Acc: 86.71%,  Time: 0:00:37 *\n",
      "Iter:    200,  Train Loss:  0.16,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 85.55%,  Time: 0:00:38 \n",
      "Iter:    210,  Train Loss:  0.33,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 86.71%,  Time: 0:00:39 \n",
      "--------------\n",
      "[0.41549041867256165, 0.3683646619319916, 0.3532477915287018, 0.39769530296325684, 0.3730809688568115]\n",
      "Epoch [5/30]\n",
      "Iter:    220,  Train Loss:  0.06,  Train Acc: 100.00%,  Val Loss:  0.35,  Val Acc: 88.44%,  Time: 0:00:41 *\n",
      "Iter:    230,  Train Loss:  0.11,  Train Acc: 100.00%,  Val Loss:  0.31,  Val Acc: 89.02%,  Time: 0:00:43 *\n",
      "Iter:    240,  Train Loss:  0.19,  Train Acc: 87.50%,  Val Loss:  0.33,  Val Acc: 89.60%,  Time: 0:00:44 \n",
      "Iter:    250,  Train Loss:  0.07,  Train Acc: 100.00%,  Val Loss:  0.36,  Val Acc: 87.28%,  Time: 0:00:46 \n",
      "Iter:    260,  Train Loss:  0.11,  Train Acc: 100.00%,  Val Loss:  0.36,  Val Acc: 87.86%,  Time: 0:00:47 \n",
      "--------------\n",
      "[0.3474048972129822, 0.3088832497596741, 0.32750436663627625, 0.3630358874797821, 0.35786911845207214]\n",
      "Epoch [6/30]\n",
      "Iter:    270,  Train Loss: 0.055,  Train Acc: 100.00%,  Val Loss:  0.35,  Val Acc: 87.86%,  Time: 0:00:48 \n",
      "Iter:    280,  Train Loss:   0.3,  Train Acc: 87.50%,  Val Loss:  0.38,  Val Acc: 87.28%,  Time: 0:00:49 \n",
      "Iter:    290,  Train Loss:  0.05,  Train Acc: 100.00%,  Val Loss:  0.33,  Val Acc: 87.86%,  Time: 0:00:51 \n",
      "Iter:    300,  Train Loss: 0.044,  Train Acc: 100.00%,  Val Loss:  0.31,  Val Acc: 90.17%,  Time: 0:00:52 \n",
      "Iter:    310,  Train Loss: 0.066,  Train Acc: 100.00%,  Val Loss:  0.35,  Val Acc: 87.86%,  Time: 0:00:53 \n",
      "Iter:    320,  Train Loss: 0.056,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 87.28%,  Time: 0:00:54 \n",
      "--------------\n",
      "[0.3533978760242462, 0.3775250017642975, 0.32536765933036804, 0.3128631114959717, 0.34965619444847107, 0.3761068880558014]\n",
      "Epoch [7/30]\n",
      "Iter:    330,  Train Loss: 0.028,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 87.86%,  Time: 0:00:56 \n",
      "Iter:    340,  Train Loss: 0.039,  Train Acc: 100.00%,  Val Loss:  0.35,  Val Acc: 89.02%,  Time: 0:00:57 \n",
      "Iter:    350,  Train Loss: 0.042,  Train Acc: 100.00%,  Val Loss:  0.32,  Val Acc: 90.75%,  Time: 0:00:58 \n",
      "Iter:    360,  Train Loss:  0.03,  Train Acc: 100.00%,  Val Loss:  0.34,  Val Acc: 89.02%,  Time: 0:00:59 \n",
      "Iter:    370,  Train Loss: 0.021,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 86.71%,  Time: 0:01:00 \n",
      "--------------\n",
      "[0.41296347975730896, 0.3505270183086395, 0.3221608102321625, 0.34178662300109863, 0.3810366690158844]\n",
      "Epoch [8/30]\n",
      "Iter:    380,  Train Loss: 0.024,  Train Acc: 100.00%,  Val Loss:  0.36,  Val Acc: 90.17%,  Time: 0:01:02 \n",
      "Iter:    390,  Train Loss: 0.036,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 89.60%,  Time: 0:01:03 \n",
      "Iter:    400,  Train Loss:  0.24,  Train Acc: 75.00%,  Val Loss:  0.38,  Val Acc: 89.02%,  Time: 0:01:04 \n",
      "Iter:    410,  Train Loss: 0.071,  Train Acc: 100.00%,  Val Loss:  0.33,  Val Acc: 90.17%,  Time: 0:01:05 \n",
      "Iter:    420,  Train Loss: 0.037,  Train Acc: 100.00%,  Val Loss:  0.34,  Val Acc: 89.02%,  Time: 0:01:07 \n",
      "Iter:    430,  Train Loss: 0.039,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 87.86%,  Time: 0:01:08 \n",
      "--------------\n",
      "[0.35793402791023254, 0.36925825476646423, 0.3818279206752777, 0.3348841071128845, 0.3439326286315918, 0.38524264097213745]\n",
      "Epoch [9/30]\n",
      "Iter:    440,  Train Loss: 0.016,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 87.86%,  Time: 0:01:09 \n",
      "Iter:    450,  Train Loss: 0.016,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 89.60%,  Time: 0:01:10 \n",
      "Iter:    460,  Train Loss: 0.031,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 89.02%,  Time: 0:01:12 \n",
      "Iter:    470,  Train Loss: 0.023,  Train Acc: 100.00%,  Val Loss:  0.36,  Val Acc: 90.17%,  Time: 0:01:13 \n",
      "Iter:    480,  Train Loss: 0.099,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 89.02%,  Time: 0:01:14 \n",
      "--------------\n",
      "[0.3888656198978424, 0.36795100569725037, 0.36823222041130066, 0.35632458329200745, 0.36547526717185974]\n",
      "Epoch [10/30]\n",
      "Iter:    490,  Train Loss: 0.013,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 89.02%,  Time: 0:01:15 \n",
      "Iter:    500,  Train Loss: 0.055,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 89.60%,  Time: 0:01:17 \n",
      "Iter:    510,  Train Loss:  0.09,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.75%,  Time: 0:01:18 \n",
      "Iter:    520,  Train Loss: 0.019,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 91.33%,  Time: 0:01:19 \n",
      "Iter:    530,  Train Loss: 0.026,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 89.60%,  Time: 0:01:20 \n",
      "--------------\n",
      "[0.3911464512348175, 0.3781619966030121, 0.37787294387817383, 0.37034329771995544, 0.3757838010787964]\n",
      "Epoch [11/30]\n",
      "Iter:    540,  Train Loss: 0.013,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 88.44%,  Time: 0:01:22 \n",
      "Iter:    550,  Train Loss: 0.036,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 89.60%,  Time: 0:01:23 \n",
      "Iter:    560,  Train Loss: 0.019,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.17%,  Time: 0:01:24 \n",
      "Iter:    570,  Train Loss: 0.016,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 87.86%,  Time: 0:01:25 \n",
      "Iter:    580,  Train Loss: 0.029,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.75%,  Time: 0:01:26 \n",
      "Iter:    590,  Train Loss: 0.014,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.75%,  Time: 0:01:28 \n",
      "--------------\n",
      "[0.38577166199684143, 0.3975999355316162, 0.381679892539978, 0.39161571860313416, 0.3696557879447937, 0.3673340380191803]\n",
      "Epoch [12/30]\n",
      "Iter:    600,  Train Loss: 0.0087,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.17%,  Time: 0:01:29 \n",
      "Iter:    610,  Train Loss: 0.027,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.17%,  Time: 0:01:30 \n",
      "Iter:    620,  Train Loss: 0.021,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.17%,  Time: 0:01:31 \n",
      "Iter:    630,  Train Loss: 0.013,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 91.33%,  Time: 0:01:33 \n",
      "Iter:    640,  Train Loss: 0.011,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 90.75%,  Time: 0:01:34 \n",
      "--------------\n",
      "[0.3686140775680542, 0.3680087625980377, 0.3843812644481659, 0.38948553800582886, 0.3873306214809418]\n",
      "Epoch [13/30]\n",
      "Iter:    650,  Train Loss: 0.014,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.17%,  Time: 0:01:35 \n",
      "Iter:    660,  Train Loss:  0.02,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.17%,  Time: 0:01:36 \n",
      "Iter:    670,  Train Loss: 0.031,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.17%,  Time: 0:01:38 \n",
      "Iter:    680,  Train Loss: 0.017,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.17%,  Time: 0:01:39 \n",
      "Iter:    690,  Train Loss: 0.017,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.17%,  Time: 0:01:40 \n",
      "Iter:    700,  Train Loss: 0.014,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.75%,  Time: 0:01:41 \n",
      "--------------\n",
      "[0.3815785348415375, 0.38126835227012634, 0.38318824768066406, 0.37949618697166443, 0.37345990538597107, 0.37389934062957764]\n",
      "Epoch [14/30]\n",
      "Iter:    710,  Train Loss: 0.0057,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.75%,  Time: 0:01:43 \n",
      "Iter:    720,  Train Loss: 0.0082,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.17%,  Time: 0:01:44 \n",
      "Iter:    730,  Train Loss: 0.016,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.75%,  Time: 0:01:45 \n",
      "Iter:    740,  Train Loss: 0.012,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.75%,  Time: 0:01:46 \n",
      "Iter:    750,  Train Loss:  0.22,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 90.75%,  Time: 0:01:47 \n",
      "--------------\n",
      "[0.37814927101135254, 0.37787386775016785, 0.3811696469783783, 0.3772888779640198, 0.3685939610004425]\n",
      "Epoch [15/30]\n",
      "Iter:    760,  Train Loss: 0.0073,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.17%,  Time: 0:01:49 \n",
      "Iter:    770,  Train Loss: 0.014,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.17%,  Time: 0:01:50 \n",
      "Iter:    780,  Train Loss:  0.23,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 90.75%,  Time: 0:01:51 \n",
      "Iter:    790,  Train Loss: 0.015,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.75%,  Time: 0:01:52 \n",
      "Iter:    800,  Train Loss: 0.014,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.17%,  Time: 0:01:54 \n",
      "--------------\n",
      "[0.36884447932243347, 0.37079405784606934, 0.37093856930732727, 0.37112465500831604, 0.37020134925842285]\n",
      "Epoch [16/30]\n",
      "Iter:    810,  Train Loss: 0.0095,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 91.33%,  Time: 0:01:55 \n",
      "Iter:    820,  Train Loss:  0.02,  Train Acc: 100.00%,  Val Loss:  0.36,  Val Acc: 90.75%,  Time: 0:01:56 \n",
      "Iter:    830,  Train Loss: 0.0099,  Train Acc: 100.00%,  Val Loss:  0.36,  Val Acc: 90.17%,  Time: 0:01:57 \n",
      "Iter:    840,  Train Loss: 0.0094,  Train Acc: 100.00%,  Val Loss:  0.36,  Val Acc: 90.75%,  Time: 0:01:59 \n",
      "Iter:    850,  Train Loss: 0.014,  Train Acc: 100.00%,  Val Loss:  0.36,  Val Acc: 90.75%,  Time: 0:02:00 \n",
      "Iter:    860,  Train Loss: 0.011,  Train Acc: 100.00%,  Val Loss:  0.36,  Val Acc: 90.75%,  Time: 0:02:01 \n",
      "--------------\n",
      "[0.3661390244960785, 0.36096587777137756, 0.36070743203163147, 0.3631519675254822, 0.3648936152458191, 0.36321380734443665]\n",
      "Epoch [17/30]\n",
      "Iter:    870,  Train Loss: 0.0057,  Train Acc: 100.00%,  Val Loss:  0.36,  Val Acc: 90.17%,  Time: 0:02:02 \n",
      "Iter:    880,  Train Loss: 0.013,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.17%,  Time: 0:02:04 \n",
      "Iter:    890,  Train Loss: 0.011,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.75%,  Time: 0:02:05 \n",
      "Iter:    900,  Train Loss: 0.0085,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.75%,  Time: 0:02:06 \n",
      "Iter:    910,  Train Loss: 0.0077,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.75%,  Time: 0:02:07 \n",
      "--------------\n",
      "[0.3613923490047455, 0.36575326323509216, 0.37384581565856934, 0.3732384741306305, 0.37260350584983826]\n",
      "Epoch [18/30]\n",
      "Iter:    920,  Train Loss: 0.0098,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.75%,  Time: 0:02:09 \n",
      "Iter:    930,  Train Loss: 0.015,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 90.75%,  Time: 0:02:10 \n",
      "Iter:    940,  Train Loss: 0.016,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 90.17%,  Time: 0:02:11 \n",
      "Iter:    950,  Train Loss: 0.015,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:12 \n",
      "Iter:    960,  Train Loss: 0.011,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:13 \n",
      "Iter:    970,  Train Loss:  0.02,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:15 \n",
      "--------------\n",
      "[0.3733023405075073, 0.37443098425865173, 0.38859057426452637, 0.39899563789367676, 0.40146198868751526, 0.4013611674308777]\n",
      "Epoch [19/30]\n",
      "Iter:    980,  Train Loss: 0.0039,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 89.60%,  Time: 0:02:16 \n",
      "Iter:    990,  Train Loss: 0.0059,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 89.60%,  Time: 0:02:17 \n",
      "Iter:   1000,  Train Loss: 0.012,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 90.17%,  Time: 0:02:19 \n",
      "Iter:   1010,  Train Loss: 0.0093,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 90.17%,  Time: 0:02:20 \n",
      "Iter:   1020,  Train Loss: 0.072,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 90.75%,  Time: 0:02:21 \n",
      "--------------\n",
      "[0.40114808082580566, 0.40025511384010315, 0.3946128487586975, 0.3882133960723877, 0.3858891427516937]\n",
      "Epoch [20/30]\n",
      "Iter:   1030,  Train Loss: 0.0053,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.75%,  Time: 0:02:22 \n",
      "Iter:   1040,  Train Loss: 0.011,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.75%,  Time: 0:02:24 \n",
      "Iter:   1050,  Train Loss: 0.052,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.75%,  Time: 0:02:25 \n",
      "Iter:   1060,  Train Loss: 0.0099,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.75%,  Time: 0:02:26 \n",
      "Iter:   1070,  Train Loss: 0.0097,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.75%,  Time: 0:02:27 \n",
      "--------------\n",
      "[0.38412991166114807, 0.38374781608581543, 0.38386887311935425, 0.3847537636756897, 0.3844700753688812]\n",
      "Epoch [21/30]\n",
      "Iter:   1080,  Train Loss: 0.0066,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 90.75%,  Time: 0:02:29 \n",
      "Iter:   1090,  Train Loss: 0.014,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 90.17%,  Time: 0:02:30 \n",
      "Iter:   1100,  Train Loss: 0.007,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:31 \n",
      "Iter:   1110,  Train Loss: 0.0076,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:32 \n",
      "Iter:   1120,  Train Loss: 0.0099,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.75%,  Time: 0:02:34 \n",
      "Iter:   1130,  Train Loss: 0.0085,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.75%,  Time: 0:02:35 \n",
      "--------------\n",
      "[0.3832678496837616, 0.3854121267795563, 0.39558160305023193, 0.3966388404369354, 0.39655396342277527, 0.3958844244480133]\n",
      "Epoch [22/30]\n",
      "Iter:   1140,  Train Loss: 0.0047,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:36 \n",
      "Iter:   1150,  Train Loss: 0.011,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:37 \n",
      "Iter:   1160,  Train Loss: 0.013,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:39 \n",
      "Iter:   1170,  Train Loss: 0.007,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:40 \n",
      "Iter:   1180,  Train Loss: 0.0061,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.75%,  Time: 0:02:41 \n",
      "--------------\n",
      "[0.400508850812912, 0.4024242162704468, 0.402113676071167, 0.39886796474456787, 0.3971506953239441]\n",
      "Epoch [23/30]\n",
      "Iter:   1190,  Train Loss: 0.0063,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.75%,  Time: 0:02:42 \n",
      "Iter:   1200,  Train Loss: 0.011,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.75%,  Time: 0:02:44 \n",
      "Iter:   1210,  Train Loss: 0.013,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:45 \n",
      "Iter:   1220,  Train Loss: 0.015,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:46 \n",
      "Iter:   1230,  Train Loss: 0.0094,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:47 \n",
      "Iter:   1240,  Train Loss: 0.0091,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:48 \n",
      "--------------\n",
      "[0.3975030481815338, 0.39848652482032776, 0.4003596901893616, 0.40238502621650696, 0.4009506106376648, 0.40187394618988037]\n",
      "Epoch [24/30]\n",
      "Iter:   1250,  Train Loss: 0.0035,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:50 \n",
      "Iter:   1260,  Train Loss: 0.005,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:51 \n",
      "Iter:   1270,  Train Loss:  0.01,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 90.17%,  Time: 0:02:52 \n",
      "Iter:   1280,  Train Loss: 0.0079,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:53 \n",
      "Iter:   1290,  Train Loss:  0.17,  Train Acc: 87.50%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:54 \n",
      "--------------\n",
      "[0.4039336144924164, 0.4047519266605377, 0.40516728162765503, 0.40452802181243896, 0.4034964144229889]\n",
      "Epoch [25/30]\n",
      "Iter:   1300,  Train Loss: 0.0046,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:56 \n",
      "Iter:   1310,  Train Loss:  0.01,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:57 \n",
      "Iter:   1320,  Train Loss: 0.081,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:02:58 \n",
      "Iter:   1330,  Train Loss: 0.0088,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 90.17%,  Time: 0:02:59 \n",
      "Iter:   1340,  Train Loss: 0.011,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 90.17%,  Time: 0:03:00 \n",
      "--------------\n",
      "[0.40283915400505066, 0.4025944769382477, 0.4045614004135132, 0.4059976637363434, 0.4062025547027588]\n",
      "Epoch [26/30]\n",
      "Iter:   1350,  Train Loss: 0.0062,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 90.17%,  Time: 0:03:02 \n",
      "Iter:   1360,  Train Loss: 0.011,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 90.17%,  Time: 0:03:03 \n",
      "Iter:   1370,  Train Loss: 0.0071,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 90.17%,  Time: 0:03:04 \n",
      "Iter:   1380,  Train Loss: 0.0063,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 90.17%,  Time: 0:03:05 \n",
      "Iter:   1390,  Train Loss: 0.011,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 90.17%,  Time: 0:03:07 \n",
      "Iter:   1400,  Train Loss: 0.0077,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 90.17%,  Time: 0:03:08 \n",
      "--------------\n",
      "[0.4073149561882019, 0.40801265835762024, 0.40782544016838074, 0.4078041911125183, 0.40777525305747986, 0.4077339172363281]\n",
      "Epoch [27/30]\n",
      "Iter:   1410,  Train Loss: 0.0041,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 90.17%,  Time: 0:03:09 \n",
      "Iter:   1420,  Train Loss: 0.0079,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 90.17%,  Time: 0:03:10 \n",
      "Iter:   1430,  Train Loss: 0.0098,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:03:11 \n",
      "Iter:   1440,  Train Loss: 0.0063,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:03:13 \n",
      "Iter:   1450,  Train Loss: 0.0058,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:03:14 \n",
      "--------------\n",
      "[0.40923601388931274, 0.4059546887874603, 0.4031306803226471, 0.40193745493888855, 0.4012165069580078]\n",
      "Epoch [28/30]\n",
      "Iter:   1460,  Train Loss: 0.0045,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:03:15 \n",
      "Iter:   1470,  Train Loss: 0.0088,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:03:17 \n",
      "Iter:   1480,  Train Loss: 0.015,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:03:18 \n",
      "Iter:   1490,  Train Loss:  0.01,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:03:19 \n",
      "Iter:   1500,  Train Loss: 0.0088,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:03:20 \n",
      "Iter:   1510,  Train Loss: 0.0078,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:03:21 \n",
      "--------------\n",
      "[0.401202529668808, 0.4019358456134796, 0.4024464190006256, 0.39946287870407104, 0.39738577604293823, 0.3966531455516815]\n",
      "Epoch [29/30]\n",
      "Iter:   1520,  Train Loss: 0.0032,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:03:23 \n",
      "Iter:   1530,  Train Loss: 0.0043,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 90.17%,  Time: 0:03:24 \n",
      "Iter:   1540,  Train Loss: 0.0085,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 90.75%,  Time: 0:03:25 \n",
      "Iter:   1550,  Train Loss: 0.007,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 90.75%,  Time: 0:03:26 \n",
      "Iter:   1560,  Train Loss:  0.13,  Train Acc: 87.50%,  Val Loss:  0.39,  Val Acc: 90.75%,  Time: 0:03:27 \n",
      "--------------\n",
      "[0.39643725752830505, 0.39621496200561523, 0.3949355185031891, 0.39418816566467285, 0.3937051296234131]\n",
      "Epoch [30/30]\n",
      "Iter:   1570,  Train Loss: 0.0043,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 90.75%,  Time: 0:03:29 \n",
      "Iter:   1580,  Train Loss: 0.0094,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 90.75%,  Time: 0:03:30 \n",
      "Iter:   1590,  Train Loss:  0.18,  Train Acc: 87.50%,  Val Loss:  0.39,  Val Acc: 90.75%,  Time: 0:03:31 \n",
      "Iter:   1600,  Train Loss: 0.0076,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 90.75%,  Time: 0:03:33 \n",
      "Iter:   1610,  Train Loss: 0.0075,  Train Acc: 100.00%,  Val Loss:  0.39,  Val Acc: 90.75%,  Time: 0:03:34 \n",
      "--------------\n",
      "[0.3935717046260834, 0.3935403525829315, 0.39358004927635193, 0.39355209469795227, 0.39351919293403625]\n",
      "Test Loss:  0.25,  Test Acc: 92.24%\n",
      "Precision, Recall and F1-Score...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     med_use     1.0000    0.6818    0.8108        22\n",
      "     med_con     0.8333    1.0000    0.9091        25\n",
      "     med_sym     0.9412    1.0000    0.9697        32\n",
      "     sym_med     0.9091    1.0000    0.9524        20\n",
      "     unknown     1.0000    0.8824    0.9375        17\n",
      "\n",
      "    accuracy                         0.9224       116\n",
      "   macro avg     0.9367    0.9128    0.9159       116\n",
      "weighted avg     0.9322    0.9224    0.9188       116\n",
      "\n",
      "Confusion Matrix...\n",
      "[[15  5  2  0  0]\n",
      " [ 0 25  0  0  0]\n",
      " [ 0  0 32  0  0]\n",
      " [ 0  0  0 20  0]\n",
      " [ 0  0  0  2 15]]\n",
      "Time usage: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "#from train_eval import train, init_network\n",
    "import argparse\n",
    "from utils import build_dataset, build_iterator, get_time_dif\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Chinese Text Classification')\n",
    "#parser.add_argument('--model', type=str, required=True, help='choose a model: Bert, ERNIE')\n",
    "parser.add_argument('--model',default='bert',help='Bert')\n",
    "#args = parser.parse_args()\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = '../cyx＆xy/bert/THUCNews'  # 数据集\n",
    "\n",
    "    model_name = args.model  # bert\n",
    "    config = Config(dataset)\n",
    "    np.random.seed(2)\n",
    "    torch.manual_seed(2)\n",
    "    torch.cuda.manual_seed_all(2)\n",
    "    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Loading data...\")\n",
    "    train_data, dev_data, test_data = build_dataset(config)\n",
    "    train_iter = build_iterator(train_data, config)\n",
    "    dev_iter = build_iterator(dev_data, config)\n",
    "    test_iter = build_iterator(test_data, config)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "    # train\n",
    "    model = Model(config).to(config.device)\n",
    "    dev_log,train_log = train(config, model, train_iter, dev_iter, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e327700",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'../cyx＆xy/bert/THUCNews/saved_dict/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "775e6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_=[]\n",
    "for i in train_log:\n",
    "    train_.append(i.item())\n",
    "\n",
    "dev_=[]\n",
    "for i in dev_log:\n",
    "    dev_.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87126fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename='../cyx＆xy/bert/THUCNews/saved_dict/train_log.json'\n",
    "with open(filename,'w',encoding='utf-8') as file_obj:\n",
    "    json.dump(train_,file_obj,ensure_ascii=False,indent = 4)\n",
    "\n",
    "filename='../cyx＆xy/bert/THUCNews/saved_dict/dev_log.json'\n",
    "with open(filename,'w',encoding='utf-8') as file_obj:\n",
    "    json.dump(dev_,file_obj,ensure_ascii=False,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aade6eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Bert 训练曲线')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "/home/lll/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 35757 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/lll/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 32451 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/lll/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26354 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/lll/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 32447 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/lll/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 35757 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/lll/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 32451 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/lll/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26354 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/lll/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 32447 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] \n",
    "\n",
    " \n",
    "names = range(0,30)\n",
    "names = [str(x) for x in list(names)]\n",
    " \n",
    "x = range(len(names))\n",
    "#plt.plot(x, y, 'ro-')\n",
    "#plt.plot(x, y1, 'bo-')\n",
    "#pl.xlim(-1, 11)  # 限定横轴的范围\n",
    "#pl.ylim(-1, 110)  # 限定纵轴的范围\n",
    " \n",
    "plt.plot(x, train_, marker='*', mec='r', mfc='w',label='train_loss')\n",
    "plt.plot(x, dev_, marker='o', mec='b', mfc='y',label='dev_loss')\n",
    "plt.legend()  # 让图例生效\n",
    "plt.xticks(x, names, rotation=1)\n",
    " \n",
    "plt.margins(0)\n",
    "plt.subplots_adjust(bottom=0.10)\n",
    "plt.xlabel('epoch') #X轴标签\n",
    "plt.ylabel(\"loss\") #Y轴标签\n",
    "#pyplot.yticks([0.0075,0.800,50])\n",
    "plt.title(\"Bert 训练曲线\")\n",
    "#plt.savefig('D:\\\\f1.jpg',dpi = 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7246a60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f98b6e01e20>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, dev_, marker='o', mec='b', mfc='y',label='dev_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0d314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41519c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "key = {\n",
    "    0: 'med_use',\n",
    "    1: 'med_con',\n",
    "    2: 'med_sym',\n",
    "    3: 'sym_med',\n",
    "    4: 'unknown'\n",
    "}\n",
    "\n",
    "\n",
    "config = Config('../cyx＆xy/bert/THUCNews')\n",
    "model = Model(config).to(config.device)\n",
    "model.load_state_dict(torch.load(config.save_path, map_location='cpu'))\n",
    "\n",
    "\n",
    "def build_predict_text(text):\n",
    "    token = config.tokenizer.tokenize(text)\n",
    "    token = ['[CLS]'] + token\n",
    "    seq_len = len(token)\n",
    "    mask = []\n",
    "    token_ids = config.tokenizer.convert_tokens_to_ids(token)\n",
    "    pad_size = config.pad_size\n",
    "    if pad_size:\n",
    "        if len(token) < pad_size:\n",
    "            mask = [1] * len(token_ids) + ([0] * (pad_size - len(token)))\n",
    "            token_ids += ([0] * (pad_size - len(token)))\n",
    "        else:\n",
    "            mask = [1] * pad_size\n",
    "            token_ids = token_ids[:pad_size]\n",
    "            seq_len = pad_size\n",
    "    ids = torch.LongTensor([token_ids]).cuda()\n",
    "    seq_len = torch.LongTensor([seq_len]).cuda()\n",
    "    mask = torch.LongTensor([mask]).cuda()\n",
    "    return ids, seq_len, mask\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "    \"\"\"\n",
    "    单个文本预测\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = build_predict_text(text)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data)\n",
    "        num = torch.argmax(outputs)\n",
    "    return int(num),key[int(num)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a71012d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('../cyx＆xy/bert/THUCNews/data/test.txt','r',encoding='utf-8')\n",
    "test_set=f.readlines()\n",
    "sen=[]\n",
    "gold=[]\n",
    "for i in test_set:\n",
    "    temp=i.replace('\\n','')\n",
    "    temp=temp.split('\\t')\n",
    "    sen.append(temp[0])\n",
    "    gold.append(int(temp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37a503d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre=[]\n",
    "for i in sen:\n",
    "    cate = predict(i)[0]\n",
    "    pre.append(cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0a705a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     med_use     0.8750    0.6087    0.7179        23\n",
      "     med_con     0.8214    0.8846    0.8519        26\n",
      "     med_sym     0.9143    0.9697    0.9412        33\n",
      "     sym_med     0.9524    0.9524    0.9524        21\n",
      "     unknown     0.8000    0.9412    0.8649        17\n",
      "\n",
      "    accuracy                         0.8750       120\n",
      "   macro avg     0.8726    0.8713    0.8656       120\n",
      "weighted avg     0.8771    0.8750    0.8702       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(gold, pre, target_names=config.class_list, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4dbed91d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，SYM，SYM怎么办，小儿？\n",
      "(3, 'sym_med')\n"
     ]
    }
   ],
   "source": [
    "ques=input()\n",
    "print(predict(ques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYM怎么办，小儿"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
