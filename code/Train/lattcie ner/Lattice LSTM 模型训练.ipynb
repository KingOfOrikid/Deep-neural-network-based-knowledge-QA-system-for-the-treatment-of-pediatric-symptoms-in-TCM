{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lattice LSTM 模型训练.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMuj5NO7iFiP4VOt5TFtbNf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import json"],"metadata":{"id":"TPl1RKue9cVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WhD6kxUcnK9B","executionInfo":{"status":"ok","timestamp":1648015773339,"user_tz":-480,"elapsed":29926,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15214000089252266369"}},"outputId":"7672462e-4e30-4653-ab3a-90a5ffd2298f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import sys\n","from google.colab import drive\n","drive.mount('/content/drive')\n","sys.path.append('/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER')\n","\n","sys.argv=['']\n","del sys"]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/cyx_数据/问句汇总.json',encoding=\"utf-8\") as file_obj:\n","  ques_all=json.load(file_obj)"],"metadata":{"id":"LGfFXtct9hON"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ques_clean=[]\n","for i in ques_all:\n","  ques_clean.append([list(i),ques_all[i]])"],"metadata":{"id":"0EqO2aqN9u2D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_set,test_set=train_test_split(ques_clean,test_size=0.4)\n","dev_set,test_set=train_test_split(test_set,test_size=0.4)"],"metadata":{"id":"Ztu9cyW69zSE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f=open('/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/train.txt','w',encoding='utf-8')\n","for i in train_set:\n","    for index in range(0,len(i[0])):\n","        f.write(i[0][index]+'\t'+i[1][index]+'\\n')\n","    f.write('\\n')\n","f.close()\n","\n","f=open('/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/dev.txt','w',encoding='utf-8')\n","for i in dev_set:\n","    for index in range(0,len(i[0])):\n","        f.write(i[0][index]+'\t'+i[1][index]+'\\n')\n","    f.write('\\n')\n","f.close()\n","\n","f=open('/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/test.txt','w',encoding='utf-8')\n","for i in test_set:\n","    for index in range(0,len(i[0])):\n","        f.write(i[0][index]+'\t'+i[1][index]+'\\n')\n","    f.write('\\n')\n","f.close()"],"metadata":{"id":"6vyLS3Hi97iY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import sys\n","import argparse\n","import random\n","import copy\n","import torch\n","import json\n","import gc\n","import pickle\n","import torch.autograd as autograd\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from utils.metric import get_ner_fmeasure\n","from model.bilstmcrf import BiLSTM_CRF as SeqModel\n","from utils.data import Data\n","\n","seed_num = 100\n","random.seed(seed_num)\n","torch.manual_seed(seed_num)\n","np.random.seed(seed_num)\n","\n","\n","def data_initialization(data, gaz_file, train_file, dev_file, test_file):\n","    data.build_alphabet(train_file)\n","    data.build_alphabet(dev_file)\n","    data.build_alphabet(test_file)\n","    data.build_gaz_file(gaz_file)\n","\n","    #gaz_alphabet train,dev,test file在embedding中匹配到的词语\n","    data.build_gaz_alphabet(train_file)\n","    data.build_gaz_alphabet(dev_file)\n","    data.build_gaz_alphabet(test_file)\n","    data.fix_alphabet()\n","    return data\n","\n","\n","def predict_check(pred_variable, gold_variable, mask_variable):\n","    \"\"\"\n","        input:\n","            pred_variable (batch_size, sent_len): pred tag result, in numpy format\n","            gold_variable (batch_size, sent_len): gold  result variable\n","            mask_variable (batch_size, sent_len): mask variable\n","    \"\"\"\n","    pred = pred_variable.cpu().data.numpy()\n","    gold = gold_variable.cpu().data.numpy()\n","    mask = mask_variable.cpu().data.numpy()\n","    overlaped = (pred == gold)\n","    right_token = np.sum(overlaped * mask)\n","    total_token = mask.sum()\n","    # print(\"right: %s, total: %s\"%(right_token, total_token))\n","    return right_token, total_token\n","\n","\n","def recover_label(pred_variable, gold_variable, mask_variable, label_alphabet, word_recover):\n","    \"\"\"\n","        input:\n","            pred_variable (batch_size, sent_len): pred tag result\n","            gold_variable (batch_size, sent_len): gold result variable\n","            mask_variable (batch_size, sent_len): mask variable\n","    \"\"\"\n","    \n","    pred_variable = pred_variable[word_recover]\n","    gold_variable = gold_variable[word_recover]\n","    mask_variable = mask_variable[word_recover]\n","    batch_size = gold_variable.size(0)\n","    seq_len = gold_variable.size(1)\n","    mask = mask_variable.cpu().data.numpy()\n","    pred_tag = pred_variable.cpu().data.numpy()\n","    gold_tag = gold_variable.cpu().data.numpy()\n","    batch_size = mask.shape[0]\n","    pred_label = []\n","    gold_label = []\n","    for idx in range(batch_size):\n","        pred = [label_alphabet.get_instance(pred_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n","        gold = [label_alphabet.get_instance(gold_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n","        # print \"p:\",pred, pred_tag.tolist()\n","        # print \"g:\", gold, gold_tag.tolist()\n","        assert(len(pred)==len(gold))\n","        pred_label.append(pred)\n","        gold_label.append(gold)\n","    return pred_label, gold_label\n","\n","\n","def save_data_setting(data, save_file):\n","    \"\"\"\n","    new_data = copy.deepcopy(data)\n","    ## remove input instances\n","    new_data.train_texts = []\n","    new_data.dev_texts = []\n","    new_data.test_texts = []\n","    new_data.raw_texts = []\n","    new_data.train_Ids = []\n","    new_data.dev_Ids = []\n","    new_data.test_Ids = []\n","    new_data.raw_Ids = []\"\"\"\n","    ## save data settings\n","    with open(save_file, 'wb') as fp:\n","        pickle.dump(data, fp)\n","    print (\"Data setting saved to file: \", save_file)\n","\n","\n","def load_data_setting(save_file):\n","    with open(save_file, 'rb') as fp:\n","        data = pickle.load(fp)\n","    print (\"Data setting loaded from file: \", save_file)\n","    data.show_data_summary()\n","    return data\n","\n","def lr_decay(optimizer, epoch, decay_rate, init_lr):\n","    lr = init_lr * ((1-decay_rate)**epoch)\n","    print (\" Learning rate is setted as:\", lr)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","    return optimizer\n","\n","\n","\n","def evaluate(data, model, name):\n","    if name == \"train\":\n","        instances = data.train_Ids\n","    elif name == \"dev\":\n","        instances = data.dev_Ids\n","    elif name == 'test':\n","        instances = data.test_Ids\n","    elif name == 'raw':\n","        instances = data.raw_Ids\n","    else:\n","        print (\"Error: wrong evaluate name,\", name)\n","    pred_results = []\n","    gold_results = []\n","    ## set model in eval model\n","    model.eval()\n","    batch_size = 10\n","    start_time = time.time()\n","    train_num = len(instances)\n","    total_batch = train_num//batch_size+1\n","    for batch_id in range(total_batch):\n","        start = batch_id*batch_size\n","        end = (batch_id+1)*batch_size \n","        if end >train_num:\n","            end =  train_num\n","        instance = instances[start:end]\n","        if not instance:\n","            continue\n","        gaz_list,batch_word, batch_biword, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu, True)\n","        tag_seq = model(gaz_list,batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask)\n","        # print \"tag:\",tag_seq\n","        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet, batch_wordrecover)\n","        pred_results += pred_label\n","        gold_results += gold_label\n","    decode_time = time.time() - start_time\n","    speed = len(instances)/decode_time\n","    acc, p, r, f, right_num = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)\n","    return speed, acc, p, r, f, pred_results, right_num\n","\n","\n","def batchify_with_label(input_batch_list, gpu, volatile_flag=False):\n","    \"\"\"\n","        input: list of words, chars and labels, various length. [[words,biwords,chars,gaz, labels],[words,biwords,chars,labels],...]\n","            words: word ids for one sentence. (batch_size, sent_len) \n","            chars: char ids for on sentences, various length. (batch_size, sent_len, each_word_length)\n","        output:\n","            zero padding for word and char, with their batch length\n","            word_seq_tensor: (batch_size, max_sent_len) Variable\n","            word_seq_lengths: (batch_size,1) Tensor\n","            char_seq_tensor: (batch_size*max_sent_len, max_word_len) Variable\n","            char_seq_lengths: (batch_size*max_sent_len,1) Tensor\n","            char_seq_recover: (batch_size*max_sent_len,1)  recover char sequence order \n","            label_seq_tensor: (batch_size, max_sent_len)\n","            mask: (batch_size, max_sent_len) \n","    \"\"\"\n","    batch_size = len(input_batch_list)\n","    words = [sent[0] for sent in input_batch_list]\n","    biwords = [sent[1] for sent in input_batch_list]\n","    chars = [sent[2] for sent in input_batch_list]\n","\n","\n","    gazs = [sent[3] for sent in input_batch_list]\n","    labels = [sent[4] for sent in input_batch_list]\n","    word_seq_lengths = torch.LongTensor(list(map(len, words)))\n","    max_seq_len = word_seq_lengths.max().item()\n","\n","    word_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile =  volatile_flag).long()\n","    biword_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile =  volatile_flag).long()\n","    label_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).long()\n","    mask = autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).byte()\n","\n","    for idx, (seq, biseq, label, seqlen) in enumerate(zip(words, biwords, labels, word_seq_lengths)):\n","        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n","        biword_seq_tensor[idx, :seqlen] = torch.LongTensor(biseq)\n","        label_seq_tensor[idx, :seqlen] = torch.LongTensor(label)\n","        mask[idx, :seqlen] = torch.Tensor([1]*seqlen.item())\n","\n","    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n","    word_seq_tensor = word_seq_tensor[word_perm_idx]\n","    biword_seq_tensor = biword_seq_tensor[word_perm_idx]\n","    label_seq_tensor = label_seq_tensor[word_perm_idx]\n","    mask = mask[word_perm_idx]\n","\n","    ### deal with char\n","    # pad_chars (batch_size, max_seq_len)\n","    pad_chars = [chars[idx] + [[0]] * (max_seq_len-len(chars[idx])) for idx in range(len(chars))]\n","    length_list = [list(map(len, pad_char)) for pad_char in pad_chars]\n","    #length_list = [len(pad_char) for pad_char in pad_chars]\n","    max_word_len = max(map(max, length_list))\n","    char_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len, max_word_len)), volatile =  volatile_flag).long()\n","    char_seq_lengths = torch.LongTensor(length_list)\n","    for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):\n","        for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):\n","            # print len(word), wordlen\n","            char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)\n","    char_seq_tensor = char_seq_tensor[word_perm_idx].view(batch_size*max_seq_len,-1)\n","    char_seq_lengths = char_seq_lengths[word_perm_idx].view(batch_size*max_seq_len,)\n","    char_seq_lengths, char_perm_idx = char_seq_lengths.sort(0, descending=True)\n","    char_seq_tensor = char_seq_tensor[char_perm_idx]\n","    _, char_seq_recover = char_perm_idx.sort(0, descending=False)\n","    _, word_seq_recover = word_perm_idx.sort(0, descending=False)\n","    \n","    ## keep the gaz_list in orignial order\n","    \n","    gaz_list = [ gazs[i] for i in word_perm_idx]\n","    gaz_list.append(volatile_flag)\n","    if gpu:\n","        word_seq_tensor = word_seq_tensor.cuda()\n","        biword_seq_tensor = biword_seq_tensor.cuda()\n","        word_seq_lengths = word_seq_lengths.cuda()\n","        word_seq_recover = word_seq_recover.cuda()\n","        label_seq_tensor = label_seq_tensor.cuda()\n","        char_seq_tensor = char_seq_tensor.cuda()\n","        char_seq_recover = char_seq_recover.cuda()\n","        mask = mask.cuda()\n","    return gaz_list, word_seq_tensor, biword_seq_tensor, word_seq_lengths, word_seq_recover, char_seq_tensor, char_seq_lengths, char_seq_recover, label_seq_tensor, mask\n","\n","\n","def train(data, save_model_dir,save_data_set, seg=True):\n","    dev_log=[]\n","    #test_log=[]\n","    train_log=[]\n","    print (\"Training model...\")\n","    data.show_data_summary()\n","    #save_data_name = save_data_set\n","    #save_data_setting(data, save_data_name)\n","\n","    save_data_name = save_data_set\n","    save_data_setting(data, save_data_name)\n","\n","    model = SeqModel(data)\n","    print (\"finished built model.\")\n","    loss_function = nn.NLLLoss()\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optimizer = optim.SGD(parameters, lr=data.HP_lr, momentum=data.HP_momentum)\n","    best_dev = -1\n","    #data.HP_iteration = 100#epoch次数\n","    data.HP_iteration = 25#epoch次数\n","    ## start training data.HP_iteration\n","    for idx in range(data.HP_iteration):\n","        epoch_start = time.time()\n","        temp_start = epoch_start\n","        print(\"Epoch: %s/%s\" %(idx,data.HP_iteration))\n","        optimizer = lr_decay(optimizer, idx, data.HP_lr_decay, data.HP_lr)\n","        instance_count = 0\n","        sample_id = 0\n","        sample_loss = 0\n","        batch_loss = 0\n","        total_loss = 0\n","        right_token = 0\n","        whole_token = 0\n","        random.shuffle(data.train_Ids)\n","        ## set model in train model\n","        model.train()\n","        model.zero_grad()\n","        batch_size = 10 ## current only support batch size = 1 to compulate and accumulate to data.HP_batch_size update weights\n","        train_num = len(data.train_Ids)\n","        total_batch = train_num//batch_size+1\n","        for batch_id in range(total_batch):\n","            start = batch_id*batch_size\n","            end = (batch_id+1)*batch_size \n","            if end >train_num:\n","                end = train_num\n","            instance = data.train_Ids[start:end]\n","            if not instance:\n","                continue\n","            gaz_list,  batch_word, batch_biword, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu)\n","            # print \"gaz_list:\",gaz_list\n","            # exit(0)\n","            instance_count += 1\n","            loss, tag_seq = model.neg_log_likelihood_loss(gaz_list, batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, batch_label, mask)\n","            right, whole = predict_check(tag_seq, batch_label, mask)\n","            right_token += right\n","            whole_token += whole\n","            #sample_loss += loss.data[0]\n","            #total_loss += loss.data[0]\n","            sample_loss += loss.data.item()\n","            total_loss += loss.data.item()\n","            batch_loss += loss\n","\n","            #if end%2000 == 0:\n","            if end%500 == 0:\n","                temp_time = time.time()\n","                temp_cost = temp_time - temp_start\n","                temp_start = temp_time\n","                print(\"Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f\"%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))\n","                sys.stdout.flush()\n","                sample_loss = 0\n","            if end%data.HP_batch_size == 0:\n","                batch_loss.backward()\n","                optimizer.step()\n","                model.zero_grad()\n","                batch_loss = 0\n","        train_log.append(right_token)\n","        temp_time = time.time()\n","        temp_cost = temp_time - temp_start\n","        print(\"     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f\"%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))       \n","        epoch_finish = time.time()\n","        epoch_cost = epoch_finish - epoch_start\n","        print(\"Epoch: %s training finished. Time: %.2fs, speed: %.2fst/s,  total loss: %s\"%(idx, epoch_cost, train_num/epoch_cost, total_loss))\n","        \n","        # exit(0)\n","        # continue\n","        speed, acc, p, r, f, _, right_num = evaluate(data, model, \"dev\")\n","        dev_log.append(right_num)\n","        dev_finish = time.time()\n","        dev_cost = dev_finish - epoch_finish\n","\n","        if seg:\n","            current_score = f\n","            print(\"Dev: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(dev_cost, speed, acc, p, r, f))\n","        else:\n","            current_score = acc\n","            print(\"Dev: time: %.2fs speed: %.2fst/s; acc: %.4f\"%(dev_cost, speed, acc))\n","        \n","        if current_score > best_dev:\n","            if seg:\n","                print (\"Exceed previous best f score:\", best_dev)\n","            else:\n","                print (\"Exceed previous best acc score:\", best_dev)\n","\n","            model_name = save_model_dir\n","            torch.save(model.state_dict(), model_name)\n","            #model_name = save_model_dir +'.'+ str(idx) + \".model\"\n","            #torch.save(model.state_dict(), model_name)\n","            best_dev = current_score \n","        # ## decode test\n","        speed, acc, p, r, f, _, right_num = evaluate(data, model, \"test\")\n","        #test_log.append(_)\n","        test_finish = time.time()\n","        test_cost = test_finish - dev_finish\n","        if seg:\n","            print(\"Test: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(test_cost, speed, acc, p, r, f))\n","        else:\n","            print(\"Test: time: %.2fs, speed: %.2fst/s; acc: %.4f\"%(test_cost, speed, acc))\n","        \n","        gc.collect()\n","    return model,train_log,dev_log\n","\n","\n","def load_model_decode(model_dir, data, name, gpu, seg=True):\n","    data.HP_gpu = gpu\n","    print (\"Load Model from file: \", model_dir)\n","    model = SeqModel(data)\n","    ## load model need consider if the model trained in GPU and load in CPU, or vice versa\n","    # if not gpu:\n","    #     model.load_state_dict(torch.load(model_dir), map_location=lambda storage, loc: storage)\n","    #     # model = torch.load(model_dir, map_location=lambda storage, loc: storage)\n","    # else:\n","    model.load_state_dict(torch.load(model_dir))\n","        # model = torch.load(model_dir)\n","    #model = torch.load(model_dir)\n","    \n","    print(\"Decode %s data ...\"%(name))\n","    start_time = time.time()\n","    speed, acc, p, r, f, pred_results, right_num = evaluate(data, model, name)\n","    end_time = time.time()\n","    time_cost = end_time - start_time\n","    if seg:\n","        print(\"%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(name, time_cost, speed, acc, p, r, f))\n","    else:\n","        print(\"%s: time:%.2fs, speed:%.2fst/s; acc: %.4f\"%(name, time_cost, speed, acc))\n","    return pred_results\n","\n","def load_model_decode_with_model(model, data, name, gpu, seg=True):\n","    data.HP_gpu = gpu\n","    \n","    print(\"Decode %s data ...\"%(name))\n","    start_time = time.time()\n","    speed, acc, p, r, f, pred_results, right_num = evaluate(data, model, name)\n","    end_time = time.time()\n","    time_cost = end_time - start_time\n","    if seg:\n","        print(\"%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(name, time_cost, speed, acc, p, r, f))\n","    else:\n","        print(\"%s: time:%.2fs, speed:%.2fst/s; acc: %.4f\"%(name, time_cost, speed, acc))\n","    return pred_results"],"metadata":{"id":"6-H7vgvoqzly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='Tuning with bi-directional LSTM-CRF')\n","    parser.add_argument('--embedding',  help='Embedding for words', default='None')\n","    parser.add_argument('--status', choices=['train', 'test', 'decode'], help='update algorithm', default='train')\n","    #parser.add_argument('--savemodel', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/2-222-saved_model2.lstmcrf\")\n","    #parser.add_argument('--savedset', help='Dir of saved data setting', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/save-222.dset\")\n","    parser.add_argument('--savemodel', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/lattice-lstm_model_final.model\")\n","    parser.add_argument('--savedset', help='Dir of saved data setting', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/model-train-data_final.dset\")\n","    #parser.add_argument('--train', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/train.char.bmes\")\n","    #parser.add_argument('--dev', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/dev.char.bmes\" )\n","    #parser.add_argument('--test', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/test.char.bmes\")\n","\n","    parser.add_argument('--train', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/train.txt\")\n","    parser.add_argument('--dev', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/dev.txt\" )\n","    parser.add_argument('--test', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/test.txt\")\n","\n","    #parser.add_argument('--train', default=\"test_data/fyz.train.embs\")\n","    #parser.add_argument('--dev', default=\"test_data/fyz.dev.embs\")\n","    #parser.add_argument('--test', default=\"test_data/fyz.test.embs\")\n","    parser.add_argument('--seg', default=\"True\") \n","    parser.add_argument('--extendalphabet', default=\"True\") \n","    parser.add_argument('--raw') \n","    #parser.add_argument('--loadmodel',default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/2-222-saved_model2.lstmcrf\")\n","    parser.add_argument('--loadmodel',default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/lattice-lstm_model_final.model\")\n","    parser.add_argument('--output') \n","    args = parser.parse_args()\n","   \n","    train_file = args.train\n","    dev_file = args.dev\n","    test_file = args.test\n","    raw_file = args.raw\n","    model_dir = args.loadmodel\n","    dset_dir = args.savedset\n","    output_file = args.output\n","    if args.seg.lower() == \"true\":\n","        seg = True \n","    else:\n","        seg = False\n","    status = args.status.lower()\n","\n","    save_model_dir = args.savemodel\n","    gpu = torch.cuda.is_available()\n","\n","    char_emb = \"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/gigaword_chn.all.a2b.uni.ite50.vec\"\n","    #char_emb = \"/content/drive/MyDrive/lattcie ner/TCM_NER-master/TCM_NER-master/data/sgns.sikuquanshu.vec\"\n","    bichar_emb = '/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/gigaword_chn.all.a2b.bi.ite50.vec'\n","    #bichar_emb = \"/content/drive/MyDrive/lattcie ner/TCM_NER-master/TCM_NER-master/data/sgns.sikuquanshu.bigram.vec\"\n","    gaz_file = \"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/ctb.50d.vec\"\n","    # gaz_file = None\n","    # char_emb = None\n","    #bichar_emb = None\n","\n","    print (\"CuDNN:\", torch.backends.cudnn.enabled)\n","    # gpu = False\n","    print (\"GPU available:\", gpu)\n","    print (\"Status:\", status)\n","    print (\"Seg: \", seg)\n","    print (\"Train file:\", train_file)\n","    print (\"Dev file:\", dev_file)\n","    print (\"Test file:\", test_file)\n","    print (\"Raw file:\", raw_file)\n","    print (\"Char emb:\", char_emb)\n","    print (\"Bichar emb:\", bichar_emb)\n","    print (\"Gaz file:\",gaz_file)\n","    if status == 'train':\n","        print (\"Model saved to:\", save_model_dir)\n","    sys.stdout.flush()\n","    \n","    if status == 'train':\n","        data = Data()\n","        data.HP_gpu = gpu\n","        data.HP_use_char = False\n","        data.HP_batch_size = 10\n","        data.use_bigram = False\n","        data.gaz_dropout = 0.5\n","        data.norm_gaz_emb = False\n","        data.HP_fix_gaz_emb = False\n","        data_initialization(data, gaz_file, train_file, dev_file, test_file)\n","\n","        data.generate_instance_with_gaz(train_file,'train')\n","        data.generate_instance_with_gaz(dev_file,'dev')\n","        data.generate_instance_with_gaz(test_file,'test')\n","\n","        data.build_word_pretrain_emb(char_emb)\n","        data.build_biword_pretrain_emb(bichar_emb)\n","        data.build_gaz_pretrain_emb(gaz_file)\n","        #data = load_data_setting(dset_dir)\n","        model,train_log,dev_log = train(data, save_model_dir,dset_dir, seg)\n","    elif status == 'test':      \n","        data = load_data_setting(dset_dir)\n","        data.generate_instance_with_gaz(dev_file,'dev')\n","        load_model_decode(model_dir, data , 'dev', gpu, seg)\n","        data.generate_instance_with_gaz(test_file,'test')\n","        load_model_decode(model_dir, data, 'test', gpu, seg)\n","    elif status == 'decode':       \n","        data = load_data_setting(dset_dir)\n","        data.generate_instance_with_gaz(raw_file,'raw')\n","        decode_results = load_model_decode(model_dir, data, 'raw', gpu, seg)\n","        data.write_decoded_results(output_file, decode_results, 'raw')\n","    else:\n","        print (\"Invalid argument! Please use valid arguments! (train/test/decode)\")\n","\n","torch.save(model,\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/321final.model\")\n","\n","\n","'''filename='/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/test_log.json'\n","with open(filename,'w',encoding='utf-8') as file_obj:\n","    json.dump(test_log,file_obj,ensure_ascii=False,indent = 4)'''\n","\n","filename='/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/train_log.json'\n","with open(filename,'w',encoding='utf-8') as file_obj:\n","    json.dump(train_log,file_obj,ensure_ascii=False,indent = 4)\n","\n","filename='/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/dev_log.json'\n","with open(filename,'w',encoding='utf-8') as file_obj:\n","    json.dump(dev_log,file_obj,ensure_ascii=False,indent = 4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ys3wnFXxrO5f","executionInfo":{"status":"ok","timestamp":1647877027128,"user_tz":-480,"elapsed":13142765,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15214000089252266369"}},"outputId":"557eee92-2bcc-4783-9a4d-74b78c6f8c87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CuDNN: True\n","GPU available: True\n","Status: train\n","Seg:  True\n","Train file: /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/train.txt\n","Dev file: /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/dev.txt\n","Test file: /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/test.txt\n","Raw file: None\n","Char emb: /content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/gigaword_chn.all.a2b.uni.ite50.vec\n","Bichar emb: /content/drive/MyDrive/lattcie ner/TCM_NER-master/TCM_NER-master/data/gigaword_chn.all.a2b.bi.ite50.vec\n","Gaz file: /content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/ctb.50d.vec\n","Model saved to: /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/lattice-lstm_model_final.model\n","Load gaz file:  /content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/ctb.50d.vec  total size: 704368\n","gaz alphabet size: 4235\n","gaz alphabet size: 4495\n","gaz alphabet size: 4632\n","build word pretrain emb...\n","Embedding:\n","     pretrain word:11327, prefect match:1696, case_match:0, oov:35, oov%:0.02020785219399538\n","build biword pretrain emb...\n","Embedding:\n","     pretrain word:3986686, prefect match:16297, case_match:0, oov:13721, oov%:0.4570771844498484\n","build gaz pretrain emb...\n","Embedding:\n","     pretrain word:704368, prefect match:4630, case_match:0, oov:1, oov%:0.0002158894645941278\n","Training model...\n","DATA SUMMARY START:\n","     Tag          scheme: BIO\n","     MAX SENTENCE LENGTH: 250\n","     MAX   WORD   LENGTH: -1\n","     Number   normalized: True\n","     Use          bigram: False\n","     Word  alphabet size: 1732\n","     Biword alphabet size: 30019\n","     Char  alphabet size: 1732\n","     Gaz   alphabet size: 4632\n","     Label alphabet size: 8\n","     Word embedding size: 50\n","     Biword embedding size: 50\n","     Char embedding size: 30\n","     Gaz embedding size: 50\n","     Norm     word   emb: True\n","     Norm     biword emb: True\n","     Norm     gaz    emb: False\n","     Norm   gaz  dropout: 0.5\n","     Train instance number: 15473\n","     Dev   instance number: 6189\n","     Test  instance number: 4127\n","     Raw   instance number: 0\n","     Hyperpara  iteration: 100\n","     Hyperpara  batch size: 10\n","     Hyperpara          lr: 0.015\n","     Hyperpara    lr_decay: 0.05\n","     Hyperpara     HP_clip: 5.0\n","     Hyperpara    momentum: 0\n","     Hyperpara  hidden_dim: 200\n","     Hyperpara     dropout: 0.5\n","     Hyperpara  lstm_layer: 1\n","     Hyperpara      bilstm: True\n","     Hyperpara         GPU: True\n","     Hyperpara     use_gaz: True\n","     Hyperpara fix gaz emb: False\n","     Hyperpara    use_char: False\n","DATA SUMMARY END.\n","Data setting saved to file:  /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/model-train-data_final.dset\n","build batched lstmcrf...\n","build batched bilstm...\n","build LatticeLSTM...  forward , Fix emb: False  gaz drop: 0.5\n","load pretrain word emb... (4632, 50)\n","build LatticeLSTM...  backward , Fix emb: False  gaz drop: 0.5\n","load pretrain word emb... (4632, 50)\n","build batched crf...\n","finished built model.\n","Epoch: 0/25\n"," Learning rate is setted as: 0.015\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:105: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n","  init.orthogonal(self.weight_ih.data)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:106: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n","  init.orthogonal(self.alpha_weight_ih.data)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:118: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  init.constant(self.bias.data, val=0)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:119: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  init.constant(self.alpha_bias.data, val=0)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:37: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n","  init.orthogonal(self.weight_ih.data)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:43: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  init.constant(self.bias.data, val=0)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:94: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:30.)\n","  masked_cur_partition = cur_partition.masked_select(mask_idx)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:99: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:447.)\n","  partition.masked_scatter_(mask_idx, masked_cur_partition)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:243: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:30.)\n","  tg_energy = tg_energy.masked_select(mask.transpose(1,0))\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:156: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n","  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"]},{"output_type":"stream","name":"stdout","text":["Instance: 500; Time: 13.88s; loss: 5101.4219; acc: 3642.0/6446.0=0.5650\n","Instance: 1000; Time: 16.35s; loss: 1787.6117; acc: 8997.0/12900.0=0.6974\n","Instance: 1500; Time: 14.03s; loss: 1417.7816; acc: 14776.0/19702.0=0.7500\n","Instance: 2000; Time: 13.38s; loss: 929.8085; acc: 20438.0/26119.0=0.7825\n","Instance: 2500; Time: 13.34s; loss: 885.6719; acc: 26135.0/32494.0=0.8043\n","Instance: 3000; Time: 13.31s; loss: 763.6597; acc: 31952.0/38973.0=0.8198\n","Instance: 3500; Time: 13.33s; loss: 782.4998; acc: 37734.0/45402.0=0.8311\n","Instance: 4000; Time: 13.61s; loss: 669.3196; acc: 43738.0/51980.0=0.8414\n","Instance: 4500; Time: 13.75s; loss: 694.6201; acc: 49745.0/58610.0=0.8487\n","Instance: 5000; Time: 13.54s; loss: 649.2617; acc: 55728.0/65163.0=0.8552\n","Instance: 5500; Time: 13.37s; loss: 664.3034; acc: 61648.0/71677.0=0.8601\n","Instance: 6000; Time: 13.84s; loss: 681.6650; acc: 67844.0/78459.0=0.8647\n","Instance: 6500; Time: 13.58s; loss: 621.3430; acc: 73924.0/85096.0=0.8687\n","Instance: 7000; Time: 13.20s; loss: 504.3776; acc: 79891.0/91496.0=0.8732\n","Instance: 7500; Time: 13.59s; loss: 577.4188; acc: 85903.0/98004.0=0.8765\n","Instance: 8000; Time: 13.75s; loss: 581.9147; acc: 92053.0/104666.0=0.8795\n","Instance: 8500; Time: 13.52s; loss: 594.3475; acc: 98091.0/111230.0=0.8819\n","Instance: 9000; Time: 13.45s; loss: 519.8613; acc: 104234.0/117803.0=0.8848\n","Instance: 9500; Time: 13.24s; loss: 511.9738; acc: 110243.0/124282.0=0.8870\n","Instance: 10000; Time: 13.02s; loss: 518.5723; acc: 116178.0/130709.0=0.8888\n","Instance: 10500; Time: 13.42s; loss: 482.2205; acc: 122321.0/137274.0=0.8911\n","Instance: 11000; Time: 12.84s; loss: 406.6580; acc: 128218.0/143556.0=0.8932\n","Instance: 11500; Time: 13.19s; loss: 458.8347; acc: 134211.0/149972.0=0.8949\n","Instance: 12000; Time: 13.59s; loss: 493.5731; acc: 140470.0/156651.0=0.8967\n","Instance: 12500; Time: 13.18s; loss: 492.7513; acc: 146436.0/163059.0=0.8981\n","Instance: 13000; Time: 12.66s; loss: 379.2944; acc: 152200.0/169163.0=0.8997\n","Instance: 13500; Time: 14.09s; loss: 519.3082; acc: 158572.0/176006.0=0.9009\n","Instance: 14000; Time: 12.95s; loss: 445.8607; acc: 164559.0/182379.0=0.9023\n","Instance: 14500; Time: 13.01s; loss: 454.0469; acc: 170424.0/188693.0=0.9032\n","Instance: 15000; Time: 13.73s; loss: 497.4216; acc: 176630.0/195352.0=0.9042\n","     Instance: 15473; Time: 12.56s; loss: 394.4587; acc: 182356.0/201422.0=0.9053\n","Epoch: 0 training finished. Time: 418.31s, speed: 36.99st/s,  total loss: 24481.861938476562\n"]},{"output_type":"stream","name":"stderr","text":["__main__:185: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:186: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:187: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:188: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:208: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:263: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  word_var = autograd.Variable(torch.LongTensor(skip_input_[t][0]),volatile =  volatile_flag)\n"]},{"output_type":"stream","name":"stdout","text":["gold_num =  11703  pred_num =  10830  right_num =  9079\n","Dev: time: 69.69s, speed: 88.96st/s; acc: 0.9545, p: 0.8383, r: 0.7758, f: 0.8058\n","Exceed previous best f score: -1\n","gold_num =  7806  pred_num =  7199  right_num =  5965\n","Test: time: 44.77s, speed: 92.43st/s; acc: 0.9523, p: 0.8286, r: 0.7642, f: 0.7951\n","Epoch: 1/25\n"," Learning rate is setted as: 0.014249999999999999\n","Instance: 500; Time: 13.12s; loss: 435.0030; acc: 6134.0/6487.0=0.9456\n","Instance: 1000; Time: 13.09s; loss: 410.2589; acc: 12190.0/12891.0=0.9456\n","Instance: 1500; Time: 13.09s; loss: 343.2539; acc: 18239.0/19237.0=0.9481\n","Instance: 2000; Time: 13.10s; loss: 396.2875; acc: 24248.0/25586.0=0.9477\n","Instance: 2500; Time: 13.55s; loss: 448.4299; acc: 30474.0/32229.0=0.9455\n","Instance: 3000; Time: 13.58s; loss: 407.7635; acc: 36664.0/38751.0=0.9461\n","Instance: 3500; Time: 13.39s; loss: 451.5262; acc: 42814.0/45288.0=0.9454\n","Instance: 4000; Time: 13.32s; loss: 403.1479; acc: 49023.0/51877.0=0.9450\n","Instance: 4500; Time: 13.46s; loss: 397.8978; acc: 55248.0/58434.0=0.9455\n","Instance: 5000; Time: 13.17s; loss: 352.6111; acc: 61497.0/64980.0=0.9464\n","Instance: 5500; Time: 13.47s; loss: 426.7022; acc: 67743.0/71608.0=0.9460\n","Instance: 6000; Time: 13.32s; loss: 375.0865; acc: 73985.0/78177.0=0.9464\n","Instance: 6500; Time: 13.82s; loss: 455.9791; acc: 80389.0/84942.0=0.9464\n","Instance: 7000; Time: 13.26s; loss: 313.8943; acc: 86599.0/91415.0=0.9473\n","Instance: 7500; Time: 13.26s; loss: 337.2633; acc: 92717.0/97825.0=0.9478\n","Instance: 8000; Time: 13.41s; loss: 348.6859; acc: 99040.0/104447.0=0.9482\n","Instance: 8500; Time: 13.18s; loss: 384.0731; acc: 105118.0/110865.0=0.9482\n","Instance: 9000; Time: 13.38s; loss: 356.0753; acc: 111407.0/117439.0=0.9486\n","Instance: 9500; Time: 13.12s; loss: 374.4654; acc: 117528.0/123889.0=0.9487\n","Instance: 10000; Time: 12.92s; loss: 392.4199; acc: 123614.0/130307.0=0.9486\n","Instance: 10500; Time: 13.23s; loss: 336.1065; acc: 129707.0/136729.0=0.9486\n","Instance: 11000; Time: 13.00s; loss: 355.8906; acc: 135707.0/143044.0=0.9487\n","Instance: 11500; Time: 12.85s; loss: 316.0804; acc: 141863.0/149482.0=0.9490\n","Instance: 12000; Time: 12.93s; loss: 320.2543; acc: 147938.0/155825.0=0.9494\n","Instance: 12500; Time: 13.14s; loss: 360.7072; acc: 154193.0/162391.0=0.9495\n","Instance: 13000; Time: 13.19s; loss: 332.4042; acc: 160335.0/168842.0=0.9496\n","Instance: 13500; Time: 13.83s; loss: 377.0500; acc: 166813.0/175618.0=0.9499\n","Instance: 14000; Time: 13.25s; loss: 361.8663; acc: 172923.0/182051.0=0.9499\n","Instance: 14500; Time: 13.79s; loss: 391.1045; acc: 179356.0/188830.0=0.9498\n","Instance: 15000; Time: 13.16s; loss: 279.6232; acc: 185554.0/195256.0=0.9503\n","     Instance: 15473; Time: 12.72s; loss: 369.8652; acc: 191392.0/201422.0=0.9502\n","Epoch: 1 training finished. Time: 411.13s, speed: 37.64st/s,  total loss: 11611.77719116211\n","gold_num =  11703  pred_num =  12269  right_num =  9746\n","Dev: time: 68.84s, speed: 90.07st/s; acc: 0.9544, p: 0.7944, r: 0.8328, f: 0.8131\n","Exceed previous best f score: 0.8058403230817024\n","gold_num =  7806  pred_num =  8203  right_num =  6429\n","Test: time: 44.69s, speed: 92.62st/s; acc: 0.9513, p: 0.7837, r: 0.8236, f: 0.8032\n","Epoch: 2/25\n"," Learning rate is setted as: 0.0135375\n","Instance: 500; Time: 13.58s; loss: 421.1173; acc: 6340.0/6720.0=0.9435\n","Instance: 1000; Time: 13.39s; loss: 317.7992; acc: 12607.0/13295.0=0.9483\n","Instance: 1500; Time: 12.92s; loss: 277.2599; acc: 18752.0/19692.0=0.9523\n","Instance: 2000; Time: 13.44s; loss: 333.5499; acc: 25021.0/26263.0=0.9527\n","Instance: 2500; Time: 12.87s; loss: 349.7811; acc: 31135.0/32666.0=0.9531\n","Instance: 3000; Time: 13.11s; loss: 297.1550; acc: 37445.0/39212.0=0.9549\n","Instance: 3500; Time: 13.57s; loss: 302.0139; acc: 43953.0/45951.0=0.9565\n","Instance: 4000; Time: 13.20s; loss: 333.6356; acc: 50091.0/52394.0=0.9560\n","Instance: 4500; Time: 12.86s; loss: 278.9557; acc: 56187.0/58705.0=0.9571\n","Instance: 5000; Time: 13.02s; loss: 328.6898; acc: 62331.0/65119.0=0.9572\n","Instance: 5500; Time: 13.55s; loss: 355.1981; acc: 68641.0/71745.0=0.9567\n","Instance: 6000; Time: 13.59s; loss: 364.6117; acc: 75003.0/78414.0=0.9565\n","Instance: 6500; Time: 12.63s; loss: 268.3806; acc: 81033.0/84676.0=0.9570\n","Instance: 7000; Time: 13.06s; loss: 277.0787; acc: 87254.0/91133.0=0.9574\n","Instance: 7500; Time: 13.36s; loss: 297.2089; acc: 93526.0/97674.0=0.9575\n","Instance: 8000; Time: 12.61s; loss: 283.2761; acc: 99500.0/103901.0=0.9576\n","Instance: 8500; Time: 13.65s; loss: 360.1726; acc: 105948.0/110673.0=0.9573\n","Instance: 9000; Time: 12.84s; loss: 231.8608; acc: 112104.0/117034.0=0.9579\n","Instance: 9500; Time: 13.18s; loss: 351.0436; acc: 118269.0/123502.0=0.9576\n","Instance: 10000; Time: 13.62s; loss: 256.3720; acc: 124697.0/130166.0=0.9580\n","Instance: 10500; Time: 13.35s; loss: 293.6664; acc: 130999.0/136712.0=0.9582\n","Instance: 11000; Time: 13.21s; loss: 306.6598; acc: 137213.0/143207.0=0.9581\n","Instance: 11500; Time: 13.04s; loss: 277.7678; acc: 143484.0/149723.0=0.9583\n","Instance: 12000; Time: 13.32s; loss: 324.5503; acc: 149701.0/156238.0=0.9582\n","Instance: 12500; Time: 12.81s; loss: 285.7037; acc: 155706.0/162481.0=0.9583\n","Instance: 13000; Time: 13.35s; loss: 276.8639; acc: 162039.0/169073.0=0.9584\n","Instance: 13500; Time: 13.09s; loss: 297.0358; acc: 168349.0/175640.0=0.9585\n","Instance: 14000; Time: 13.05s; loss: 307.5623; acc: 174526.0/182061.0=0.9586\n","Instance: 14500; Time: 13.27s; loss: 354.5988; acc: 180761.0/188626.0=0.9583\n","Instance: 15000; Time: 13.56s; loss: 263.8665; acc: 187028.0/195136.0=0.9584\n","     Instance: 15473; Time: 12.75s; loss: 309.8004; acc: 193025.0/201422.0=0.9583\n","Epoch: 2 training finished. Time: 408.88s, speed: 37.84st/s,  total loss: 9583.236114501953\n","gold_num =  11703  pred_num =  11641  right_num =  9940\n","Dev: time: 68.86s, speed: 90.04st/s; acc: 0.9637, p: 0.8539, r: 0.8494, f: 0.8516\n","Exceed previous best f score: 0.8131153011847155\n","gold_num =  7806  pred_num =  7771  right_num =  6615\n","Test: time: 44.50s, speed: 92.99st/s; acc: 0.9629, p: 0.8512, r: 0.8474, f: 0.8493\n","Epoch: 3/25\n"," Learning rate is setted as: 0.012860624999999997\n","Instance: 500; Time: 13.33s; loss: 277.4194; acc: 6359.0/6634.0=0.9585\n","Instance: 1000; Time: 13.26s; loss: 288.4051; acc: 12653.0/13204.0=0.9583\n","Instance: 1500; Time: 12.64s; loss: 227.7367; acc: 18794.0/19538.0=0.9619\n","Instance: 2000; Time: 13.22s; loss: 297.0648; acc: 24920.0/25930.0=0.9610\n","Instance: 2500; Time: 12.88s; loss: 267.2497; acc: 31048.0/32297.0=0.9613\n","Instance: 3000; Time: 12.91s; loss: 264.5374; acc: 37224.0/38695.0=0.9620\n","Instance: 3500; Time: 12.88s; loss: 273.1595; acc: 43365.0/45067.0=0.9622\n","Instance: 4000; Time: 12.85s; loss: 270.4929; acc: 49421.0/51380.0=0.9619\n","Instance: 4500; Time: 12.98s; loss: 324.7944; acc: 55602.0/57800.0=0.9620\n","Instance: 5000; Time: 13.38s; loss: 271.6524; acc: 61944.0/64388.0=0.9620\n","Instance: 5500; Time: 13.67s; loss: 333.6334; acc: 68337.0/71084.0=0.9614\n","Instance: 6000; Time: 13.57s; loss: 289.9910; acc: 74716.0/77699.0=0.9616\n","Instance: 6500; Time: 13.57s; loss: 258.3925; acc: 81111.0/84315.0=0.9620\n","Instance: 7000; Time: 13.38s; loss: 295.2563; acc: 87440.0/90906.0=0.9619\n","Instance: 7500; Time: 13.34s; loss: 296.1263; acc: 93728.0/97483.0=0.9615\n","Instance: 8000; Time: 13.46s; loss: 327.7960; acc: 100064.0/104126.0=0.9610\n","Instance: 8500; Time: 12.93s; loss: 226.5684; acc: 106322.0/110563.0=0.9616\n","Instance: 9000; Time: 12.83s; loss: 250.8171; acc: 112392.0/116883.0=0.9616\n","Instance: 9500; Time: 13.45s; loss: 307.6848; acc: 118776.0/123519.0=0.9616\n","Instance: 10000; Time: 13.36s; loss: 240.4905; acc: 125167.0/130123.0=0.9619\n","Instance: 10500; Time: 12.98s; loss: 222.0016; acc: 131454.0/136622.0=0.9622\n","Instance: 11000; Time: 12.84s; loss: 274.9414; acc: 137569.0/142984.0=0.9621\n","Instance: 11500; Time: 13.24s; loss: 238.0449; acc: 143986.0/149608.0=0.9624\n","Instance: 12000; Time: 13.49s; loss: 250.0272; acc: 150264.0/156116.0=0.9625\n","Instance: 12500; Time: 13.35s; loss: 235.6128; acc: 156614.0/162685.0=0.9627\n","Instance: 13000; Time: 12.81s; loss: 242.6597; acc: 162776.0/169020.0=0.9631\n","Instance: 13500; Time: 13.10s; loss: 281.5888; acc: 169032.0/175512.0=0.9631\n","Instance: 14000; Time: 12.82s; loss: 209.7646; acc: 175225.0/181850.0=0.9636\n","Instance: 14500; Time: 13.53s; loss: 266.7625; acc: 181717.0/188600.0=0.9635\n","Instance: 15000; Time: 13.73s; loss: 298.2960; acc: 188241.0/195394.0=0.9634\n","     Instance: 15473; Time: 12.40s; loss: 278.6911; acc: 194028.0/201422.0=0.9633\n","Epoch: 3 training finished. Time: 408.19s, speed: 37.91st/s,  total loss: 8387.659240722656\n","gold_num =  11703  pred_num =  11167  right_num =  9928\n","Dev: time: 69.00s, speed: 89.85st/s; acc: 0.9693, p: 0.8890, r: 0.8483, f: 0.8682\n","Exceed previous best f score: 0.8516106922549691\n","gold_num =  7806  pred_num =  7473  right_num =  6599\n","Test: time: 44.45s, speed: 93.11st/s; acc: 0.9678, p: 0.8830, r: 0.8454, f: 0.8638\n","Epoch: 4/25\n"," Learning rate is setted as: 0.012217593749999998\n","Instance: 500; Time: 13.44s; loss: 237.6797; acc: 6409.0/6633.0=0.9662\n","Instance: 1000; Time: 12.61s; loss: 203.3330; acc: 12542.0/12940.0=0.9692\n","Instance: 1500; Time: 13.58s; loss: 291.9642; acc: 18863.0/19501.0=0.9673\n","Instance: 2000; Time: 13.05s; loss: 222.7167; acc: 25089.0/25965.0=0.9663\n","Instance: 2500; Time: 13.35s; loss: 242.9128; acc: 31345.0/32456.0=0.9658\n","Instance: 3000; Time: 12.82s; loss: 216.3669; acc: 37434.0/38756.0=0.9659\n","Instance: 3500; Time: 13.67s; loss: 217.6016; acc: 43888.0/45369.0=0.9674\n","Instance: 4000; Time: 13.64s; loss: 256.6537; acc: 50368.0/52056.0=0.9676\n","Instance: 4500; Time: 13.11s; loss: 228.8143; acc: 56687.0/58556.0=0.9681\n","Instance: 5000; Time: 13.94s; loss: 269.7109; acc: 63213.0/65339.0=0.9675\n","Instance: 5500; Time: 13.44s; loss: 221.0679; acc: 69577.0/71922.0=0.9674\n","Instance: 6000; Time: 13.11s; loss: 248.4130; acc: 75846.0/78411.0=0.9673\n","Instance: 6500; Time: 12.72s; loss: 208.0551; acc: 82001.0/84740.0=0.9677\n","Instance: 7000; Time: 13.32s; loss: 266.6227; acc: 88283.0/91263.0=0.9673\n","Instance: 7500; Time: 13.33s; loss: 246.0504; acc: 94546.0/97749.0=0.9672\n","Instance: 8000; Time: 13.20s; loss: 234.6477; acc: 100884.0/104287.0=0.9674\n","Instance: 8500; Time: 13.37s; loss: 208.9229; acc: 107247.0/110842.0=0.9676\n","Instance: 9000; Time: 12.86s; loss: 251.6052; acc: 113451.0/117282.0=0.9673\n","Instance: 9500; Time: 13.54s; loss: 263.3750; acc: 119818.0/123896.0=0.9671\n","Instance: 10000; Time: 12.79s; loss: 193.9522; acc: 126012.0/130269.0=0.9673\n","Instance: 10500; Time: 13.14s; loss: 257.7965; acc: 132163.0/136647.0=0.9672\n","Instance: 11000; Time: 12.43s; loss: 212.7261; acc: 138218.0/142876.0=0.9674\n","Instance: 11500; Time: 13.19s; loss: 266.1133; acc: 144549.0/149428.0=0.9673\n","Instance: 12000; Time: 13.30s; loss: 273.7389; acc: 150863.0/155992.0=0.9671\n","Instance: 12500; Time: 13.27s; loss: 227.6326; acc: 157214.0/162561.0=0.9671\n","Instance: 13000; Time: 13.15s; loss: 192.4902; acc: 163581.0/169095.0=0.9674\n","Instance: 13500; Time: 13.31s; loss: 237.7216; acc: 169999.0/175726.0=0.9674\n","Instance: 14000; Time: 13.46s; loss: 226.7502; acc: 176314.0/182245.0=0.9675\n","Instance: 14500; Time: 13.57s; loss: 306.7417; acc: 182598.0/188808.0=0.9671\n","Instance: 15000; Time: 13.05s; loss: 220.5684; acc: 188827.0/195227.0=0.9672\n","     Instance: 15473; Time: 12.63s; loss: 234.3253; acc: 194818.0/201422.0=0.9672\n","Epoch: 4 training finished. Time: 409.41s, speed: 37.79st/s,  total loss: 7387.070556640625\n","gold_num =  11703  pred_num =  11499  right_num =  10223\n","Dev: time: 68.74s, speed: 90.20st/s; acc: 0.9712, p: 0.8890, r: 0.8735, f: 0.8812\n","Exceed previous best f score: 0.8682116309575865\n","gold_num =  7806  pred_num =  7692  right_num =  6802\n","Test: time: 44.59s, speed: 92.79st/s; acc: 0.9703, p: 0.8843, r: 0.8714, f: 0.8778\n","Epoch: 5/25\n"," Learning rate is setted as: 0.011606714062499995\n","Instance: 500; Time: 13.34s; loss: 235.6484; acc: 6488.0/6706.0=0.9675\n","Instance: 1000; Time: 12.72s; loss: 233.7969; acc: 12590.0/13018.0=0.9671\n","Instance: 1500; Time: 12.91s; loss: 209.0137; acc: 18858.0/19490.0=0.9676\n","Instance: 2000; Time: 12.95s; loss: 192.3430; acc: 25088.0/25896.0=0.9688\n","Instance: 2500; Time: 12.83s; loss: 213.3113; acc: 31276.0/32275.0=0.9690\n","Instance: 3000; Time: 13.11s; loss: 211.7949; acc: 37517.0/38703.0=0.9694\n","Instance: 3500; Time: 13.41s; loss: 222.0660; acc: 43845.0/45223.0=0.9695\n","Instance: 4000; Time: 13.32s; loss: 223.1642; acc: 50247.0/51827.0=0.9695\n","Instance: 4500; Time: 12.80s; loss: 171.6266; acc: 56372.0/58129.0=0.9698\n","Instance: 5000; Time: 12.78s; loss: 197.5648; acc: 62575.0/64532.0=0.9697\n","Instance: 5500; Time: 13.40s; loss: 213.7930; acc: 68933.0/71102.0=0.9695\n","Instance: 6000; Time: 12.43s; loss: 179.9944; acc: 75030.0/77355.0=0.9699\n","Instance: 6500; Time: 12.68s; loss: 179.3718; acc: 81184.0/83656.0=0.9705\n","Instance: 7000; Time: 12.67s; loss: 227.1597; acc: 87385.0/90041.0=0.9705\n","Instance: 7500; Time: 12.96s; loss: 225.7656; acc: 93685.0/96551.0=0.9703\n","Instance: 8000; Time: 13.00s; loss: 210.6428; acc: 99931.0/102979.0=0.9704\n","Instance: 8500; Time: 13.24s; loss: 185.5289; acc: 106311.0/109547.0=0.9705\n","Instance: 9000; Time: 13.29s; loss: 220.9519; acc: 112635.0/116041.0=0.9706\n","Instance: 9500; Time: 13.18s; loss: 215.7103; acc: 118997.0/122583.0=0.9707\n","Instance: 10000; Time: 13.51s; loss: 201.7844; acc: 125565.0/129339.0=0.9708\n","Instance: 10500; Time: 13.29s; loss: 212.5469; acc: 131915.0/135876.0=0.9708\n","Instance: 11000; Time: 13.25s; loss: 215.4002; acc: 138222.0/142417.0=0.9705\n","Instance: 11500; Time: 13.29s; loss: 247.5254; acc: 144437.0/148855.0=0.9703\n","Instance: 12000; Time: 12.94s; loss: 199.6859; acc: 150644.0/155255.0=0.9703\n","Instance: 12500; Time: 13.49s; loss: 218.0462; acc: 157141.0/161934.0=0.9704\n","Instance: 13000; Time: 13.00s; loss: 207.8604; acc: 163468.0/168442.0=0.9705\n","Instance: 13500; Time: 13.34s; loss: 224.8898; acc: 169831.0/175006.0=0.9704\n","Instance: 14000; Time: 13.59s; loss: 194.3685; acc: 176370.0/181690.0=0.9707\n","Instance: 14500; Time: 13.72s; loss: 258.6108; acc: 182871.0/188430.0=0.9705\n","Instance: 15000; Time: 13.61s; loss: 226.1288; acc: 189361.0/195114.0=0.9705\n","     Instance: 15473; Time: 12.97s; loss: 237.6277; acc: 195456.0/201422.0=0.9704\n","Epoch: 5 training finished. Time: 407.00s, speed: 38.02st/s,  total loss: 6613.722991943359\n","gold_num =  11703  pred_num =  11762  right_num =  10337\n","Dev: time: 68.42s, speed: 90.63st/s; acc: 0.9709, p: 0.8788, r: 0.8833, f: 0.8811\n","gold_num =  7806  pred_num =  7850  right_num =  6877\n","Test: time: 44.04s, speed: 93.89st/s; acc: 0.9699, p: 0.8761, r: 0.8810, f: 0.8785\n","Epoch: 6/25\n"," Learning rate is setted as: 0.011026378359374997\n","Instance: 500; Time: 13.35s; loss: 204.6117; acc: 6485.0/6670.0=0.9723\n","Instance: 1000; Time: 13.13s; loss: 191.1140; acc: 12937.0/13280.0=0.9742\n","Instance: 1500; Time: 14.09s; loss: 243.3019; acc: 19556.0/20142.0=0.9709\n","Instance: 2000; Time: 13.23s; loss: 220.4487; acc: 25860.0/26647.0=0.9705\n","Instance: 2500; Time: 12.80s; loss: 187.1191; acc: 32131.0/33092.0=0.9710\n","Instance: 3000; Time: 13.19s; loss: 184.0634; acc: 38484.0/39594.0=0.9720\n","Instance: 3500; Time: 13.28s; loss: 205.3567; acc: 44850.0/46155.0=0.9717\n","Instance: 4000; Time: 12.88s; loss: 204.2280; acc: 51135.0/52632.0=0.9716\n","Instance: 4500; Time: 13.08s; loss: 210.7668; acc: 57418.0/59091.0=0.9717\n","Instance: 5000; Time: 13.40s; loss: 181.6920; acc: 63784.0/65634.0=0.9718\n","Instance: 5500; Time: 13.10s; loss: 210.6561; acc: 70108.0/72113.0=0.9722\n","Instance: 6000; Time: 13.38s; loss: 214.9164; acc: 76488.0/78687.0=0.9721\n","Instance: 6500; Time: 13.48s; loss: 180.1819; acc: 82986.0/85352.0=0.9723\n","Instance: 7000; Time: 13.23s; loss: 209.8508; acc: 89368.0/91913.0=0.9723\n","Instance: 7500; Time: 13.28s; loss: 241.9349; acc: 95763.0/98531.0=0.9719\n","Instance: 8000; Time: 13.06s; loss: 195.9456; acc: 101941.0/104891.0=0.9719\n","Instance: 8500; Time: 12.91s; loss: 158.0525; acc: 108178.0/111270.0=0.9722\n","Instance: 9000; Time: 12.88s; loss: 205.8519; acc: 114426.0/117702.0=0.9722\n","Instance: 9500; Time: 13.30s; loss: 215.8491; acc: 120870.0/124360.0=0.9719\n","Instance: 10000; Time: 13.23s; loss: 201.6880; acc: 127221.0/130890.0=0.9720\n","Instance: 10500; Time: 12.18s; loss: 166.4682; acc: 133191.0/137014.0=0.9721\n","Instance: 11000; Time: 13.24s; loss: 186.7208; acc: 139531.0/143504.0=0.9723\n","Instance: 11500; Time: 12.91s; loss: 179.2312; acc: 145829.0/149951.0=0.9725\n","Instance: 12000; Time: 12.72s; loss: 213.0704; acc: 151964.0/156277.0=0.9724\n","Instance: 12500; Time: 13.04s; loss: 222.0559; acc: 158315.0/162826.0=0.9723\n","Instance: 13000; Time: 13.20s; loss: 225.5851; acc: 164644.0/169365.0=0.9721\n","Instance: 13500; Time: 13.50s; loss: 233.5977; acc: 171060.0/175975.0=0.9721\n","Instance: 14000; Time: 12.71s; loss: 127.1403; acc: 177214.0/182231.0=0.9725\n","Instance: 14500; Time: 13.01s; loss: 191.6276; acc: 183345.0/188560.0=0.9723\n","Instance: 15000; Time: 12.93s; loss: 197.7537; acc: 189689.0/195074.0=0.9724\n","     Instance: 15473; Time: 12.96s; loss: 216.7059; acc: 195853.0/201422.0=0.9724\n","Epoch: 6 training finished. Time: 406.68s, speed: 38.05st/s,  total loss: 6227.586181640625\n","gold_num =  11703  pred_num =  11462  right_num =  10307\n","Dev: time: 69.13s, speed: 89.68st/s; acc: 0.9737, p: 0.8992, r: 0.8807, f: 0.8899\n","Exceed previous best f score: 0.881217136453754\n","gold_num =  7806  pred_num =  7670  right_num =  6879\n","Test: time: 44.52s, speed: 92.95st/s; acc: 0.9729, p: 0.8969, r: 0.8812, f: 0.8890\n","Epoch: 7/25\n"," Learning rate is setted as: 0.010475059441406245\n","Instance: 500; Time: 13.03s; loss: 148.6367; acc: 6389.0/6522.0=0.9796\n","Instance: 1000; Time: 13.40s; loss: 203.9642; acc: 12709.0/13067.0=0.9726\n","Instance: 1500; Time: 13.39s; loss: 210.1284; acc: 19167.0/19699.0=0.9730\n","Instance: 2000; Time: 13.38s; loss: 211.3961; acc: 25585.0/26290.0=0.9732\n","Instance: 2500; Time: 13.25s; loss: 142.1948; acc: 31932.0/32765.0=0.9746\n","Instance: 3000; Time: 13.06s; loss: 153.7181; acc: 38324.0/39290.0=0.9754\n","Instance: 3500; Time: 13.60s; loss: 224.5728; acc: 44818.0/46006.0=0.9742\n","Instance: 4000; Time: 12.75s; loss: 184.7242; acc: 51048.0/52385.0=0.9745\n","Instance: 4500; Time: 13.36s; loss: 165.0303; acc: 57492.0/58959.0=0.9751\n","Instance: 5000; Time: 12.81s; loss: 182.8412; acc: 63653.0/65259.0=0.9754\n","Instance: 5500; Time: 13.11s; loss: 180.8707; acc: 70040.0/71792.0=0.9756\n","Instance: 6000; Time: 12.63s; loss: 162.9497; acc: 76222.0/78093.0=0.9760\n","Instance: 6500; Time: 13.57s; loss: 206.4027; acc: 82730.0/84805.0=0.9755\n","Instance: 7000; Time: 13.26s; loss: 235.4730; acc: 89042.0/91323.0=0.9750\n","Instance: 7500; Time: 12.87s; loss: 196.8583; acc: 95212.0/97675.0=0.9748\n","Instance: 8000; Time: 13.37s; loss: 165.8082; acc: 101678.0/104274.0=0.9751\n","Instance: 8500; Time: 12.99s; loss: 178.7760; acc: 107988.0/110753.0=0.9750\n","Instance: 9000; Time: 13.16s; loss: 172.2539; acc: 114276.0/117193.0=0.9751\n","Instance: 9500; Time: 12.98s; loss: 161.8689; acc: 120461.0/123547.0=0.9750\n","Instance: 10000; Time: 13.25s; loss: 199.6808; acc: 126859.0/130137.0=0.9748\n","Instance: 10500; Time: 13.68s; loss: 197.2878; acc: 133458.0/136916.0=0.9747\n","Instance: 11000; Time: 13.30s; loss: 165.9531; acc: 139726.0/143333.0=0.9748\n","Instance: 11500; Time: 13.13s; loss: 212.8374; acc: 145938.0/149750.0=0.9745\n","Instance: 12000; Time: 13.77s; loss: 195.3965; acc: 152509.0/156515.0=0.9744\n","Instance: 12500; Time: 13.11s; loss: 212.2338; acc: 158625.0/162806.0=0.9743\n","Instance: 13000; Time: 13.03s; loss: 184.9969; acc: 164966.0/169316.0=0.9743\n","Instance: 13500; Time: 13.43s; loss: 188.4224; acc: 171385.0/175905.0=0.9743\n","Instance: 14000; Time: 12.94s; loss: 164.6327; acc: 177645.0/182343.0=0.9742\n","Instance: 14500; Time: 13.46s; loss: 195.7305; acc: 184103.0/188971.0=0.9742\n","Instance: 15000; Time: 13.36s; loss: 209.0890; acc: 190438.0/195496.0=0.9741\n","     Instance: 15473; Time: 12.11s; loss: 143.4931; acc: 196245.0/201422.0=0.9743\n","Epoch: 7 training finished. Time: 408.53s, speed: 37.87st/s,  total loss: 5758.222381591797\n","gold_num =  11703  pred_num =  11541  right_num =  10376\n","Dev: time: 69.38s, speed: 89.37st/s; acc: 0.9741, p: 0.8991, r: 0.8866, f: 0.8928\n","Exceed previous best f score: 0.8898769695661558\n","gold_num =  7806  pred_num =  7717  right_num =  6887\n","Test: time: 44.60s, speed: 92.78st/s; acc: 0.9729, p: 0.8924, r: 0.8823, f: 0.8873\n","Epoch: 8/25\n"," Learning rate is setted as: 0.009951306469335933\n","Instance: 500; Time: 13.06s; loss: 164.4886; acc: 6320.0/6482.0=0.9750\n","Instance: 1000; Time: 13.14s; loss: 175.6016; acc: 12639.0/12977.0=0.9740\n","Instance: 1500; Time: 12.68s; loss: 156.8391; acc: 18791.0/19277.0=0.9748\n","Instance: 2000; Time: 13.51s; loss: 171.2557; acc: 25241.0/25884.0=0.9752\n","Instance: 2500; Time: 13.10s; loss: 187.8342; acc: 31624.0/32430.0=0.9751\n","Instance: 3000; Time: 12.73s; loss: 144.1919; acc: 37737.0/38669.0=0.9759\n","Instance: 3500; Time: 13.53s; loss: 190.0559; acc: 44265.0/45369.0=0.9757\n","Instance: 4000; Time: 13.04s; loss: 145.4512; acc: 50554.0/51772.0=0.9765\n","Instance: 4500; Time: 13.04s; loss: 163.6595; acc: 56903.0/58285.0=0.9763\n","Instance: 5000; Time: 13.13s; loss: 195.1343; acc: 63171.0/64718.0=0.9761\n","Instance: 5500; Time: 13.11s; loss: 134.8173; acc: 69512.0/71184.0=0.9765\n","Instance: 6000; Time: 13.24s; loss: 205.8093; acc: 75985.0/77819.0=0.9764\n","Instance: 6500; Time: 13.21s; loss: 165.9456; acc: 82360.0/84336.0=0.9766\n","Instance: 7000; Time: 12.82s; loss: 146.5574; acc: 88586.0/90693.0=0.9768\n","Instance: 7500; Time: 13.08s; loss: 188.2791; acc: 94919.0/97196.0=0.9766\n","Instance: 8000; Time: 13.38s; loss: 148.3313; acc: 101380.0/103780.0=0.9769\n","Instance: 8500; Time: 12.79s; loss: 167.6417; acc: 107570.0/110143.0=0.9766\n","Instance: 9000; Time: 13.22s; loss: 198.6466; acc: 113916.0/116643.0=0.9766\n","Instance: 9500; Time: 13.44s; loss: 151.3474; acc: 120445.0/123325.0=0.9766\n","Instance: 10000; Time: 12.90s; loss: 148.5193; acc: 126662.0/129689.0=0.9767\n","Instance: 10500; Time: 13.43s; loss: 211.4155; acc: 133095.0/136323.0=0.9763\n","Instance: 11000; Time: 13.50s; loss: 200.4808; acc: 139503.0/142930.0=0.9760\n","Instance: 11500; Time: 13.23s; loss: 171.3257; acc: 145798.0/149364.0=0.9761\n","Instance: 12000; Time: 13.64s; loss: 244.3953; acc: 152236.0/156070.0=0.9754\n","Instance: 12500; Time: 13.27s; loss: 164.6953; acc: 158693.0/162666.0=0.9756\n","Instance: 13000; Time: 13.19s; loss: 157.8195; acc: 165031.0/169144.0=0.9757\n","Instance: 13500; Time: 12.59s; loss: 168.7162; acc: 171166.0/175429.0=0.9757\n","Instance: 14000; Time: 13.50s; loss: 223.8890; acc: 177622.0/182071.0=0.9756\n","Instance: 14500; Time: 13.39s; loss: 174.0798; acc: 184149.0/188747.0=0.9756\n","Instance: 15000; Time: 13.57s; loss: 156.8552; acc: 190662.0/195384.0=0.9758\n","     Instance: 15473; Time: 12.25s; loss: 167.0103; acc: 196547.0/201422.0=0.9758\n","Epoch: 8 training finished. Time: 407.69s, speed: 37.95st/s,  total loss: 5391.089660644531\n","gold_num =  11703  pred_num =  11460  right_num =  10375\n","Dev: time: 69.10s, speed: 89.73st/s; acc: 0.9751, p: 0.9053, r: 0.8865, f: 0.8958\n","Exceed previous best f score: 0.892789537084839\n","gold_num =  7806  pred_num =  7686  right_num =  6910\n","Test: time: 44.52s, speed: 92.95st/s; acc: 0.9738, p: 0.8990, r: 0.8852, f: 0.8921\n","Epoch: 9/25\n"," Learning rate is setted as: 0.009453741145869136\n","Instance: 500; Time: 13.38s; loss: 154.7511; acc: 6463.0/6603.0=0.9788\n","Instance: 1000; Time: 12.32s; loss: 136.6377; acc: 12433.0/12689.0=0.9798\n","Instance: 1500; Time: 13.31s; loss: 175.5360; acc: 18904.0/19318.0=0.9786\n","Instance: 2000; Time: 12.91s; loss: 196.3903; acc: 25187.0/25763.0=0.9776\n","Instance: 2500; Time: 13.21s; loss: 164.4631; acc: 31528.0/32261.0=0.9773\n","Instance: 3000; Time: 13.14s; loss: 183.3688; acc: 37798.0/38680.0=0.9772\n","Instance: 3500; Time: 13.58s; loss: 166.6678; acc: 44235.0/45253.0=0.9775\n","Instance: 4000; Time: 12.85s; loss: 192.1234; acc: 50440.0/51643.0=0.9767\n","Instance: 4500; Time: 13.28s; loss: 160.8105; acc: 56817.0/58169.0=0.9768\n","Instance: 5000; Time: 13.62s; loss: 175.0283; acc: 63299.0/64797.0=0.9769\n","Instance: 5500; Time: 13.12s; loss: 134.3199; acc: 69637.0/71266.0=0.9771\n","Instance: 6000; Time: 13.40s; loss: 164.2677; acc: 76013.0/77787.0=0.9772\n","Instance: 6500; Time: 13.50s; loss: 205.9994; acc: 82506.0/84438.0=0.9771\n","Instance: 7000; Time: 13.25s; loss: 152.2458; acc: 88887.0/90944.0=0.9774\n","Instance: 7500; Time: 13.41s; loss: 205.3413; acc: 95320.0/97560.0=0.9770\n","Instance: 8000; Time: 13.17s; loss: 138.9137; acc: 101675.0/104054.0=0.9771\n","Instance: 8500; Time: 13.23s; loss: 166.6477; acc: 108141.0/110668.0=0.9772\n","Instance: 9000; Time: 12.83s; loss: 178.7352; acc: 114300.0/117004.0=0.9769\n","Instance: 9500; Time: 13.15s; loss: 147.8638; acc: 120683.0/123527.0=0.9770\n","Instance: 10000; Time: 13.17s; loss: 148.0088; acc: 127130.0/130100.0=0.9772\n","Instance: 10500; Time: 13.30s; loss: 145.7875; acc: 133555.0/136658.0=0.9773\n","Instance: 11000; Time: 12.88s; loss: 174.3065; acc: 139802.0/143053.0=0.9773\n","Instance: 11500; Time: 13.64s; loss: 179.9962; acc: 146366.0/149783.0=0.9772\n","Instance: 12000; Time: 12.77s; loss: 154.8475; acc: 152543.0/156135.0=0.9770\n","Instance: 12500; Time: 13.01s; loss: 166.0657; acc: 158845.0/162585.0=0.9770\n","Instance: 13000; Time: 12.95s; loss: 153.8916; acc: 165187.0/169054.0=0.9771\n","Instance: 13500; Time: 12.91s; loss: 162.2855; acc: 171470.0/175460.0=0.9773\n","Instance: 14000; Time: 13.05s; loss: 150.1649; acc: 177853.0/181976.0=0.9773\n","Instance: 14500; Time: 13.13s; loss: 171.5151; acc: 184169.0/188440.0=0.9773\n","Instance: 15000; Time: 13.34s; loss: 192.7566; acc: 190685.0/195119.0=0.9773\n","     Instance: 15473; Time: 12.90s; loss: 193.0511; acc: 196816.0/201422.0=0.9771\n","Epoch: 9 training finished. Time: 407.74s, speed: 37.95st/s,  total loss: 5192.788818359375\n","gold_num =  11703  pred_num =  11505  right_num =  10467\n","Dev: time: 68.81s, speed: 90.11st/s; acc: 0.9764, p: 0.9098, r: 0.8944, f: 0.9020\n","Exceed previous best f score: 0.895825238526961\n","gold_num =  7806  pred_num =  7683  right_num =  6954\n","Test: time: 44.58s, speed: 92.81st/s; acc: 0.9753, p: 0.9051, r: 0.8909, f: 0.8979\n","Epoch: 10/25\n"," Learning rate is setted as: 0.00898105408857568\n","Instance: 500; Time: 13.20s; loss: 140.2449; acc: 6427.0/6574.0=0.9776\n","Instance: 1000; Time: 13.44s; loss: 141.2881; acc: 12877.0/13168.0=0.9779\n","Instance: 1500; Time: 13.13s; loss: 124.2841; acc: 19310.0/19717.0=0.9794\n","Instance: 2000; Time: 13.27s; loss: 159.3961; acc: 25602.0/26176.0=0.9781\n","Instance: 2500; Time: 13.65s; loss: 190.5995; acc: 32214.0/32955.0=0.9775\n","Instance: 3000; Time: 12.63s; loss: 146.7544; acc: 38475.0/39345.0=0.9779\n","Instance: 3500; Time: 12.83s; loss: 130.2816; acc: 44754.0/45742.0=0.9784\n","Instance: 4000; Time: 12.78s; loss: 172.5195; acc: 50993.0/52122.0=0.9783\n","Instance: 4500; Time: 13.14s; loss: 158.6058; acc: 57423.0/58674.0=0.9787\n","Instance: 5000; Time: 13.82s; loss: 183.0577; acc: 64114.0/65538.0=0.9783\n","Instance: 5500; Time: 13.47s; loss: 196.8889; acc: 70583.0/72174.0=0.9780\n","Instance: 6000; Time: 13.19s; loss: 144.8270; acc: 76900.0/78628.0=0.9780\n","Instance: 6500; Time: 13.47s; loss: 178.5333; acc: 83384.0/85283.0=0.9777\n","Instance: 7000; Time: 12.91s; loss: 166.5601; acc: 89622.0/91676.0=0.9776\n","Instance: 7500; Time: 13.24s; loss: 184.7078; acc: 96025.0/98244.0=0.9774\n","Instance: 8000; Time: 13.32s; loss: 191.3860; acc: 102449.0/104836.0=0.9772\n","Instance: 8500; Time: 13.68s; loss: 173.4786; acc: 109033.0/111558.0=0.9774\n","Instance: 9000; Time: 12.88s; loss: 144.0685; acc: 115353.0/118008.0=0.9775\n","Instance: 9500; Time: 12.57s; loss: 180.4108; acc: 121558.0/124369.0=0.9774\n","Instance: 10000; Time: 12.78s; loss: 159.3066; acc: 127809.0/130770.0=0.9774\n","Instance: 10500; Time: 13.17s; loss: 170.8723; acc: 134182.0/137314.0=0.9772\n","Instance: 11000; Time: 13.00s; loss: 149.9933; acc: 140495.0/143773.0=0.9772\n","Instance: 11500; Time: 13.25s; loss: 181.5212; acc: 146853.0/150312.0=0.9770\n","Instance: 12000; Time: 12.92s; loss: 144.3123; acc: 153229.0/156799.0=0.9772\n","Instance: 12500; Time: 12.29s; loss: 160.5280; acc: 159267.0/162957.0=0.9774\n","Instance: 13000; Time: 13.28s; loss: 194.1835; acc: 165758.0/169625.0=0.9772\n","Instance: 13500; Time: 12.36s; loss: 132.1628; acc: 171921.0/175889.0=0.9774\n","Instance: 14000; Time: 12.81s; loss: 153.5326; acc: 178155.0/182256.0=0.9775\n","Instance: 14500; Time: 13.32s; loss: 140.8173; acc: 184671.0/188909.0=0.9776\n","Instance: 15000; Time: 12.77s; loss: 138.8948; acc: 191034.0/195396.0=0.9777\n","     Instance: 15473; Time: 12.25s; loss: 113.2322; acc: 196953.0/201422.0=0.9778\n","Epoch: 10 training finished. Time: 404.80s, speed: 38.22st/s,  total loss: 4947.24951171875\n","gold_num =  11703  pred_num =  11823  right_num =  10525\n","Dev: time: 68.15s, speed: 90.97st/s; acc: 0.9745, p: 0.8902, r: 0.8993, f: 0.8948\n","gold_num =  7806  pred_num =  7884  right_num =  6965\n","Test: time: 43.72s, speed: 94.59st/s; acc: 0.9728, p: 0.8834, r: 0.8923, f: 0.8878\n","Epoch: 11/25\n"," Learning rate is setted as: 0.008532001384146894\n","Instance: 500; Time: 13.05s; loss: 130.4958; acc: 6346.0/6465.0=0.9816\n","Instance: 1000; Time: 13.04s; loss: 154.3444; acc: 12771.0/13020.0=0.9809\n","Instance: 1500; Time: 12.45s; loss: 128.5656; acc: 18884.0/19262.0=0.9804\n","Instance: 2000; Time: 13.47s; loss: 184.4083; acc: 25394.0/25935.0=0.9791\n","Instance: 2500; Time: 13.52s; loss: 197.1594; acc: 31999.0/32689.0=0.9789\n","Instance: 3000; Time: 14.00s; loss: 143.9500; acc: 38775.0/39582.0=0.9796\n","Instance: 3500; Time: 12.39s; loss: 132.0298; acc: 44873.0/45800.0=0.9798\n","Instance: 4000; Time: 12.96s; loss: 192.7323; acc: 51229.0/52325.0=0.9791\n","Instance: 4500; Time: 13.18s; loss: 137.1644; acc: 57628.0/58842.0=0.9794\n","Instance: 5000; Time: 13.10s; loss: 154.8986; acc: 64074.0/65424.0=0.9794\n","Instance: 5500; Time: 12.44s; loss: 115.6476; acc: 70249.0/71689.0=0.9799\n","Instance: 6000; Time: 12.80s; loss: 172.6742; acc: 76445.0/78043.0=0.9795\n","Instance: 6500; Time: 12.85s; loss: 124.8978; acc: 82747.0/84455.0=0.9798\n","Instance: 7000; Time: 13.40s; loss: 183.5380; acc: 89264.0/91152.0=0.9793\n","Instance: 7500; Time: 12.35s; loss: 122.3007; acc: 95358.0/97359.0=0.9794\n","Instance: 8000; Time: 12.88s; loss: 124.9620; acc: 101746.0/103863.0=0.9796\n","Instance: 8500; Time: 12.96s; loss: 147.8246; acc: 108046.0/110292.0=0.9796\n","Instance: 9000; Time: 12.87s; loss: 156.1965; acc: 114407.0/116799.0=0.9795\n","Instance: 9500; Time: 14.01s; loss: 140.9200; acc: 121150.0/123667.0=0.9796\n","Instance: 10000; Time: 13.36s; loss: 162.0963; acc: 127635.0/130292.0=0.9796\n","Instance: 10500; Time: 12.97s; loss: 138.0463; acc: 134025.0/136802.0=0.9797\n","Instance: 11000; Time: 13.10s; loss: 121.6007; acc: 140485.0/143363.0=0.9799\n","Instance: 11500; Time: 12.67s; loss: 140.6696; acc: 146722.0/149739.0=0.9799\n","Instance: 12000; Time: 13.48s; loss: 190.2684; acc: 153237.0/156412.0=0.9797\n","Instance: 12500; Time: 12.56s; loss: 133.7241; acc: 159514.0/162808.0=0.9798\n","Instance: 13000; Time: 13.16s; loss: 149.2786; acc: 165903.0/169354.0=0.9796\n","Instance: 13500; Time: 13.17s; loss: 163.8724; acc: 172344.0/175948.0=0.9795\n","Instance: 14000; Time: 12.80s; loss: 147.6534; acc: 178644.0/182391.0=0.9795\n","Instance: 14500; Time: 12.92s; loss: 132.5090; acc: 184969.0/188841.0=0.9795\n","Instance: 15000; Time: 13.43s; loss: 202.4673; acc: 191507.0/195547.0=0.9793\n","     Instance: 15473; Time: 11.91s; loss: 136.1353; acc: 197272.0/201422.0=0.9794\n","Epoch: 11 training finished. Time: 403.24s, speed: 38.37st/s,  total loss: 4663.031463623047\n","gold_num =  11703  pred_num =  11473  right_num =  10494\n","Dev: time: 68.02s, speed: 91.14st/s; acc: 0.9774, p: 0.9147, r: 0.8967, f: 0.9056\n","Exceed previous best f score: 0.9020165460186143\n","gold_num =  7806  pred_num =  7672  right_num =  6972\n","Test: time: 43.98s, speed: 94.09st/s; acc: 0.9759, p: 0.9088, r: 0.8932, f: 0.9009\n","Epoch: 12/25\n"," Learning rate is setted as: 0.00810540131493955\n","Instance: 500; Time: 12.93s; loss: 141.1432; acc: 6362.0/6497.0=0.9792\n","Instance: 1000; Time: 12.82s; loss: 144.7882; acc: 12649.0/12892.0=0.9812\n","Instance: 1500; Time: 12.57s; loss: 146.2213; acc: 18855.0/19224.0=0.9808\n","Instance: 2000; Time: 13.29s; loss: 139.8174; acc: 25361.0/25849.0=0.9811\n","Instance: 2500; Time: 12.41s; loss: 103.1307; acc: 31457.0/32031.0=0.9821\n","Instance: 3000; Time: 13.02s; loss: 166.5505; acc: 37869.0/38575.0=0.9817\n","Instance: 3500; Time: 13.00s; loss: 164.4358; acc: 44220.0/45079.0=0.9809\n","Instance: 4000; Time: 12.87s; loss: 173.9712; acc: 50453.0/51468.0=0.9803\n","Instance: 4500; Time: 13.38s; loss: 154.0721; acc: 56954.0/58096.0=0.9803\n","Instance: 5000; Time: 12.77s; loss: 120.4431; acc: 63210.0/64454.0=0.9807\n","Instance: 5500; Time: 12.99s; loss: 126.0334; acc: 69552.0/70900.0=0.9810\n","Instance: 6000; Time: 13.23s; loss: 112.8262; acc: 76054.0/77519.0=0.9811\n","Instance: 6500; Time: 12.59s; loss: 150.3029; acc: 82285.0/83887.0=0.9809\n","Instance: 7000; Time: 13.40s; loss: 156.6946; acc: 88807.0/90534.0=0.9809\n","Instance: 7500; Time: 12.65s; loss: 137.0874; acc: 94933.0/96806.0=0.9807\n","Instance: 8000; Time: 12.93s; loss: 156.5992; acc: 101291.0/103291.0=0.9806\n","Instance: 8500; Time: 12.92s; loss: 133.8639; acc: 107636.0/109748.0=0.9808\n","Instance: 9000; Time: 12.83s; loss: 133.7626; acc: 113882.0/116143.0=0.9805\n","Instance: 9500; Time: 13.52s; loss: 177.7859; acc: 120516.0/122939.0=0.9803\n","Instance: 10000; Time: 13.24s; loss: 122.5347; acc: 127108.0/129637.0=0.9805\n","Instance: 10500; Time: 13.07s; loss: 141.9224; acc: 133583.0/136242.0=0.9805\n","Instance: 11000; Time: 13.52s; loss: 141.5361; acc: 140224.0/143010.0=0.9805\n","Instance: 11500; Time: 13.02s; loss: 140.2507; acc: 146491.0/149414.0=0.9804\n","Instance: 12000; Time: 12.89s; loss: 165.6046; acc: 152857.0/155926.0=0.9803\n","Instance: 12500; Time: 13.74s; loss: 147.9039; acc: 159554.0/162740.0=0.9804\n","Instance: 13000; Time: 13.19s; loss: 170.7131; acc: 165983.0/169296.0=0.9804\n","Instance: 13500; Time: 13.20s; loss: 144.6853; acc: 172409.0/175846.0=0.9805\n","Instance: 14000; Time: 12.89s; loss: 158.7262; acc: 178598.0/182199.0=0.9802\n","Instance: 14500; Time: 12.55s; loss: 144.1310; acc: 184794.0/188513.0=0.9803\n","Instance: 15000; Time: 13.57s; loss: 172.8905; acc: 191311.0/195168.0=0.9802\n","     Instance: 15473; Time: 12.67s; loss: 134.3087; acc: 197438.0/201422.0=0.9802\n","Epoch: 12 training finished. Time: 403.68s, speed: 38.33st/s,  total loss: 4524.736877441406\n","gold_num =  11703  pred_num =  11387  right_num =  10429\n","Dev: time: 68.08s, speed: 91.07st/s; acc: 0.9772, p: 0.9159, r: 0.8911, f: 0.9033\n","gold_num =  7806  pred_num =  7615  right_num =  6955\n","Test: time: 43.81s, speed: 94.36st/s; acc: 0.9767, p: 0.9133, r: 0.8910, f: 0.9020\n","Epoch: 13/25\n"," Learning rate is setted as: 0.007700131249192572\n","Instance: 500; Time: 12.61s; loss: 91.3252; acc: 6325.0/6394.0=0.9892\n","Instance: 1000; Time: 12.93s; loss: 126.0311; acc: 12609.0/12785.0=0.9862\n","Instance: 1500; Time: 13.25s; loss: 135.6683; acc: 19032.0/19345.0=0.9838\n","Instance: 2000; Time: 13.31s; loss: 145.6619; acc: 25518.0/25952.0=0.9833\n","Instance: 2500; Time: 13.12s; loss: 126.0065; acc: 32008.0/32548.0=0.9834\n","Instance: 3000; Time: 12.82s; loss: 101.4332; acc: 38317.0/38954.0=0.9836\n","Instance: 3500; Time: 12.96s; loss: 148.9419; acc: 44719.0/45473.0=0.9834\n","Instance: 4000; Time: 13.67s; loss: 164.4540; acc: 51321.0/52215.0=0.9829\n","Instance: 4500; Time: 13.50s; loss: 158.6617; acc: 57903.0/58941.0=0.9824\n","Instance: 5000; Time: 12.95s; loss: 148.9574; acc: 64272.0/65450.0=0.9820\n","Instance: 5500; Time: 13.08s; loss: 154.7316; acc: 70686.0/71995.0=0.9818\n","Instance: 6000; Time: 13.49s; loss: 152.4062; acc: 77261.0/78725.0=0.9814\n","Instance: 6500; Time: 13.52s; loss: 127.7372; acc: 83873.0/85461.0=0.9814\n","Instance: 7000; Time: 13.21s; loss: 137.7800; acc: 90320.0/92020.0=0.9815\n","Instance: 7500; Time: 13.35s; loss: 155.4552; acc: 96777.0/98636.0=0.9812\n","Instance: 8000; Time: 12.93s; loss: 122.3839; acc: 103100.0/105056.0=0.9814\n","Instance: 8500; Time: 12.98s; loss: 122.2753; acc: 109488.0/111560.0=0.9814\n","Instance: 9000; Time: 12.21s; loss: 100.2667; acc: 115458.0/117622.0=0.9816\n","Instance: 9500; Time: 12.94s; loss: 137.1354; acc: 121802.0/124085.0=0.9816\n","Instance: 10000; Time: 12.94s; loss: 160.8627; acc: 128229.0/130652.0=0.9815\n","Instance: 10500; Time: 12.87s; loss: 134.6804; acc: 134459.0/137003.0=0.9814\n","Instance: 11000; Time: 13.46s; loss: 150.8162; acc: 141007.0/143702.0=0.9812\n","Instance: 11500; Time: 13.10s; loss: 127.3341; acc: 147373.0/150190.0=0.9812\n","Instance: 12000; Time: 12.99s; loss: 133.8965; acc: 153754.0/156700.0=0.9812\n","Instance: 12500; Time: 12.72s; loss: 161.5864; acc: 159995.0/163101.0=0.9810\n","Instance: 13000; Time: 12.89s; loss: 169.5587; acc: 166280.0/169542.0=0.9808\n","Instance: 13500; Time: 13.12s; loss: 125.3387; acc: 172644.0/176022.0=0.9808\n","Instance: 14000; Time: 12.68s; loss: 145.7570; acc: 178865.0/182371.0=0.9808\n","Instance: 14500; Time: 12.99s; loss: 143.6843; acc: 185245.0/188881.0=0.9807\n","Instance: 15000; Time: 12.92s; loss: 138.8829; acc: 191551.0/195312.0=0.9807\n","     Instance: 15473; Time: 12.19s; loss: 116.6435; acc: 197561.0/201422.0=0.9808\n","Epoch: 13 training finished. Time: 403.73s, speed: 38.33st/s,  total loss: 4266.354217529297\n","gold_num =  11703  pred_num =  11671  right_num =  10590\n","Dev: time: 68.24s, speed: 90.85st/s; acc: 0.9778, p: 0.9074, r: 0.9049, f: 0.9061\n","Exceed previous best f score: 0.9055919917155678\n","gold_num =  7806  pred_num =  7797  right_num =  7012\n","Test: time: 43.90s, speed: 94.28st/s; acc: 0.9754, p: 0.8993, r: 0.8983, f: 0.8988\n","Epoch: 14/25\n"," Learning rate is setted as: 0.007315124686732943\n","Instance: 500; Time: 13.27s; loss: 142.2902; acc: 6466.0/6602.0=0.9794\n","Instance: 1000; Time: 12.71s; loss: 123.0546; acc: 12799.0/13036.0=0.9818\n","Instance: 1500; Time: 13.58s; loss: 120.8733; acc: 19457.0/19794.0=0.9830\n","Instance: 2000; Time: 13.41s; loss: 141.8191; acc: 25979.0/26434.0=0.9828\n","Instance: 2500; Time: 13.07s; loss: 116.4558; acc: 32350.0/32914.0=0.9829\n","Instance: 3000; Time: 12.81s; loss: 129.1063; acc: 38604.0/39275.0=0.9829\n","Instance: 3500; Time: 12.75s; loss: 91.1023; acc: 44943.0/45703.0=0.9834\n","Instance: 4000; Time: 13.40s; loss: 133.6002; acc: 51516.0/52411.0=0.9829\n","Instance: 4500; Time: 12.90s; loss: 115.1304; acc: 57794.0/58791.0=0.9830\n","Instance: 5000; Time: 12.75s; loss: 130.1772; acc: 64044.0/65165.0=0.9828\n","Instance: 5500; Time: 12.73s; loss: 123.5105; acc: 70354.0/71572.0=0.9830\n","Instance: 6000; Time: 13.17s; loss: 115.4346; acc: 76835.0/78149.0=0.9832\n","Instance: 6500; Time: 13.14s; loss: 124.8539; acc: 83269.0/84710.0=0.9830\n","Instance: 7000; Time: 13.36s; loss: 166.8981; acc: 89770.0/91351.0=0.9827\n","Instance: 7500; Time: 12.75s; loss: 135.8927; acc: 96030.0/97724.0=0.9827\n","Instance: 8000; Time: 12.92s; loss: 136.6145; acc: 102370.0/104195.0=0.9825\n","Instance: 8500; Time: 12.77s; loss: 130.0752; acc: 108700.0/110634.0=0.9825\n","Instance: 9000; Time: 13.41s; loss: 168.2960; acc: 115161.0/117235.0=0.9823\n","Instance: 9500; Time: 12.92s; loss: 112.4459; acc: 121482.0/123655.0=0.9824\n","Instance: 10000; Time: 13.13s; loss: 138.7235; acc: 127829.0/130123.0=0.9824\n","Instance: 10500; Time: 12.74s; loss: 138.6901; acc: 134125.0/136531.0=0.9824\n","Instance: 11000; Time: 13.27s; loss: 149.8160; acc: 140511.0/143075.0=0.9821\n","Instance: 11500; Time: 12.75s; loss: 136.2754; acc: 146762.0/149446.0=0.9820\n","Instance: 12000; Time: 13.26s; loss: 102.9973; acc: 153231.0/156010.0=0.9822\n","Instance: 12500; Time: 13.03s; loss: 116.2646; acc: 159647.0/162545.0=0.9822\n","Instance: 13000; Time: 13.27s; loss: 197.2693; acc: 166062.0/169142.0=0.9818\n","Instance: 13500; Time: 12.86s; loss: 139.4519; acc: 172430.0/175622.0=0.9818\n","Instance: 14000; Time: 13.18s; loss: 119.8201; acc: 178860.0/182158.0=0.9819\n","Instance: 14500; Time: 13.29s; loss: 165.8765; acc: 185336.0/188775.0=0.9818\n","Instance: 15000; Time: 12.63s; loss: 157.6295; acc: 191537.0/195129.0=0.9816\n","     Instance: 15473; Time: 12.69s; loss: 123.1069; acc: 197717.0/201422.0=0.9816\n","Epoch: 14 training finished. Time: 403.93s, speed: 38.31st/s,  total loss: 4143.551818847656\n","gold_num =  11703  pred_num =  11299  right_num =  10391\n","Dev: time: 68.07s, speed: 91.09st/s; acc: 0.9775, p: 0.9196, r: 0.8879, f: 0.9035\n","gold_num =  7806  pred_num =  7562  right_num =  6940\n","Test: time: 43.74s, speed: 94.53st/s; acc: 0.9772, p: 0.9177, r: 0.8891, f: 0.9032\n","Epoch: 15/25\n"," Learning rate is setted as: 0.006949368452396296\n","Instance: 500; Time: 13.55s; loss: 128.8182; acc: 6718.0/6832.0=0.9833\n","Instance: 1000; Time: 12.78s; loss: 145.4470; acc: 13029.0/13302.0=0.9795\n","Instance: 1500; Time: 13.57s; loss: 137.2963; acc: 19574.0/19977.0=0.9798\n","Instance: 2000; Time: 12.91s; loss: 129.5781; acc: 25864.0/26386.0=0.9802\n","Instance: 2500; Time: 12.92s; loss: 114.6750; acc: 32188.0/32830.0=0.9804\n","Instance: 3000; Time: 12.43s; loss: 84.1320; acc: 38320.0/39053.0=0.9812\n","Instance: 3500; Time: 13.07s; loss: 95.9778; acc: 44783.0/45605.0=0.9820\n","Instance: 4000; Time: 13.21s; loss: 128.6620; acc: 51335.0/52264.0=0.9822\n","Instance: 4500; Time: 13.58s; loss: 112.0068; acc: 58009.0/59036.0=0.9826\n","Instance: 5000; Time: 13.34s; loss: 130.8541; acc: 64520.0/65691.0=0.9822\n","Instance: 5500; Time: 13.61s; loss: 153.0996; acc: 71101.0/72408.0=0.9819\n","Instance: 6000; Time: 12.53s; loss: 104.9125; acc: 77234.0/78640.0=0.9821\n","Instance: 6500; Time: 13.09s; loss: 172.1543; acc: 83666.0/85208.0=0.9819\n","Instance: 7000; Time: 13.11s; loss: 160.1844; acc: 90097.0/91767.0=0.9818\n","Instance: 7500; Time: 13.11s; loss: 125.0609; acc: 96576.0/98358.0=0.9819\n","Instance: 8000; Time: 13.22s; loss: 128.7322; acc: 102984.0/104882.0=0.9819\n","Instance: 8500; Time: 13.33s; loss: 151.4451; acc: 109526.0/111549.0=0.9819\n","Instance: 9000; Time: 12.43s; loss: 118.4628; acc: 115751.0/117867.0=0.9820\n","Instance: 9500; Time: 12.74s; loss: 119.0319; acc: 121977.0/124203.0=0.9821\n","Instance: 10000; Time: 13.14s; loss: 163.6812; acc: 128396.0/130770.0=0.9818\n","Instance: 10500; Time: 12.81s; loss: 121.7141; acc: 134660.0/137153.0=0.9818\n","Instance: 11000; Time: 13.29s; loss: 132.1918; acc: 141083.0/143701.0=0.9818\n","Instance: 11500; Time: 12.97s; loss: 164.8848; acc: 147380.0/150164.0=0.9815\n","Instance: 12000; Time: 12.88s; loss: 116.5244; acc: 153763.0/156640.0=0.9816\n","Instance: 12500; Time: 12.72s; loss: 121.5580; acc: 159973.0/162965.0=0.9816\n","Instance: 13000; Time: 12.98s; loss: 154.5028; acc: 166316.0/169454.0=0.9815\n","Instance: 13500; Time: 12.76s; loss: 126.0061; acc: 172600.0/175848.0=0.9815\n","Instance: 14000; Time: 13.09s; loss: 124.2308; acc: 179026.0/182388.0=0.9816\n","Instance: 14500; Time: 13.13s; loss: 103.7393; acc: 185458.0/188925.0=0.9816\n","Instance: 15000; Time: 13.19s; loss: 130.0610; acc: 191895.0/195469.0=0.9817\n","     Instance: 15473; Time: 11.74s; loss: 111.7956; acc: 197756.0/201422.0=0.9818\n","Epoch: 15 training finished. Time: 403.21s, speed: 38.37st/s,  total loss: 4011.4208374023438\n","gold_num =  11703  pred_num =  11428  right_num =  10542\n","Dev: time: 68.09s, speed: 91.05st/s; acc: 0.9793, p: 0.9225, r: 0.9008, f: 0.9115\n","Exceed previous best f score: 0.9061350218191153\n","gold_num =  7806  pred_num =  7648  right_num =  7020\n","Test: time: 43.82s, speed: 94.45st/s; acc: 0.9781, p: 0.9179, r: 0.8993, f: 0.9085\n","Epoch: 16/25\n"," Learning rate is setted as: 0.00660190002977648\n","Instance: 500; Time: 13.12s; loss: 131.0369; acc: 6493.0/6612.0=0.9820\n","Instance: 1000; Time: 13.12s; loss: 114.8967; acc: 12925.0/13159.0=0.9822\n","Instance: 1500; Time: 13.28s; loss: 124.3245; acc: 19447.0/19776.0=0.9834\n","Instance: 2000; Time: 13.05s; loss: 119.3818; acc: 25797.0/26214.0=0.9841\n","Instance: 2500; Time: 13.33s; loss: 116.8275; acc: 32374.0/32905.0=0.9839\n","Instance: 3000; Time: 12.77s; loss: 119.4847; acc: 38588.0/39232.0=0.9836\n","Instance: 3500; Time: 13.09s; loss: 123.7728; acc: 45058.0/45799.0=0.9838\n","Instance: 4000; Time: 12.80s; loss: 132.1829; acc: 51358.0/52198.0=0.9839\n","Instance: 4500; Time: 12.87s; loss: 126.8820; acc: 57611.0/58577.0=0.9835\n","Instance: 5000; Time: 13.29s; loss: 150.9556; acc: 64153.0/65244.0=0.9833\n","Instance: 5500; Time: 13.64s; loss: 138.9069; acc: 70783.0/71998.0=0.9831\n","Instance: 6000; Time: 12.89s; loss: 139.0859; acc: 77082.0/78429.0=0.9828\n","Instance: 6500; Time: 13.08s; loss: 111.3584; acc: 83518.0/84978.0=0.9828\n","Instance: 7000; Time: 12.75s; loss: 114.6034; acc: 89831.0/91391.0=0.9829\n","Instance: 7500; Time: 13.14s; loss: 109.3401; acc: 96313.0/97971.0=0.9831\n","Instance: 8000; Time: 12.71s; loss: 125.1125; acc: 102577.0/104355.0=0.9830\n","Instance: 8500; Time: 13.40s; loss: 121.2346; acc: 109088.0/110991.0=0.9829\n","Instance: 9000; Time: 12.76s; loss: 124.8934; acc: 115376.0/117391.0=0.9828\n","Instance: 9500; Time: 12.69s; loss: 127.7352; acc: 121607.0/123735.0=0.9828\n","Instance: 10000; Time: 12.87s; loss: 127.6539; acc: 127931.0/130174.0=0.9828\n","Instance: 10500; Time: 12.79s; loss: 139.3125; acc: 134229.0/136607.0=0.9826\n","Instance: 11000; Time: 12.96s; loss: 126.5375; acc: 140547.0/143035.0=0.9826\n","Instance: 11500; Time: 13.39s; loss: 126.9590; acc: 147058.0/149662.0=0.9826\n","Instance: 12000; Time: 12.63s; loss: 136.4459; acc: 153209.0/155935.0=0.9825\n","Instance: 12500; Time: 13.07s; loss: 130.9208; acc: 159593.0/162429.0=0.9825\n","Instance: 13000; Time: 13.21s; loss: 116.4792; acc: 166073.0/169011.0=0.9826\n","Instance: 13500; Time: 13.25s; loss: 109.2310; acc: 172556.0/175600.0=0.9827\n","Instance: 14000; Time: 12.67s; loss: 97.9425; acc: 178775.0/181901.0=0.9828\n","Instance: 14500; Time: 13.29s; loss: 154.3920; acc: 185307.0/188594.0=0.9826\n","Instance: 15000; Time: 13.54s; loss: 147.7092; acc: 191890.0/195292.0=0.9826\n","     Instance: 15473; Time: 12.43s; loss: 118.9922; acc: 197924.0/201422.0=0.9826\n","Epoch: 16 training finished. Time: 403.88s, speed: 38.31st/s,  total loss: 3904.591552734375\n","gold_num =  11703  pred_num =  11611  right_num =  10625\n","Dev: time: 68.20s, speed: 90.91st/s; acc: 0.9790, p: 0.9151, r: 0.9079, f: 0.9115\n","gold_num =  7806  pred_num =  7758  right_num =  7077\n","Test: time: 44.00s, speed: 93.98st/s; acc: 0.9780, p: 0.9122, r: 0.9066, f: 0.9094\n","Epoch: 17/25\n"," Learning rate is setted as: 0.006271805028287656\n","Instance: 500; Time: 13.17s; loss: 124.2759; acc: 6492.0/6599.0=0.9838\n","Instance: 1000; Time: 12.98s; loss: 119.0649; acc: 12881.0/13105.0=0.9829\n","Instance: 1500; Time: 12.93s; loss: 117.3475; acc: 19249.0/19565.0=0.9838\n","Instance: 2000; Time: 13.36s; loss: 131.9341; acc: 25729.0/26161.0=0.9835\n","Instance: 2500; Time: 12.80s; loss: 91.0919; acc: 32048.0/32574.0=0.9839\n","Instance: 3000; Time: 12.92s; loss: 97.4498; acc: 38338.0/38954.0=0.9842\n","Instance: 3500; Time: 13.04s; loss: 122.0386; acc: 44754.0/45471.0=0.9842\n","Instance: 4000; Time: 12.91s; loss: 91.4811; acc: 51137.0/51943.0=0.9845\n","Instance: 4500; Time: 12.50s; loss: 73.5460; acc: 57321.0/58184.0=0.9852\n","Instance: 5000; Time: 13.59s; loss: 106.4086; acc: 63972.0/64937.0=0.9851\n","Instance: 5500; Time: 12.85s; loss: 131.1991; acc: 70257.0/71336.0=0.9849\n","Instance: 6000; Time: 13.01s; loss: 97.6144; acc: 76809.0/77981.0=0.9850\n","Instance: 6500; Time: 12.72s; loss: 101.3776; acc: 83032.0/84303.0=0.9849\n","Instance: 7000; Time: 13.08s; loss: 118.4086; acc: 89535.0/90906.0=0.9849\n","Instance: 7500; Time: 12.84s; loss: 115.4484; acc: 95808.0/97261.0=0.9851\n","Instance: 8000; Time: 13.09s; loss: 102.1282; acc: 102264.0/103795.0=0.9852\n","Instance: 8500; Time: 13.08s; loss: 113.3422; acc: 108655.0/110277.0=0.9853\n","Instance: 9000; Time: 12.66s; loss: 108.5220; acc: 114848.0/116588.0=0.9851\n","Instance: 9500; Time: 13.05s; loss: 139.0256; acc: 121249.0/123111.0=0.9849\n","Instance: 10000; Time: 13.57s; loss: 126.1046; acc: 127842.0/129820.0=0.9848\n","Instance: 10500; Time: 12.89s; loss: 119.5679; acc: 134097.0/136221.0=0.9844\n","Instance: 11000; Time: 13.02s; loss: 117.1875; acc: 140510.0/142751.0=0.9843\n","Instance: 11500; Time: 12.81s; loss: 81.0883; acc: 146936.0/149243.0=0.9845\n","Instance: 12000; Time: 13.01s; loss: 111.2173; acc: 153246.0/155657.0=0.9845\n","Instance: 12500; Time: 12.99s; loss: 128.8719; acc: 159635.0/162155.0=0.9845\n","Instance: 13000; Time: 13.33s; loss: 183.2872; acc: 166100.0/168813.0=0.9839\n","Instance: 13500; Time: 13.00s; loss: 108.8591; acc: 172455.0/175271.0=0.9839\n","Instance: 14000; Time: 13.33s; loss: 117.0597; acc: 179048.0/181956.0=0.9840\n","Instance: 14500; Time: 13.51s; loss: 149.2130; acc: 185533.0/188592.0=0.9838\n","Instance: 15000; Time: 13.13s; loss: 115.2594; acc: 192036.0/195190.0=0.9838\n","     Instance: 15473; Time: 12.73s; loss: 127.8905; acc: 198140.0/201422.0=0.9837\n","Epoch: 17 training finished. Time: 403.91s, speed: 38.31st/s,  total loss: 3587.310760498047\n","gold_num =  11703  pred_num =  11588  right_num =  10628\n","Dev: time: 68.09s, speed: 91.05st/s; acc: 0.9792, p: 0.9172, r: 0.9081, f: 0.9126\n","Exceed previous best f score: 0.9115040421944577\n","gold_num =  7806  pred_num =  7740  right_num =  7095\n","Test: time: 43.78s, speed: 94.51st/s; acc: 0.9787, p: 0.9167, r: 0.9089, f: 0.9128\n","Epoch: 18/25\n"," Learning rate is setted as: 0.005958214776873273\n","Instance: 500; Time: 12.54s; loss: 118.3551; acc: 6234.0/6368.0=0.9790\n","Instance: 1000; Time: 13.19s; loss: 114.0831; acc: 12767.0/12986.0=0.9831\n","Instance: 1500; Time: 12.65s; loss: 97.5477; acc: 18995.0/19311.0=0.9836\n","Instance: 2000; Time: 12.50s; loss: 83.3958; acc: 25193.0/25569.0=0.9853\n","Instance: 2500; Time: 13.19s; loss: 112.3921; acc: 31730.0/32218.0=0.9849\n","Instance: 3000; Time: 12.98s; loss: 127.1371; acc: 38177.0/38799.0=0.9840\n","Instance: 3500; Time: 13.15s; loss: 116.1072; acc: 44696.0/45412.0=0.9842\n","Instance: 4000; Time: 13.21s; loss: 114.9573; acc: 51132.0/51946.0=0.9843\n","Instance: 4500; Time: 12.71s; loss: 114.0804; acc: 57365.0/58267.0=0.9845\n","Instance: 5000; Time: 12.77s; loss: 104.9681; acc: 63629.0/64623.0=0.9846\n","Instance: 5500; Time: 12.95s; loss: 88.8054; acc: 70049.0/71135.0=0.9847\n","Instance: 6000; Time: 13.00s; loss: 125.9863; acc: 76400.0/77608.0=0.9844\n","Instance: 6500; Time: 13.16s; loss: 97.6760; acc: 82888.0/84184.0=0.9846\n","Instance: 7000; Time: 12.82s; loss: 105.9945; acc: 89230.0/90614.0=0.9847\n","Instance: 7500; Time: 13.03s; loss: 129.4073; acc: 95636.0/97148.0=0.9844\n","Instance: 8000; Time: 12.42s; loss: 70.6281; acc: 101798.0/103377.0=0.9847\n","Instance: 8500; Time: 13.25s; loss: 109.8982; acc: 108301.0/109989.0=0.9847\n","Instance: 9000; Time: 13.23s; loss: 113.8647; acc: 114817.0/116606.0=0.9847\n","Instance: 9500; Time: 13.48s; loss: 122.5931; acc: 121398.0/123287.0=0.9847\n","Instance: 10000; Time: 13.21s; loss: 156.8622; acc: 127808.0/129853.0=0.9843\n","Instance: 10500; Time: 13.16s; loss: 119.9897; acc: 134248.0/136383.0=0.9843\n","Instance: 11000; Time: 12.72s; loss: 107.5748; acc: 140599.0/142824.0=0.9844\n","Instance: 11500; Time: 12.84s; loss: 149.9861; acc: 146846.0/149217.0=0.9841\n","Instance: 12000; Time: 13.39s; loss: 128.7980; acc: 153416.0/155910.0=0.9840\n","Instance: 12500; Time: 12.63s; loss: 115.5139; acc: 159626.0/162230.0=0.9839\n","Instance: 13000; Time: 12.91s; loss: 149.8746; acc: 165986.0/168739.0=0.9837\n","Instance: 13500; Time: 13.80s; loss: 147.1257; acc: 172660.0/175542.0=0.9836\n","Instance: 14000; Time: 13.05s; loss: 116.8514; acc: 179015.0/181983.0=0.9837\n","Instance: 14500; Time: 13.33s; loss: 142.2737; acc: 185575.0/188664.0=0.9836\n","Instance: 15000; Time: 13.57s; loss: 118.8251; acc: 192214.0/195422.0=0.9836\n","     Instance: 15473; Time: 12.18s; loss: 122.2632; acc: 198105.0/201422.0=0.9835\n","Epoch: 18 training finished. Time: 403.03s, speed: 38.39st/s,  total loss: 3643.8162231445312\n","gold_num =  11703  pred_num =  11562  right_num =  10631\n","Dev: time: 68.09s, speed: 91.05st/s; acc: 0.9797, p: 0.9195, r: 0.9084, f: 0.9139\n","Exceed previous best f score: 0.9126271950538835\n","gold_num =  7806  pred_num =  7737  right_num =  7085\n","Test: time: 43.95s, speed: 94.16st/s; acc: 0.9786, p: 0.9157, r: 0.9076, f: 0.9117\n","Epoch: 19/25\n"," Learning rate is setted as: 0.005660304038029609\n","Instance: 500; Time: 13.20s; loss: 124.3752; acc: 6555.0/6645.0=0.9865\n","Instance: 1000; Time: 13.39s; loss: 115.0488; acc: 13168.0/13366.0=0.9852\n","Instance: 1500; Time: 12.61s; loss: 81.9108; acc: 19360.0/19632.0=0.9861\n","Instance: 2000; Time: 12.75s; loss: 84.0170; acc: 25635.0/25973.0=0.9870\n","Instance: 2500; Time: 13.03s; loss: 96.5123; acc: 32015.0/32446.0=0.9867\n","Instance: 3000; Time: 13.41s; loss: 97.4132; acc: 38563.0/39100.0=0.9863\n","Instance: 3500; Time: 13.75s; loss: 103.2723; acc: 45274.0/45901.0=0.9863\n","Instance: 4000; Time: 12.77s; loss: 112.8495; acc: 51672.0/52379.0=0.9865\n","Instance: 4500; Time: 12.85s; loss: 121.5056; acc: 57980.0/58783.0=0.9863\n","Instance: 5000; Time: 12.89s; loss: 117.6855; acc: 64321.0/65225.0=0.9861\n","Instance: 5500; Time: 13.22s; loss: 126.7344; acc: 70766.0/71782.0=0.9858\n","Instance: 6000; Time: 12.87s; loss: 112.0787; acc: 77074.0/78195.0=0.9857\n","Instance: 6500; Time: 12.75s; loss: 144.7858; acc: 83380.0/84641.0=0.9851\n","Instance: 7000; Time: 12.51s; loss: 91.1495; acc: 89633.0/90950.0=0.9855\n","Instance: 7500; Time: 12.86s; loss: 114.9604; acc: 95994.0/97427.0=0.9853\n","Instance: 8000; Time: 12.85s; loss: 120.3455; acc: 102236.0/103782.0=0.9851\n","Instance: 8500; Time: 13.14s; loss: 147.6461; acc: 108645.0/110322.0=0.9848\n","Instance: 9000; Time: 13.69s; loss: 151.4182; acc: 115209.0/117040.0=0.9844\n","Instance: 9500; Time: 13.11s; loss: 83.1346; acc: 121601.0/123506.0=0.9846\n","Instance: 10000; Time: 13.09s; loss: 113.5229; acc: 128078.0/130080.0=0.9846\n","Instance: 10500; Time: 12.89s; loss: 106.3799; acc: 134437.0/136527.0=0.9847\n","Instance: 11000; Time: 13.17s; loss: 126.0341; acc: 140905.0/143117.0=0.9845\n","Instance: 11500; Time: 12.81s; loss: 66.8256; acc: 147209.0/149475.0=0.9848\n","Instance: 12000; Time: 12.93s; loss: 134.9811; acc: 153573.0/155979.0=0.9846\n","Instance: 12500; Time: 13.26s; loss: 120.0427; acc: 160045.0/162554.0=0.9846\n","Instance: 13000; Time: 12.95s; loss: 114.5862; acc: 166482.0/169086.0=0.9846\n","Instance: 13500; Time: 13.06s; loss: 133.1415; acc: 172879.0/175610.0=0.9844\n","Instance: 14000; Time: 12.77s; loss: 98.9971; acc: 179198.0/182024.0=0.9845\n","Instance: 14500; Time: 12.96s; loss: 112.0120; acc: 185610.0/188531.0=0.9845\n","Instance: 15000; Time: 13.15s; loss: 125.5558; acc: 192053.0/195089.0=0.9844\n","     Instance: 15473; Time: 12.96s; loss: 112.3289; acc: 198267.0/201422.0=0.9843\n","Epoch: 19 training finished. Time: 403.65s, speed: 38.33st/s,  total loss: 3511.2512817382812\n","gold_num =  11703  pred_num =  11480  right_num =  10594\n","Dev: time: 68.14s, speed: 90.98st/s; acc: 0.9798, p: 0.9228, r: 0.9052, f: 0.9139\n","Exceed previous best f score: 0.9139050075220287\n","gold_num =  7806  pred_num =  7681  right_num =  7054\n","Test: time: 44.12s, speed: 93.83st/s; acc: 0.9783, p: 0.9184, r: 0.9037, f: 0.9110\n","Epoch: 20/25\n"," Learning rate is setted as: 0.005377288836128128\n","Instance: 500; Time: 12.95s; loss: 98.1440; acc: 6412.0/6506.0=0.9856\n","Instance: 1000; Time: 12.72s; loss: 114.5122; acc: 12665.0/12856.0=0.9851\n","Instance: 1500; Time: 13.03s; loss: 86.8972; acc: 19120.0/19377.0=0.9867\n","Instance: 2000; Time: 13.10s; loss: 107.1351; acc: 25600.0/25959.0=0.9862\n","Instance: 2500; Time: 12.82s; loss: 98.9899; acc: 31984.0/32433.0=0.9862\n","Instance: 3000; Time: 13.15s; loss: 121.8993; acc: 38393.0/38955.0=0.9856\n","Instance: 3500; Time: 13.27s; loss: 93.0304; acc: 44950.0/45579.0=0.9862\n","Instance: 4000; Time: 13.18s; loss: 143.5615; acc: 51238.0/52006.0=0.9852\n","Instance: 4500; Time: 13.20s; loss: 135.6451; acc: 57690.0/58590.0=0.9846\n","Instance: 5000; Time: 12.81s; loss: 118.0094; acc: 63952.0/64947.0=0.9847\n","Instance: 5500; Time: 13.02s; loss: 104.9054; acc: 70361.0/71430.0=0.9850\n","Instance: 6000; Time: 13.86s; loss: 128.3816; acc: 77088.0/78273.0=0.9849\n","Instance: 6500; Time: 13.11s; loss: 106.7775; acc: 83573.0/84846.0=0.9850\n","Instance: 7000; Time: 12.66s; loss: 105.4033; acc: 89856.0/91217.0=0.9851\n","Instance: 7500; Time: 12.76s; loss: 116.4672; acc: 96062.0/97531.0=0.9849\n","Instance: 8000; Time: 13.38s; loss: 101.2310; acc: 102679.0/104240.0=0.9850\n","Instance: 8500; Time: 12.96s; loss: 94.7283; acc: 109038.0/110694.0=0.9850\n","Instance: 9000; Time: 12.64s; loss: 101.1398; acc: 115299.0/117038.0=0.9851\n","Instance: 9500; Time: 12.90s; loss: 114.1034; acc: 121697.0/123534.0=0.9851\n","Instance: 10000; Time: 13.33s; loss: 128.4534; acc: 128260.0/130200.0=0.9851\n","Instance: 10500; Time: 13.07s; loss: 114.5482; acc: 134713.0/136762.0=0.9850\n","Instance: 11000; Time: 12.86s; loss: 88.6116; acc: 141016.0/143147.0=0.9851\n","Instance: 11500; Time: 13.12s; loss: 97.1604; acc: 147454.0/149660.0=0.9853\n","Instance: 12000; Time: 12.93s; loss: 85.6870; acc: 153927.0/156191.0=0.9855\n","Instance: 12500; Time: 13.03s; loss: 149.3237; acc: 160344.0/162729.0=0.9853\n","Instance: 13000; Time: 14.01s; loss: 162.8865; acc: 167072.0/169636.0=0.9849\n","Instance: 13500; Time: 13.04s; loss: 108.4259; acc: 173484.0/176138.0=0.9849\n","Instance: 14000; Time: 13.16s; loss: 122.0916; acc: 179865.0/182619.0=0.9849\n","Instance: 14500; Time: 13.13s; loss: 83.1816; acc: 186329.0/189164.0=0.9850\n","Instance: 15000; Time: 12.41s; loss: 95.1055; acc: 192472.0/195413.0=0.9849\n","     Instance: 15473; Time: 12.11s; loss: 110.8052; acc: 198379.0/201422.0=0.9849\n","Epoch: 20 training finished. Time: 403.73s, speed: 38.33st/s,  total loss: 3437.2421264648438\n","gold_num =  11703  pred_num =  11616  right_num =  10654\n","Dev: time: 68.14s, speed: 90.99st/s; acc: 0.9793, p: 0.9172, r: 0.9104, f: 0.9138\n","gold_num =  7806  pred_num =  7760  right_num =  7106\n","Test: time: 43.65s, speed: 94.73st/s; acc: 0.9785, p: 0.9157, r: 0.9103, f: 0.9130\n","Epoch: 21/25\n"," Learning rate is setted as: 0.005108424394321722\n","Instance: 500; Time: 13.39s; loss: 106.3724; acc: 6638.0/6724.0=0.9872\n","Instance: 1000; Time: 12.92s; loss: 111.7994; acc: 12933.0/13128.0=0.9851\n","Instance: 1500; Time: 13.13s; loss: 141.1597; acc: 19420.0/19718.0=0.9849\n","Instance: 2000; Time: 12.93s; loss: 104.7458; acc: 25808.0/26195.0=0.9852\n","Instance: 2500; Time: 12.66s; loss: 112.7032; acc: 32194.0/32684.0=0.9850\n","Instance: 3000; Time: 12.89s; loss: 93.7205; acc: 38515.0/39093.0=0.9852\n","Instance: 3500; Time: 12.69s; loss: 107.3020; acc: 44829.0/45511.0=0.9850\n","Instance: 4000; Time: 13.53s; loss: 96.3289; acc: 51491.0/52249.0=0.9855\n","Instance: 4500; Time: 13.58s; loss: 116.0222; acc: 58149.0/59021.0=0.9852\n","Instance: 5000; Time: 13.01s; loss: 97.4880; acc: 64594.0/65558.0=0.9853\n","Instance: 5500; Time: 13.04s; loss: 80.9408; acc: 71011.0/72048.0=0.9856\n","Instance: 6000; Time: 13.26s; loss: 110.6589; acc: 77417.0/78567.0=0.9854\n","Instance: 6500; Time: 13.11s; loss: 89.9148; acc: 83835.0/85066.0=0.9855\n","Instance: 7000; Time: 13.13s; loss: 111.6968; acc: 90315.0/91653.0=0.9854\n","Instance: 7500; Time: 13.13s; loss: 86.9208; acc: 96816.0/98239.0=0.9855\n","Instance: 8000; Time: 12.93s; loss: 119.2805; acc: 103152.0/104708.0=0.9851\n","Instance: 8500; Time: 13.19s; loss: 127.4899; acc: 109583.0/111243.0=0.9851\n","Instance: 9000; Time: 13.39s; loss: 138.4376; acc: 116072.0/117860.0=0.9848\n","Instance: 9500; Time: 12.66s; loss: 94.0015; acc: 122258.0/124119.0=0.9850\n","Instance: 10000; Time: 12.66s; loss: 107.0680; acc: 128541.0/130504.0=0.9850\n","Instance: 10500; Time: 12.51s; loss: 113.7249; acc: 134812.0/136874.0=0.9849\n","Instance: 11000; Time: 13.20s; loss: 116.9948; acc: 141143.0/143319.0=0.9848\n","Instance: 11500; Time: 13.15s; loss: 103.8926; acc: 147607.0/149860.0=0.9850\n","Instance: 12000; Time: 12.58s; loss: 91.6758; acc: 153839.0/156169.0=0.9851\n","Instance: 12500; Time: 12.76s; loss: 109.6934; acc: 160212.0/162619.0=0.9852\n","Instance: 13000; Time: 12.83s; loss: 103.9652; acc: 166530.0/169011.0=0.9853\n","Instance: 13500; Time: 13.20s; loss: 133.7666; acc: 173024.0/175635.0=0.9851\n","Instance: 14000; Time: 13.30s; loss: 140.0624; acc: 179505.0/182238.0=0.9850\n","Instance: 14500; Time: 13.18s; loss: 114.1111; acc: 185929.0/188764.0=0.9850\n","Instance: 15000; Time: 13.89s; loss: 105.4290; acc: 192450.0/195377.0=0.9850\n","     Instance: 15473; Time: 12.76s; loss: 81.4810; acc: 198435.0/201422.0=0.9852\n","Epoch: 21 training finished. Time: 404.57s, speed: 38.25st/s,  total loss: 3368.8482666015625\n","gold_num =  11703  pred_num =  11448  right_num =  10597\n","Dev: time: 69.13s, speed: 89.68st/s; acc: 0.9801, p: 0.9257, r: 0.9055, f: 0.9155\n","Exceed previous best f score: 0.9139455635595047\n","gold_num =  7806  pred_num =  7657  right_num =  7080\n","Test: time: 43.85s, speed: 94.36st/s; acc: 0.9798, p: 0.9246, r: 0.9070, f: 0.9157\n","Epoch: 22/25\n"," Learning rate is setted as: 0.004853003174605635\n","Instance: 500; Time: 12.65s; loss: 105.1134; acc: 6276.0/6376.0=0.9843\n","Instance: 1000; Time: 12.91s; loss: 98.1188; acc: 12663.0/12864.0=0.9844\n","Instance: 1500; Time: 13.01s; loss: 107.2428; acc: 19024.0/19328.0=0.9843\n","Instance: 2000; Time: 12.97s; loss: 105.3650; acc: 25405.0/25805.0=0.9845\n","Instance: 2500; Time: 13.04s; loss: 102.9283; acc: 31885.0/32380.0=0.9847\n","Instance: 3000; Time: 13.04s; loss: 113.6976; acc: 38238.0/38822.0=0.9850\n","Instance: 3500; Time: 12.64s; loss: 82.5283; acc: 44529.0/45186.0=0.9855\n","Instance: 4000; Time: 13.09s; loss: 83.4333; acc: 50970.0/51701.0=0.9859\n","Instance: 4500; Time: 13.47s; loss: 103.1382; acc: 57544.0/58368.0=0.9859\n","Instance: 5000; Time: 13.29s; loss: 107.8932; acc: 64064.0/64966.0=0.9861\n","Instance: 5500; Time: 13.41s; loss: 131.4482; acc: 70599.0/71625.0=0.9857\n","Instance: 6000; Time: 12.86s; loss: 128.3503; acc: 76966.0/78100.0=0.9855\n","Instance: 6500; Time: 13.79s; loss: 128.1797; acc: 83679.0/84921.0=0.9854\n","Instance: 7000; Time: 13.63s; loss: 112.9532; acc: 90284.0/91626.0=0.9854\n","Instance: 7500; Time: 13.16s; loss: 103.2633; acc: 96704.0/98147.0=0.9853\n","Instance: 8000; Time: 12.70s; loss: 142.3069; acc: 102967.0/104549.0=0.9849\n","Instance: 8500; Time: 13.22s; loss: 123.8705; acc: 109444.0/111155.0=0.9846\n","Instance: 9000; Time: 12.92s; loss: 108.2599; acc: 115769.0/117582.0=0.9846\n","Instance: 9500; Time: 12.71s; loss: 128.7910; acc: 122045.0/123966.0=0.9845\n","Instance: 10000; Time: 13.11s; loss: 135.4806; acc: 128530.0/130566.0=0.9844\n","Instance: 10500; Time: 13.20s; loss: 120.6562; acc: 135015.0/137146.0=0.9845\n","Instance: 11000; Time: 12.72s; loss: 103.6981; acc: 141375.0/143594.0=0.9845\n","Instance: 11500; Time: 13.33s; loss: 130.9928; acc: 147863.0/150197.0=0.9845\n","Instance: 12000; Time: 12.80s; loss: 89.5734; acc: 154186.0/156606.0=0.9845\n","Instance: 12500; Time: 12.83s; loss: 107.5082; acc: 160514.0/163005.0=0.9847\n","Instance: 13000; Time: 13.17s; loss: 103.5787; acc: 166935.0/169529.0=0.9847\n","Instance: 13500; Time: 13.06s; loss: 126.1853; acc: 173338.0/176046.0=0.9846\n","Instance: 14000; Time: 12.73s; loss: 84.4524; acc: 179618.0/182385.0=0.9848\n","Instance: 14500; Time: 13.15s; loss: 93.3331; acc: 186085.0/188936.0=0.9849\n","Instance: 15000; Time: 13.03s; loss: 86.9941; acc: 192535.0/195464.0=0.9850\n","     Instance: 15473; Time: 11.97s; loss: 98.4601; acc: 198404.0/201422.0=0.9850\n","Epoch: 22 training finished. Time: 403.61s, speed: 38.34st/s,  total loss: 3397.7952270507812\n","gold_num =  11703  pred_num =  11548  right_num =  10650\n","Dev: time: 68.09s, speed: 91.06st/s; acc: 0.9799, p: 0.9222, r: 0.9100, f: 0.9161\n","Exceed previous best f score: 0.9154680143406332\n","gold_num =  7806  pred_num =  7719  right_num =  7105\n","Test: time: 43.85s, speed: 94.36st/s; acc: 0.9793, p: 0.9205, r: 0.9102, f: 0.9153\n","Epoch: 23/25\n"," Learning rate is setted as: 0.004610353015875353\n","Instance: 500; Time: 12.98s; loss: 113.9326; acc: 6461.0/6554.0=0.9858\n","Instance: 1000; Time: 13.03s; loss: 85.4806; acc: 12902.0/13052.0=0.9885\n","Instance: 1500; Time: 12.86s; loss: 107.6147; acc: 19239.0/19506.0=0.9863\n","Instance: 2000; Time: 13.07s; loss: 86.2629; acc: 25661.0/26015.0=0.9864\n","Instance: 2500; Time: 13.02s; loss: 85.3348; acc: 32146.0/32583.0=0.9866\n","Instance: 3000; Time: 12.89s; loss: 88.3441; acc: 38546.0/39061.0=0.9868\n","Instance: 3500; Time: 12.96s; loss: 69.9547; acc: 44889.0/45490.0=0.9868\n","Instance: 4000; Time: 13.43s; loss: 120.0046; acc: 51481.0/52178.0=0.9866\n","Instance: 4500; Time: 12.74s; loss: 108.2225; acc: 57828.0/58603.0=0.9868\n","Instance: 5000; Time: 13.14s; loss: 119.5612; acc: 64246.0/65126.0=0.9865\n","Instance: 5500; Time: 13.32s; loss: 93.2606; acc: 70833.0/71785.0=0.9867\n","Instance: 6000; Time: 13.79s; loss: 96.9672; acc: 77573.0/78593.0=0.9870\n","Instance: 6500; Time: 12.79s; loss: 101.3561; acc: 83829.0/84936.0=0.9870\n","Instance: 7000; Time: 12.79s; loss: 104.8499; acc: 90107.0/91302.0=0.9869\n","Instance: 7500; Time: 12.95s; loss: 114.0291; acc: 96510.0/97795.0=0.9869\n","Instance: 8000; Time: 13.22s; loss: 102.2898; acc: 103008.0/104384.0=0.9868\n","Instance: 8500; Time: 12.73s; loss: 91.2334; acc: 109282.0/110741.0=0.9868\n","Instance: 9000; Time: 13.27s; loss: 103.6554; acc: 115779.0/117317.0=0.9869\n","Instance: 9500; Time: 13.11s; loss: 136.6713; acc: 122271.0/123925.0=0.9867\n","Instance: 10000; Time: 13.52s; loss: 88.3302; acc: 128854.0/130587.0=0.9867\n","Instance: 10500; Time: 12.98s; loss: 111.1720; acc: 135236.0/137067.0=0.9866\n","Instance: 11000; Time: 12.96s; loss: 112.9972; acc: 141687.0/143621.0=0.9865\n","Instance: 11500; Time: 13.03s; loss: 86.5533; acc: 148119.0/150141.0=0.9865\n","Instance: 12000; Time: 13.11s; loss: 121.5184; acc: 154488.0/156625.0=0.9864\n","Instance: 12500; Time: 13.15s; loss: 78.7103; acc: 161025.0/163232.0=0.9865\n","Instance: 13000; Time: 12.64s; loss: 78.9211; acc: 167192.0/169478.0=0.9865\n","Instance: 13500; Time: 12.77s; loss: 80.4429; acc: 173535.0/175887.0=0.9866\n","Instance: 14000; Time: 12.74s; loss: 86.0410; acc: 179833.0/182238.0=0.9868\n","Instance: 14500; Time: 12.88s; loss: 101.6166; acc: 186164.0/188684.0=0.9866\n","Instance: 15000; Time: 12.86s; loss: 103.0265; acc: 192586.0/195208.0=0.9866\n","     Instance: 15473; Time: 12.42s; loss: 118.2679; acc: 198696.0/201422.0=0.9865\n","Epoch: 23 training finished. Time: 403.15s, speed: 38.38st/s,  total loss: 3096.623046875\n","gold_num =  11703  pred_num =  11537  right_num =  10656\n","Dev: time: 68.06s, speed: 91.12st/s; acc: 0.9804, p: 0.9236, r: 0.9105, f: 0.9170\n","Exceed previous best f score: 0.9160896305535245\n","gold_num =  7806  pred_num =  7713  right_num =  7077\n","Test: time: 43.97s, speed: 94.12st/s; acc: 0.9785, p: 0.9175, r: 0.9066, f: 0.9120\n","Epoch: 24/25\n"," Learning rate is setted as: 0.004379835365081586\n","Instance: 500; Time: 13.11s; loss: 77.3274; acc: 6479.0/6580.0=0.9847\n","Instance: 1000; Time: 13.26s; loss: 93.9421; acc: 13030.0/13217.0=0.9859\n","Instance: 1500; Time: 13.54s; loss: 117.7493; acc: 19608.0/19890.0=0.9858\n","Instance: 2000; Time: 12.82s; loss: 91.7233; acc: 25888.0/26252.0=0.9861\n","Instance: 2500; Time: 12.44s; loss: 77.4764; acc: 32070.0/32499.0=0.9868\n","Instance: 3000; Time: 12.99s; loss: 91.7875; acc: 38442.0/38952.0=0.9869\n","Instance: 3500; Time: 12.94s; loss: 94.8143; acc: 44823.0/45424.0=0.9868\n","Instance: 4000; Time: 12.60s; loss: 79.1788; acc: 51143.0/51818.0=0.9870\n","Instance: 4500; Time: 12.63s; loss: 87.8116; acc: 57440.0/58198.0=0.9870\n","Instance: 5000; Time: 13.32s; loss: 101.5372; acc: 63930.0/64764.0=0.9871\n","Instance: 5500; Time: 12.49s; loss: 93.3394; acc: 70137.0/71057.0=0.9871\n","Instance: 6000; Time: 13.70s; loss: 118.4498; acc: 76807.0/77838.0=0.9868\n","Instance: 6500; Time: 13.83s; loss: 94.7588; acc: 83507.0/84616.0=0.9869\n","Instance: 7000; Time: 13.16s; loss: 103.3157; acc: 89937.0/91124.0=0.9870\n","Instance: 7500; Time: 13.20s; loss: 92.6711; acc: 96427.0/97697.0=0.9870\n","Instance: 8000; Time: 13.45s; loss: 102.4430; acc: 103005.0/104372.0=0.9869\n","Instance: 8500; Time: 13.23s; loss: 109.4138; acc: 109536.0/111007.0=0.9867\n","Instance: 9000; Time: 13.26s; loss: 104.9948; acc: 116075.0/117648.0=0.9866\n","Instance: 9500; Time: 13.64s; loss: 102.6624; acc: 122764.0/124421.0=0.9867\n","Instance: 10000; Time: 13.55s; loss: 112.3766; acc: 129406.0/131176.0=0.9865\n","Instance: 10500; Time: 12.73s; loss: 82.4779; acc: 135718.0/137579.0=0.9865\n","Instance: 11000; Time: 12.89s; loss: 115.1675; acc: 142098.0/144066.0=0.9863\n","Instance: 11500; Time: 12.92s; loss: 135.5557; acc: 148415.0/150501.0=0.9861\n","Instance: 12000; Time: 13.07s; loss: 88.3850; acc: 154783.0/156954.0=0.9862\n","Instance: 12500; Time: 13.12s; loss: 105.0031; acc: 161329.0/163590.0=0.9862\n","Instance: 13000; Time: 12.78s; loss: 65.1923; acc: 167686.0/169998.0=0.9864\n","Instance: 13500; Time: 12.82s; loss: 66.0778; acc: 173859.0/176219.0=0.9866\n","Instance: 14000; Time: 12.51s; loss: 87.7916; acc: 180030.0/182490.0=0.9865\n","Instance: 14500; Time: 12.59s; loss: 79.9432; acc: 186263.0/188791.0=0.9866\n","Instance: 15000; Time: 12.89s; loss: 91.5017; acc: 192597.0/195218.0=0.9866\n","     Instance: 15473; Time: 12.62s; loss: 92.9900; acc: 198711.0/201422.0=0.9865\n","Epoch: 24 training finished. Time: 404.07s, speed: 38.29st/s,  total loss: 2957.8590393066406\n","gold_num =  11703  pred_num =  11461  right_num =  10634\n","Dev: time: 68.33s, speed: 90.74st/s; acc: 0.9807, p: 0.9278, r: 0.9087, f: 0.9181\n","Exceed previous best f score: 0.9170395869191051\n","gold_num =  7806  pred_num =  7671  right_num =  7073\n","Test: time: 43.86s, speed: 94.35st/s; acc: 0.9791, p: 0.9220, r: 0.9061, f: 0.9140\n"]}]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='Tuning with bi-directional LSTM-CRF')\n","    parser.add_argument('--embedding',  help='Embedding for words', default='None')\n","    parser.add_argument('--status', choices=['train', 'test', 'decode'], help='update algorithm', default='train')\n","    #parser.add_argument('--savemodel', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/2-222-saved_model2.lstmcrf\")\n","    #parser.add_argument('--savedset', help='Dir of saved data setting', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/save-222.dset\")\n","    parser.add_argument('--savemodel', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/lattice-lstm_model_final.model\")\n","    parser.add_argument('--savedset', help='Dir of saved data setting', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/model-train-data_final.dset\")\n","    #parser.add_argument('--train', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/train.char.bmes\")\n","    #parser.add_argument('--dev', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/dev.char.bmes\" )\n","    #parser.add_argument('--test', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/test.char.bmes\")\n","\n","    parser.add_argument('--train', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/train.txt\")\n","    parser.add_argument('--dev', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/dev.txt\" )\n","    parser.add_argument('--test', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/test.txt\")\n","\n","    #parser.add_argument('--train', default=\"test_data/fyz.train.embs\")\n","    #parser.add_argument('--dev', default=\"test_data/fyz.dev.embs\")\n","    #parser.add_argument('--test', default=\"test_data/fyz.test.embs\")\n","    parser.add_argument('--seg', default=\"True\") \n","    parser.add_argument('--extendalphabet', default=\"True\") \n","    parser.add_argument('--raw') \n","    #parser.add_argument('--loadmodel',default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/2-222-saved_model2.lstmcrf\")\n","    parser.add_argument('--loadmodel',default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/lattice-lstm_model_final.model\")\n","    parser.add_argument('--output') \n","    args = parser.parse_args()\n","   \n","    train_file = args.train\n","    dev_file = args.dev\n","    test_file = args.test\n","    raw_file = args.raw\n","    model_dir = args.loadmodel\n","    dset_dir = args.savedset\n","    output_file = args.output\n","    if args.seg.lower() == \"true\":\n","        seg = True \n","    else:\n","        seg = False\n","    status = args.status.lower()\n","\n","    save_model_dir = args.savemodel\n","    gpu = torch.cuda.is_available()\n","\n","    char_emb = \"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/gigaword_chn.all.a2b.uni.ite50.vec\"\n","    #char_emb = \"/content/drive/MyDrive/lattcie ner/TCM_NER-master/TCM_NER-master/data/sgns.sikuquanshu.vec\"\n","    bichar_emb = '/content/drive/MyDrive/lattcie ner/TCM_NER-master/TCM_NER-master/data/gigaword_chn.all.a2b.bi.ite50.vec'\n","    #bichar_emb = \"/content/drive/MyDrive/lattcie ner/TCM_NER-master/TCM_NER-master/data/sgns.sikuquanshu.bigram.vec\"\n","    gaz_file = \"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/ctb.50d.vec\"\n","    # gaz_file = None\n","    # char_emb = None\n","    #bichar_emb = None\n","\n","    print (\"CuDNN:\", torch.backends.cudnn.enabled)\n","    # gpu = False\n","    print (\"GPU available:\", gpu)\n","    print (\"Status:\", status)\n","    print (\"Seg: \", seg)\n","    print (\"Train file:\", train_file)\n","    print (\"Dev file:\", dev_file)\n","    print (\"Test file:\", test_file)\n","    print (\"Raw file:\", raw_file)\n","    print (\"Char emb:\", char_emb)\n","    print (\"Bichar emb:\", bichar_emb)\n","    print (\"Gaz file:\",gaz_file)\n","    if status == 'train':\n","        print (\"Model saved to:\", save_model_dir)\n","    sys.stdout.flush()\n","    \n","    if status == 'train':\n","        data = Data()\n","        data.HP_gpu = gpu\n","        data.HP_use_char = True\n","        data.HP_batch_size = 10\n","        data.use_bigram = True\n","        data.gaz_dropout = 0.5\n","        data.norm_gaz_emb = False\n","        data.HP_fix_gaz_emb = False\n","        data_initialization(data, gaz_file, train_file, dev_file, test_file)\n","\n","        data.generate_instance_with_gaz(train_file,'train')\n","        data.generate_instance_with_gaz(dev_file,'dev')\n","        data.generate_instance_with_gaz(test_file,'test')\n","\n","        data.build_word_pretrain_emb(char_emb)\n","        data.build_biword_pretrain_emb(bichar_emb)\n","        data.build_gaz_pretrain_emb(gaz_file)\n","        #data = load_data_setting(dset_dir)\n","        model,train_log,dev_log = train(data, save_model_dir,dset_dir, seg)\n","    elif status == 'test':      \n","        data = load_data_setting(dset_dir)\n","        data.generate_instance_with_gaz(dev_file,'dev')\n","        load_model_decode(model_dir, data , 'dev', gpu, seg)\n","        data.generate_instance_with_gaz(test_file,'test')\n","        load_model_decode(model_dir, data, 'test', gpu, seg)\n","    elif status == 'decode':       \n","        data = load_data_setting(dset_dir)\n","        data.generate_instance_with_gaz(raw_file,'raw')\n","        decode_results = load_model_decode(model_dir, data, 'raw', gpu, seg)\n","        data.write_decoded_results(output_file, decode_results, 'raw')\n","    else:\n","        print (\"Invalid argument! Please use valid arguments! (train/test/decode)\")\n","\n","torch.save(model,\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/321final.model\")\n","\n","\n","'''filename='/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/test_log.json'\n","with open(filename,'w',encoding='utf-8') as file_obj:\n","    json.dump(test_log,file_obj,ensure_ascii=False,indent = 4)'''\n","\n","filename='/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/train_log.json'\n","with open(filename,'w',encoding='utf-8') as file_obj:\n","    json.dump(train_log,file_obj,ensure_ascii=False,indent = 4)\n","\n","filename='/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/dev_log.json'\n","with open(filename,'w',encoding='utf-8') as file_obj:\n","    json.dump(dev_log,file_obj,ensure_ascii=False,indent = 4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o0VgW03vZldh","executionInfo":{"status":"ok","timestamp":1648029145068,"user_tz":-480,"elapsed":10508505,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15214000089252266369"}},"outputId":"813f076c-5de9-4ed1-a68c-ef7d0277deda"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["CuDNN: True\n","GPU available: True\n","Status: train\n","Seg:  True\n","Train file: /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/train.txt\n","Dev file: /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/dev.txt\n","Test file: /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/test.txt\n","Raw file: None\n","Char emb: /content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/gigaword_chn.all.a2b.uni.ite50.vec\n","Bichar emb: /content/drive/MyDrive/lattcie ner/TCM_NER-master/TCM_NER-master/data/gigaword_chn.all.a2b.bi.ite50.vec\n","Gaz file: /content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/ctb.50d.vec\n","Model saved to: /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/lattice-lstm_model_final.model\n","Load gaz file:  /content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/ctb.50d.vec  total size: 704368\n","gaz alphabet size: 4255\n","gaz alphabet size: 4474\n","gaz alphabet size: 4632\n","build word pretrain emb...\n","Embedding:\n","     pretrain word:11327, prefect match:1696, case_match:0, oov:35, oov%:0.02020785219399538\n","build biword pretrain emb...\n","Embedding:\n","     pretrain word:3986686, prefect match:16297, case_match:0, oov:13721, oov%:0.4570771844498484\n","build gaz pretrain emb...\n","Embedding:\n","     pretrain word:704368, prefect match:4630, case_match:0, oov:1, oov%:0.0002158894645941278\n","Training model...\n","DATA SUMMARY START:\n","     Tag          scheme: BIO\n","     MAX SENTENCE LENGTH: 250\n","     MAX   WORD   LENGTH: -1\n","     Number   normalized: True\n","     Use          bigram: True\n","     Word  alphabet size: 1732\n","     Biword alphabet size: 30019\n","     Char  alphabet size: 1732\n","     Gaz   alphabet size: 4632\n","     Label alphabet size: 8\n","     Word embedding size: 50\n","     Biword embedding size: 50\n","     Char embedding size: 30\n","     Gaz embedding size: 50\n","     Norm     word   emb: True\n","     Norm     biword emb: True\n","     Norm     gaz    emb: False\n","     Norm   gaz  dropout: 0.5\n","     Train instance number: 15473\n","     Dev   instance number: 6189\n","     Test  instance number: 4127\n","     Raw   instance number: 0\n","     Hyperpara  iteration: 100\n","     Hyperpara  batch size: 10\n","     Hyperpara          lr: 0.015\n","     Hyperpara    lr_decay: 0.05\n","     Hyperpara     HP_clip: 5.0\n","     Hyperpara    momentum: 0\n","     Hyperpara  hidden_dim: 200\n","     Hyperpara     dropout: 0.5\n","     Hyperpara  lstm_layer: 1\n","     Hyperpara      bilstm: True\n","     Hyperpara         GPU: True\n","     Hyperpara     use_gaz: True\n","     Hyperpara fix gaz emb: False\n","     Hyperpara    use_char: True\n","             Char_features: LSTM\n","DATA SUMMARY END.\n","Data setting saved to file:  /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/model-train-data_final.dset\n","build batched lstmcrf...\n","build batched bilstm...\n","build batched char bilstm...\n","build LatticeLSTM...  forward , Fix emb: False  gaz drop: 0.5\n","load pretrain word emb... (4632, 50)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:105: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n","  init.orthogonal(self.weight_ih.data)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:106: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n","  init.orthogonal(self.alpha_weight_ih.data)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:118: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  init.constant(self.bias.data, val=0)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:119: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  init.constant(self.alpha_bias.data, val=0)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:37: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n","  init.orthogonal(self.weight_ih.data)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:43: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  init.constant(self.bias.data, val=0)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["build LatticeLSTM...  backward , Fix emb: False  gaz drop: 0.5\n","load pretrain word emb... (4632, 50)\n","build batched crf...\n","finished built model.\n","Epoch: 0/25\n"," Learning rate is setted as: 0.015\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:94: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:30.)\n","  masked_cur_partition = cur_partition.masked_select(mask_idx)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:99: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:447.)\n","  partition.masked_scatter_(mask_idx, masked_cur_partition)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:243: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:30.)\n","  tg_energy = tg_energy.masked_select(mask.transpose(1,0))\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:156: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n","  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Instance: 500; Time: 13.99s; loss: 5004.6923; acc: 3819.0/6556.0=0.5825\n","Instance: 1000; Time: 13.18s; loss: 1793.5605; acc: 9200.0/12971.0=0.7093\n","Instance: 1500; Time: 13.05s; loss: 1230.3523; acc: 14756.0/19401.0=0.7606\n","Instance: 2000; Time: 13.55s; loss: 1058.6588; acc: 20653.0/26115.0=0.7908\n","Instance: 2500; Time: 12.93s; loss: 752.6873; acc: 26385.0/32435.0=0.8135\n","Instance: 3000; Time: 13.53s; loss: 772.8881; acc: 32415.0/39071.0=0.8296\n","Instance: 3500; Time: 13.39s; loss: 768.8669; acc: 38373.0/45660.0=0.8404\n","Instance: 4000; Time: 13.22s; loss: 724.7758; acc: 44312.0/52218.0=0.8486\n","Instance: 4500; Time: 13.00s; loss: 644.7211; acc: 50133.0/58628.0=0.8551\n","Instance: 5000; Time: 13.46s; loss: 686.8496; acc: 56096.0/65238.0=0.8599\n","Instance: 5500; Time: 13.68s; loss: 615.2045; acc: 62236.0/71898.0=0.8656\n","Instance: 6000; Time: 13.55s; loss: 590.0018; acc: 68370.0/78579.0=0.8701\n","Instance: 6500; Time: 13.16s; loss: 639.2688; acc: 74299.0/85014.0=0.8740\n","Instance: 7000; Time: 12.99s; loss: 553.1881; acc: 80252.0/91456.0=0.8775\n","Instance: 7500; Time: 13.67s; loss: 513.1675; acc: 86454.0/98090.0=0.8814\n","Instance: 8000; Time: 16.00s; loss: 546.8046; acc: 92576.0/104664.0=0.8845\n","Instance: 8500; Time: 13.12s; loss: 446.9177; acc: 98532.0/111001.0=0.8877\n","Instance: 9000; Time: 13.09s; loss: 475.3160; acc: 104546.0/117404.0=0.8905\n","Instance: 9500; Time: 13.10s; loss: 493.3141; acc: 110489.0/123810.0=0.8924\n","Instance: 10000; Time: 13.20s; loss: 435.1243; acc: 116610.0/130315.0=0.8948\n","Instance: 10500; Time: 13.23s; loss: 473.6856; acc: 122633.0/136724.0=0.8969\n","Instance: 11000; Time: 13.23s; loss: 437.1501; acc: 128679.0/143136.0=0.8990\n","Instance: 11500; Time: 13.17s; loss: 435.9291; acc: 134746.0/149581.0=0.9008\n","Instance: 12000; Time: 13.26s; loss: 482.5770; acc: 140730.0/156018.0=0.9020\n","Instance: 12500; Time: 13.29s; loss: 427.1022; acc: 146820.0/162492.0=0.9036\n","Instance: 13000; Time: 12.74s; loss: 363.9755; acc: 152730.0/168751.0=0.9051\n","Instance: 13500; Time: 13.54s; loss: 462.2267; acc: 159026.0/175419.0=0.9065\n","Instance: 14000; Time: 13.34s; loss: 465.5001; acc: 165172.0/181982.0=0.9076\n","Instance: 14500; Time: 13.68s; loss: 468.0162; acc: 171465.0/188664.0=0.9088\n","Instance: 15000; Time: 13.84s; loss: 430.0005; acc: 177734.0/195318.0=0.9100\n","     Instance: 15473; Time: 12.79s; loss: 435.1172; acc: 183649.0/201595.0=0.9110\n","Epoch: 0 training finished. Time: 414.95s, speed: 37.29st/s,  total loss: 23627.640579223633\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["__main__:185: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:186: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:187: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:188: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:208: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:263: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  word_var = autograd.Variable(torch.LongTensor(skip_input_[t][0]),volatile =  volatile_flag)\n"]},{"output_type":"stream","name":"stdout","text":["gold_num =  11549  pred_num =  11515  right_num =  9474\n","Dev: time: 66.90s, speed: 92.67st/s; acc: 0.9563, p: 0.8228, r: 0.8203, f: 0.8215\n","Exceed previous best f score: -1\n","gold_num =  7933  pred_num =  7930  right_num =  6392\n","Test: time: 47.08s, speed: 90.37st/s; acc: 0.9522, p: 0.8061, r: 0.8057, f: 0.8059\n","Epoch: 1/25\n"," Learning rate is setted as: 0.014249999999999999\n","Instance: 500; Time: 13.70s; loss: 450.0065; acc: 6353.0/6767.0=0.9388\n","Instance: 1000; Time: 13.31s; loss: 410.7344; acc: 12560.0/13357.0=0.9403\n","Instance: 1500; Time: 12.69s; loss: 315.0593; acc: 18459.0/19528.0=0.9453\n","Instance: 2000; Time: 12.77s; loss: 306.2908; acc: 24498.0/25854.0=0.9476\n","Instance: 2500; Time: 13.45s; loss: 349.0530; acc: 30784.0/32430.0=0.9492\n","Instance: 3000; Time: 13.27s; loss: 374.0533; acc: 36996.0/38967.0=0.9494\n","Instance: 3500; Time: 13.97s; loss: 391.3718; acc: 43391.0/45699.0=0.9495\n","Instance: 4000; Time: 13.37s; loss: 412.0422; acc: 49627.0/52272.0=0.9494\n","Instance: 4500; Time: 13.55s; loss: 343.4323; acc: 56005.0/58935.0=0.9503\n","Instance: 5000; Time: 12.94s; loss: 339.4441; acc: 62097.0/65301.0=0.9509\n","Instance: 5500; Time: 12.80s; loss: 287.6408; acc: 68132.0/71575.0=0.9519\n","Instance: 6000; Time: 13.51s; loss: 362.0751; acc: 74449.0/78197.0=0.9521\n","Instance: 6500; Time: 12.47s; loss: 262.8325; acc: 80316.0/84280.0=0.9530\n","Instance: 7000; Time: 13.39s; loss: 349.5499; acc: 86549.0/90778.0=0.9534\n","Instance: 7500; Time: 13.42s; loss: 369.2349; acc: 92837.0/97390.0=0.9532\n","Instance: 8000; Time: 14.23s; loss: 364.6345; acc: 99440.0/104278.0=0.9536\n","Instance: 8500; Time: 13.25s; loss: 329.1952; acc: 105585.0/110699.0=0.9538\n","Instance: 9000; Time: 13.08s; loss: 360.2667; acc: 111669.0/117102.0=0.9536\n","Instance: 9500; Time: 14.03s; loss: 385.6491; acc: 118127.0/123924.0=0.9532\n","Instance: 10000; Time: 13.21s; loss: 334.5078; acc: 124419.0/130529.0=0.9532\n","Instance: 10500; Time: 13.34s; loss: 337.7085; acc: 130620.0/137039.0=0.9532\n","Instance: 11000; Time: 13.51s; loss: 326.4235; acc: 136900.0/143616.0=0.9532\n","Instance: 11500; Time: 13.49s; loss: 378.7225; acc: 143187.0/150218.0=0.9532\n","Instance: 12000; Time: 12.68s; loss: 320.3463; acc: 149180.0/156485.0=0.9533\n","Instance: 12500; Time: 13.11s; loss: 261.9373; acc: 155407.0/162927.0=0.9538\n","Instance: 13000; Time: 12.87s; loss: 312.0139; acc: 161462.0/169274.0=0.9538\n","Instance: 13500; Time: 13.63s; loss: 318.4521; acc: 167803.0/175903.0=0.9540\n","Instance: 14000; Time: 13.30s; loss: 333.6438; acc: 174040.0/182441.0=0.9540\n","Instance: 14500; Time: 13.44s; loss: 309.7981; acc: 180209.0/188867.0=0.9542\n","Instance: 15000; Time: 13.40s; loss: 333.4928; acc: 186507.0/195465.0=0.9542\n","     Instance: 15473; Time: 12.63s; loss: 253.5294; acc: 192424.0/201595.0=0.9545\n","Epoch: 1 training finished. Time: 411.81s, speed: 37.57st/s,  total loss: 10583.142547607422\n","gold_num =  11549  pred_num =  10919  right_num =  9708\n","Dev: time: 69.73s, speed: 88.92st/s; acc: 0.9694, p: 0.8891, r: 0.8406, f: 0.8642\n","Exceed previous best f score: 0.8215400624349636\n","gold_num =  7933  pred_num =  7489  right_num =  6562\n","Test: time: 45.62s, speed: 90.71st/s; acc: 0.9655, p: 0.8762, r: 0.8272, f: 0.8510\n","Epoch: 2/25\n"," Learning rate is setted as: 0.0135375\n","Instance: 500; Time: 12.79s; loss: 281.4272; acc: 6179.0/6415.0=0.9632\n","Instance: 1000; Time: 13.56s; loss: 298.0392; acc: 12574.0/13065.0=0.9624\n","Instance: 1500; Time: 13.02s; loss: 274.3383; acc: 18813.0/19513.0=0.9641\n","Instance: 2000; Time: 13.29s; loss: 280.7230; acc: 25162.0/26094.0=0.9643\n","Instance: 2500; Time: 12.52s; loss: 237.4707; acc: 31168.0/32307.0=0.9647\n","Instance: 3000; Time: 13.62s; loss: 252.1818; acc: 37553.0/38902.0=0.9653\n","Instance: 3500; Time: 13.31s; loss: 222.9964; acc: 43952.0/45503.0=0.9659\n","Instance: 4000; Time: 12.84s; loss: 233.2083; acc: 50032.0/51797.0=0.9659\n","Instance: 4500; Time: 12.98s; loss: 226.3866; acc: 56266.0/58214.0=0.9665\n","Instance: 5000; Time: 13.58s; loss: 303.0881; acc: 62736.0/64924.0=0.9663\n","Instance: 5500; Time: 13.29s; loss: 265.4118; acc: 68951.0/71374.0=0.9661\n","Instance: 6000; Time: 14.08s; loss: 308.5833; acc: 75509.0/78174.0=0.9659\n","Instance: 6500; Time: 12.69s; loss: 258.7565; acc: 81555.0/84436.0=0.9659\n","Instance: 7000; Time: 12.86s; loss: 236.6908; acc: 87671.0/90773.0=0.9658\n","Instance: 7500; Time: 13.03s; loss: 242.5397; acc: 93831.0/97148.0=0.9659\n","Instance: 8000; Time: 13.15s; loss: 231.2981; acc: 100098.0/103615.0=0.9661\n","Instance: 8500; Time: 13.36s; loss: 269.6078; acc: 106556.0/110246.0=0.9665\n","Instance: 9000; Time: 12.80s; loss: 242.9995; acc: 112739.0/116635.0=0.9666\n","Instance: 9500; Time: 13.08s; loss: 229.8854; acc: 119031.0/123121.0=0.9668\n","Instance: 10000; Time: 13.53s; loss: 278.5219; acc: 125455.0/129752.0=0.9669\n","Instance: 10500; Time: 13.15s; loss: 215.0314; acc: 131646.0/136121.0=0.9671\n","Instance: 11000; Time: 14.09s; loss: 291.3516; acc: 138218.0/142965.0=0.9668\n","Instance: 11500; Time: 13.16s; loss: 239.5227; acc: 144495.0/149462.0=0.9668\n","Instance: 12000; Time: 13.98s; loss: 281.3040; acc: 151013.0/156225.0=0.9666\n","Instance: 12500; Time: 13.63s; loss: 262.4314; acc: 157555.0/162979.0=0.9667\n","Instance: 13000; Time: 13.39s; loss: 255.7756; acc: 163861.0/169522.0=0.9666\n","Instance: 13500; Time: 12.74s; loss: 217.4757; acc: 169952.0/175777.0=0.9669\n","Instance: 14000; Time: 13.24s; loss: 203.7839; acc: 176278.0/182289.0=0.9670\n","Instance: 14500; Time: 13.48s; loss: 268.0959; acc: 182573.0/188837.0=0.9668\n","Instance: 15000; Time: 13.04s; loss: 217.3866; acc: 188779.0/195227.0=0.9670\n","     Instance: 15473; Time: 13.25s; loss: 253.5944; acc: 194916.0/201595.0=0.9669\n","Epoch: 2 training finished. Time: 410.55s, speed: 37.69st/s,  total loss: 7879.907531738281\n","gold_num =  11549  pred_num =  10965  right_num =  9951\n","Dev: time: 69.81s, speed: 88.81st/s; acc: 0.9743, p: 0.9075, r: 0.8616, f: 0.8840\n","Exceed previous best f score: 0.8641623642513798\n","gold_num =  7933  pred_num =  7522  right_num =  6746\n","Test: time: 46.03s, speed: 89.90st/s; acc: 0.9710, p: 0.8968, r: 0.8504, f: 0.8730\n","Epoch: 3/25\n"," Learning rate is setted as: 0.012860624999999997\n","Instance: 500; Time: 13.49s; loss: 280.1289; acc: 6415.0/6662.0=0.9629\n","Instance: 1000; Time: 13.15s; loss: 172.2375; acc: 12697.0/13087.0=0.9702\n","Instance: 1500; Time: 12.99s; loss: 174.9011; acc: 18858.0/19425.0=0.9708\n","Instance: 2000; Time: 13.56s; loss: 169.0070; acc: 25286.0/26000.0=0.9725\n","Instance: 2500; Time: 12.70s; loss: 176.4086; acc: 31518.0/32367.0=0.9738\n","Instance: 3000; Time: 13.61s; loss: 218.1083; acc: 37991.0/39022.0=0.9736\n","Instance: 3500; Time: 13.09s; loss: 202.3049; acc: 44293.0/45497.0=0.9735\n","Instance: 4000; Time: 13.10s; loss: 174.4915; acc: 50550.0/51915.0=0.9737\n","Instance: 4500; Time: 13.44s; loss: 215.6015; acc: 56896.0/58467.0=0.9731\n","Instance: 5000; Time: 13.42s; loss: 167.9915; acc: 63337.0/65062.0=0.9735\n","Instance: 5500; Time: 13.24s; loss: 224.4537; acc: 69662.0/71586.0=0.9731\n","Instance: 6000; Time: 13.52s; loss: 175.8120; acc: 76019.0/78109.0=0.9732\n","Instance: 6500; Time: 13.41s; loss: 207.6558; acc: 82425.0/84696.0=0.9732\n","Instance: 7000; Time: 13.14s; loss: 200.3585; acc: 88655.0/91110.0=0.9731\n","Instance: 7500; Time: 13.30s; loss: 247.0983; acc: 94972.0/97623.0=0.9728\n","Instance: 8000; Time: 12.62s; loss: 154.6996; acc: 101060.0/103833.0=0.9733\n","Instance: 8500; Time: 13.47s; loss: 248.5707; acc: 107484.0/110488.0=0.9728\n","Instance: 9000; Time: 13.81s; loss: 193.4698; acc: 114035.0/117196.0=0.9730\n","Instance: 9500; Time: 12.79s; loss: 169.5706; acc: 120222.0/123526.0=0.9733\n","Instance: 10000; Time: 13.12s; loss: 214.1862; acc: 126521.0/129997.0=0.9733\n","Instance: 10500; Time: 13.73s; loss: 196.1750; acc: 133066.0/136712.0=0.9733\n","Instance: 11000; Time: 13.36s; loss: 187.8746; acc: 139464.0/143245.0=0.9736\n","Instance: 11500; Time: 13.20s; loss: 204.4619; acc: 145759.0/149721.0=0.9735\n","Instance: 12000; Time: 13.37s; loss: 190.6827; acc: 152082.0/156203.0=0.9736\n","Instance: 12500; Time: 12.73s; loss: 149.6720; acc: 158183.0/162449.0=0.9737\n","Instance: 13000; Time: 12.83s; loss: 110.6217; acc: 164410.0/168786.0=0.9741\n","Instance: 13500; Time: 13.10s; loss: 137.2283; acc: 170681.0/175170.0=0.9744\n","Instance: 14000; Time: 13.85s; loss: 166.3735; acc: 177292.0/181937.0=0.9745\n","Instance: 14500; Time: 13.94s; loss: 239.3929; acc: 183850.0/188712.0=0.9742\n","Instance: 15000; Time: 13.68s; loss: 193.1801; acc: 190371.0/195399.0=0.9743\n","     Instance: 15473; Time: 12.80s; loss: 160.4560; acc: 196427.0/201595.0=0.9744\n","Epoch: 3 training finished. Time: 411.56s, speed: 37.60st/s,  total loss: 5923.174652099609\n","gold_num =  11549  pred_num =  11349  right_num =  10504\n","Dev: time: 69.49s, speed: 89.22st/s; acc: 0.9810, p: 0.9255, r: 0.9095, f: 0.9175\n","Exceed previous best f score: 0.8839832992804478\n","gold_num =  7933  pred_num =  7797  right_num =  7128\n","Test: time: 45.11s, speed: 91.74st/s; acc: 0.9780, p: 0.9142, r: 0.8985, f: 0.9063\n","Epoch: 4/25\n"," Learning rate is setted as: 0.012217593749999998\n","Instance: 500; Time: 13.04s; loss: 192.5165; acc: 6429.0/6620.0=0.9711\n","Instance: 1000; Time: 13.30s; loss: 148.8470; acc: 12846.0/13167.0=0.9756\n","Instance: 1500; Time: 12.88s; loss: 152.2146; acc: 19023.0/19474.0=0.9768\n","Instance: 2000; Time: 13.90s; loss: 159.7969; acc: 25540.0/26135.0=0.9772\n","Instance: 2500; Time: 12.73s; loss: 139.5240; acc: 31668.0/32377.0=0.9781\n","Instance: 3000; Time: 13.37s; loss: 134.5907; acc: 38116.0/38947.0=0.9787\n","Instance: 3500; Time: 14.02s; loss: 141.7699; acc: 44731.0/45670.0=0.9794\n","Instance: 4000; Time: 13.08s; loss: 141.6752; acc: 51083.0/52135.0=0.9798\n","Instance: 4500; Time: 13.35s; loss: 156.5945; acc: 57436.0/58628.0=0.9797\n","Instance: 5000; Time: 13.14s; loss: 113.5878; acc: 63799.0/65090.0=0.9802\n","Instance: 5500; Time: 13.90s; loss: 148.8643; acc: 70439.0/71851.0=0.9803\n","Instance: 6000; Time: 13.32s; loss: 177.7037; acc: 76861.0/78426.0=0.9800\n","Instance: 6500; Time: 13.61s; loss: 147.0072; acc: 83403.0/85083.0=0.9803\n","Instance: 7000; Time: 13.16s; loss: 152.5087; acc: 89740.0/91558.0=0.9801\n","Instance: 7500; Time: 13.09s; loss: 128.0811; acc: 95943.0/97865.0=0.9804\n","Instance: 8000; Time: 13.36s; loss: 150.8320; acc: 102286.0/104336.0=0.9804\n","Instance: 8500; Time: 14.37s; loss: 168.8936; acc: 109064.0/111251.0=0.9803\n","Instance: 9000; Time: 13.06s; loss: 136.0245; acc: 115380.0/117656.0=0.9807\n","Instance: 9500; Time: 13.37s; loss: 111.2007; acc: 121844.0/124209.0=0.9810\n","Instance: 10000; Time: 13.82s; loss: 182.9757; acc: 128479.0/130993.0=0.9808\n","Instance: 10500; Time: 14.02s; loss: 169.6384; acc: 135192.0/137860.0=0.9806\n","Instance: 11000; Time: 13.32s; loss: 117.7971; acc: 141640.0/144395.0=0.9809\n","Instance: 11500; Time: 12.52s; loss: 98.9054; acc: 147725.0/150561.0=0.9812\n","Instance: 12000; Time: 13.24s; loss: 130.5907; acc: 154059.0/157013.0=0.9812\n","Instance: 12500; Time: 13.74s; loss: 152.2500; acc: 160620.0/163703.0=0.9812\n","Instance: 13000; Time: 12.98s; loss: 118.3396; acc: 166939.0/170131.0=0.9812\n","Instance: 13500; Time: 12.61s; loss: 130.5504; acc: 172981.0/176312.0=0.9811\n","Instance: 14000; Time: 13.07s; loss: 134.0105; acc: 179317.0/182764.0=0.9811\n","Instance: 14500; Time: 13.05s; loss: 121.6836; acc: 185648.0/189204.0=0.9812\n","Instance: 15000; Time: 13.04s; loss: 142.2424; acc: 191919.0/195596.0=0.9812\n","     Instance: 15473; Time: 12.39s; loss: 123.1260; acc: 197825.0/201595.0=0.9813\n","Epoch: 4 training finished. Time: 411.84s, speed: 37.57st/s,  total loss: 4424.342712402344\n","gold_num =  11549  pred_num =  11396  right_num =  10659\n","Dev: time: 69.43s, speed: 89.30st/s; acc: 0.9836, p: 0.9353, r: 0.9229, f: 0.9291\n","Exceed previous best f score: 0.9174600401781814\n","gold_num =  7933  pred_num =  7845  right_num =  7275\n","Test: time: 45.43s, speed: 91.10st/s; acc: 0.9816, p: 0.9273, r: 0.9171, f: 0.9222\n","Epoch: 5/25\n"," Learning rate is setted as: 0.011606714062499995\n","Instance: 500; Time: 13.25s; loss: 104.0804; acc: 6502.0/6590.0=0.9866\n","Instance: 1000; Time: 13.33s; loss: 111.9807; acc: 13042.0/13205.0=0.9877\n","Instance: 1500; Time: 13.28s; loss: 112.1781; acc: 19481.0/19742.0=0.9868\n","Instance: 2000; Time: 13.02s; loss: 97.4220; acc: 25905.0/26250.0=0.9869\n","Instance: 2500; Time: 13.48s; loss: 95.9066; acc: 32495.0/32931.0=0.9868\n","Instance: 3000; Time: 12.64s; loss: 134.7272; acc: 38690.0/39249.0=0.9858\n","Instance: 3500; Time: 13.79s; loss: 125.9933; acc: 45362.0/46033.0=0.9854\n","Instance: 4000; Time: 13.09s; loss: 139.5887; acc: 51672.0/52465.0=0.9849\n","Instance: 4500; Time: 13.45s; loss: 140.2875; acc: 58186.0/59081.0=0.9849\n","Instance: 5000; Time: 12.99s; loss: 93.0190; acc: 64559.0/65545.0=0.9850\n","Instance: 5500; Time: 13.23s; loss: 98.0961; acc: 70917.0/71982.0=0.9852\n","Instance: 6000; Time: 13.29s; loss: 132.5967; acc: 77439.0/78638.0=0.9848\n","Instance: 6500; Time: 13.17s; loss: 109.2844; acc: 83994.0/85290.0=0.9848\n","Instance: 7000; Time: 12.89s; loss: 87.7719; acc: 90339.0/91707.0=0.9851\n","Instance: 7500; Time: 13.34s; loss: 124.1610; acc: 96868.0/98342.0=0.9850\n","Instance: 8000; Time: 13.54s; loss: 108.5797; acc: 103448.0/105021.0=0.9850\n","Instance: 8500; Time: 12.82s; loss: 73.6232; acc: 109646.0/111276.0=0.9854\n","Instance: 9000; Time: 12.43s; loss: 66.8185; acc: 115837.0/117515.0=0.9857\n","Instance: 9500; Time: 13.57s; loss: 120.2079; acc: 122289.0/124061.0=0.9857\n","Instance: 10000; Time: 12.99s; loss: 105.9813; acc: 128580.0/130447.0=0.9857\n","Instance: 10500; Time: 12.61s; loss: 111.9910; acc: 134727.0/136671.0=0.9858\n","Instance: 11000; Time: 13.08s; loss: 111.4713; acc: 140939.0/142976.0=0.9858\n","Instance: 11500; Time: 13.57s; loss: 103.3633; acc: 147524.0/149653.0=0.9858\n","Instance: 12000; Time: 13.35s; loss: 112.3552; acc: 154027.0/156246.0=0.9858\n","Instance: 12500; Time: 13.26s; loss: 107.3411; acc: 160482.0/162789.0=0.9858\n","Instance: 13000; Time: 13.73s; loss: 81.7191; acc: 166991.0/169380.0=0.9859\n","Instance: 13500; Time: 12.94s; loss: 72.0189; acc: 173174.0/175613.0=0.9861\n","Instance: 14000; Time: 13.24s; loss: 91.7172; acc: 179539.0/182053.0=0.9862\n","Instance: 14500; Time: 13.74s; loss: 113.4875; acc: 186193.0/188791.0=0.9862\n","Instance: 15000; Time: 13.46s; loss: 111.2076; acc: 192686.0/195385.0=0.9862\n","     Instance: 15473; Time: 12.74s; loss: 99.6899; acc: 198813.0/201595.0=0.9862\n","Epoch: 5 training finished. Time: 409.27s, speed: 37.81st/s,  total loss: 3298.6664428710938\n","gold_num =  11549  pred_num =  11342  right_num =  10693\n","Dev: time: 68.97s, speed: 89.89st/s; acc: 0.9850, p: 0.9428, r: 0.9259, f: 0.9343\n","Exceed previous best f score: 0.9290913052952714\n","gold_num =  7933  pred_num =  7792  right_num =  7334\n","Test: time: 45.72s, speed: 90.50st/s; acc: 0.9844, p: 0.9412, r: 0.9245, f: 0.9328\n","Epoch: 6/25\n"," Learning rate is setted as: 0.011026378359374997\n","Instance: 500; Time: 12.54s; loss: 76.9894; acc: 6243.0/6313.0=0.9889\n","Instance: 1000; Time: 13.37s; loss: 57.8855; acc: 12765.0/12875.0=0.9915\n","Instance: 1500; Time: 12.76s; loss: 74.3029; acc: 19066.0/19236.0=0.9912\n","Instance: 2000; Time: 13.54s; loss: 79.2275; acc: 25659.0/25885.0=0.9913\n","Instance: 2500; Time: 13.61s; loss: 81.9520; acc: 32168.0/32465.0=0.9909\n","Instance: 3000; Time: 13.62s; loss: 96.1342; acc: 38874.0/39256.0=0.9903\n","Instance: 3500; Time: 13.15s; loss: 78.6571; acc: 45366.0/45809.0=0.9903\n","Instance: 4000; Time: 12.85s; loss: 70.9922; acc: 51609.0/52105.0=0.9905\n","Instance: 4500; Time: 12.86s; loss: 72.9288; acc: 57911.0/58474.0=0.9904\n","Instance: 5000; Time: 13.33s; loss: 113.0470; acc: 64366.0/65027.0=0.9898\n","Instance: 5500; Time: 13.33s; loss: 90.3369; acc: 70667.0/71403.0=0.9897\n","Instance: 6000; Time: 13.07s; loss: 58.0320; acc: 76908.0/77688.0=0.9900\n","Instance: 6500; Time: 13.11s; loss: 107.7622; acc: 83320.0/84197.0=0.9896\n","Instance: 7000; Time: 13.40s; loss: 73.0936; acc: 89738.0/90674.0=0.9897\n","Instance: 7500; Time: 13.29s; loss: 91.0533; acc: 96123.0/97129.0=0.9896\n","Instance: 8000; Time: 13.36s; loss: 100.2097; acc: 102608.0/103695.0=0.9895\n","Instance: 8500; Time: 13.32s; loss: 85.1246; acc: 109060.0/110212.0=0.9895\n","Instance: 9000; Time: 13.28s; loss: 61.2883; acc: 115527.0/116726.0=0.9897\n","Instance: 9500; Time: 13.28s; loss: 77.7516; acc: 122064.0/123315.0=0.9899\n","Instance: 10000; Time: 13.20s; loss: 75.5382; acc: 128383.0/129699.0=0.9899\n","Instance: 10500; Time: 13.62s; loss: 48.9519; acc: 134809.0/136168.0=0.9900\n","Instance: 11000; Time: 14.28s; loss: 84.1169; acc: 141555.0/142995.0=0.9899\n","Instance: 11500; Time: 13.16s; loss: 96.7781; acc: 147873.0/149412.0=0.9897\n","Instance: 12000; Time: 13.50s; loss: 95.3645; acc: 154304.0/155930.0=0.9896\n","Instance: 12500; Time: 13.51s; loss: 86.5217; acc: 160879.0/162572.0=0.9896\n","Instance: 13000; Time: 12.87s; loss: 63.4888; acc: 167195.0/168936.0=0.9897\n","Instance: 13500; Time: 13.15s; loss: 73.6357; acc: 173658.0/175455.0=0.9898\n","Instance: 14000; Time: 13.43s; loss: 64.7780; acc: 180197.0/182038.0=0.9899\n","Instance: 14500; Time: 13.46s; loss: 72.0006; acc: 186628.0/188533.0=0.9899\n","Instance: 15000; Time: 13.68s; loss: 62.4739; acc: 193303.0/195246.0=0.9900\n","     Instance: 15473; Time: 13.17s; loss: 86.3770; acc: 199575.0/201595.0=0.9900\n","Epoch: 6 training finished. Time: 412.08s, speed: 37.55st/s,  total loss: 2456.794219970703\n","gold_num =  11549  pred_num =  11380  right_num =  10746\n","Dev: time: 70.62s, speed: 87.80st/s; acc: 0.9856, p: 0.9443, r: 0.9305, f: 0.9373\n","Exceed previous best f score: 0.9342536368004892\n","gold_num =  7933  pred_num =  7827  right_num =  7374\n","Test: time: 46.39s, speed: 89.22st/s; acc: 0.9850, p: 0.9421, r: 0.9295, f: 0.9358\n","Epoch: 7/25\n"," Learning rate is setted as: 0.010475059441406245\n","Instance: 500; Time: 12.71s; loss: 64.6486; acc: 6285.0/6326.0=0.9935\n","Instance: 1000; Time: 12.91s; loss: 50.5127; acc: 12637.0/12718.0=0.9936\n","Instance: 1500; Time: 13.76s; loss: 52.4008; acc: 19213.0/19347.0=0.9931\n","Instance: 2000; Time: 13.50s; loss: 52.1752; acc: 25659.0/25840.0=0.9930\n","Instance: 2500; Time: 13.48s; loss: 80.1547; acc: 32160.0/32413.0=0.9922\n","Instance: 3000; Time: 13.78s; loss: 65.2812; acc: 38865.0/39181.0=0.9919\n","Instance: 3500; Time: 13.50s; loss: 55.1823; acc: 45419.0/45782.0=0.9921\n","Instance: 4000; Time: 13.34s; loss: 70.9507; acc: 51883.0/52308.0=0.9919\n","Instance: 4500; Time: 13.30s; loss: 61.6730; acc: 58368.0/58837.0=0.9920\n","Instance: 5000; Time: 13.33s; loss: 64.6938; acc: 64825.0/65351.0=0.9920\n","Instance: 5500; Time: 13.22s; loss: 53.3679; acc: 71164.0/71744.0=0.9919\n","Instance: 6000; Time: 13.58s; loss: 40.3828; acc: 77805.0/78423.0=0.9921\n","Instance: 6500; Time: 13.53s; loss: 58.7009; acc: 84357.0/85012.0=0.9923\n","Instance: 7000; Time: 13.31s; loss: 57.5640; acc: 90786.0/91500.0=0.9922\n","Instance: 7500; Time: 13.07s; loss: 54.4204; acc: 97066.0/97822.0=0.9923\n","Instance: 8000; Time: 13.74s; loss: 60.4430; acc: 103791.0/104602.0=0.9922\n","Instance: 8500; Time: 13.00s; loss: 84.5000; acc: 110193.0/111071.0=0.9921\n","Instance: 9000; Time: 13.59s; loss: 52.7688; acc: 116644.0/117569.0=0.9921\n","Instance: 9500; Time: 13.54s; loss: 69.2681; acc: 123263.0/124253.0=0.9920\n","Instance: 10000; Time: 13.26s; loss: 74.3444; acc: 129720.0/130772.0=0.9920\n","Instance: 10500; Time: 13.31s; loss: 46.7585; acc: 136194.0/137282.0=0.9921\n","Instance: 11000; Time: 12.85s; loss: 56.9421; acc: 142515.0/143647.0=0.9921\n","Instance: 11500; Time: 12.66s; loss: 53.6851; acc: 148758.0/149928.0=0.9922\n","Instance: 12000; Time: 13.37s; loss: 70.5538; acc: 155264.0/156484.0=0.9922\n","Instance: 12500; Time: 12.99s; loss: 54.3075; acc: 161521.0/162776.0=0.9923\n","Instance: 13000; Time: 14.12s; loss: 92.8018; acc: 168336.0/169671.0=0.9921\n","Instance: 13500; Time: 13.53s; loss: 78.6158; acc: 174823.0/176215.0=0.9921\n","Instance: 14000; Time: 13.05s; loss: 51.0726; acc: 181245.0/182672.0=0.9922\n","Instance: 14500; Time: 13.18s; loss: 60.3380; acc: 187611.0/189082.0=0.9922\n","Instance: 15000; Time: 13.11s; loss: 50.7622; acc: 193927.0/195450.0=0.9922\n","     Instance: 15473; Time: 12.61s; loss: 68.3273; acc: 200017.0/201595.0=0.9922\n","Epoch: 7 training finished. Time: 412.19s, speed: 37.54st/s,  total loss: 1907.597900390625\n","gold_num =  11549  pred_num =  11313  right_num =  10732\n","Dev: time: 69.42s, speed: 89.30st/s; acc: 0.9863, p: 0.9486, r: 0.9293, f: 0.9389\n","Exceed previous best f score: 0.9373282742378648\n","gold_num =  7933  pred_num =  7767  right_num =  7359\n","Test: time: 45.55s, speed: 90.85st/s; acc: 0.9859, p: 0.9475, r: 0.9276, f: 0.9375\n","Epoch: 8/25\n"," Learning rate is setted as: 0.009951306469335933\n","Instance: 500; Time: 12.67s; loss: 54.4456; acc: 6348.0/6387.0=0.9939\n","Instance: 1000; Time: 13.93s; loss: 52.6841; acc: 12932.0/13018.0=0.9934\n","Instance: 1500; Time: 13.40s; loss: 47.6814; acc: 19449.0/19570.0=0.9938\n","Instance: 2000; Time: 12.93s; loss: 39.9332; acc: 25776.0/25927.0=0.9942\n","Instance: 2500; Time: 13.73s; loss: 56.5690; acc: 32466.0/32671.0=0.9937\n","Instance: 3000; Time: 13.65s; loss: 48.4219; acc: 39092.0/39346.0=0.9935\n","Instance: 3500; Time: 13.26s; loss: 59.4219; acc: 45643.0/45932.0=0.9937\n","Instance: 4000; Time: 12.77s; loss: 53.1688; acc: 51938.0/52267.0=0.9937\n","Instance: 4500; Time: 13.05s; loss: 65.1288; acc: 58308.0/58680.0=0.9937\n","Instance: 5000; Time: 14.16s; loss: 61.6659; acc: 65038.0/65462.0=0.9935\n","Instance: 5500; Time: 12.94s; loss: 45.9749; acc: 71417.0/71883.0=0.9935\n","Instance: 6000; Time: 12.65s; loss: 29.2548; acc: 77625.0/78122.0=0.9936\n","Instance: 6500; Time: 13.16s; loss: 32.4275; acc: 84085.0/84600.0=0.9939\n","Instance: 7000; Time: 13.72s; loss: 58.5010; acc: 90605.0/91176.0=0.9937\n","Instance: 7500; Time: 13.22s; loss: 38.1978; acc: 97054.0/97651.0=0.9939\n","Instance: 8000; Time: 12.76s; loss: 42.9939; acc: 103372.0/103999.0=0.9940\n","Instance: 8500; Time: 13.94s; loss: 32.8029; acc: 110127.0/110776.0=0.9941\n","Instance: 9000; Time: 13.10s; loss: 37.6906; acc: 116471.0/117150.0=0.9942\n","Instance: 9500; Time: 13.19s; loss: 57.2750; acc: 122909.0/123639.0=0.9941\n","Instance: 10000; Time: 13.86s; loss: 56.1132; acc: 129629.0/130406.0=0.9940\n","Instance: 10500; Time: 13.23s; loss: 51.5667; acc: 136051.0/136875.0=0.9940\n","Instance: 11000; Time: 12.86s; loss: 47.2677; acc: 142341.0/143208.0=0.9939\n","Instance: 11500; Time: 14.02s; loss: 53.7889; acc: 149012.0/149925.0=0.9939\n","Instance: 12000; Time: 12.55s; loss: 40.0719; acc: 155195.0/156129.0=0.9940\n","Instance: 12500; Time: 12.67s; loss: 31.1228; acc: 161466.0/162422.0=0.9941\n","Instance: 13000; Time: 13.37s; loss: 51.2429; acc: 167976.0/168972.0=0.9941\n","Instance: 13500; Time: 13.34s; loss: 39.7428; acc: 174453.0/175482.0=0.9941\n","Instance: 14000; Time: 13.06s; loss: 32.4834; acc: 180870.0/181922.0=0.9942\n","Instance: 14500; Time: 13.64s; loss: 56.0033; acc: 187510.0/188602.0=0.9942\n","Instance: 15000; Time: 13.81s; loss: 45.7872; acc: 194229.0/195357.0=0.9942\n","     Instance: 15473; Time: 12.96s; loss: 46.1063; acc: 200440.0/201595.0=0.9943\n","Epoch: 8 training finished. Time: 411.61s, speed: 37.59st/s,  total loss: 1465.5357666015625\n","gold_num =  11549  pred_num =  11391  right_num =  10853\n","Dev: time: 69.21s, speed: 89.58st/s; acc: 0.9877, p: 0.9528, r: 0.9397, f: 0.9462\n","Exceed previous best f score: 0.9388504942699677\n","gold_num =  7933  pred_num =  7843  right_num =  7450\n","Test: time: 45.37s, speed: 91.21st/s; acc: 0.9870, p: 0.9499, r: 0.9391, f: 0.9445\n","Epoch: 9/25\n"," Learning rate is setted as: 0.009453741145869136\n","Instance: 500; Time: 13.46s; loss: 58.5912; acc: 6575.0/6623.0=0.9928\n","Instance: 1000; Time: 12.93s; loss: 36.2391; acc: 12813.0/12884.0=0.9945\n","Instance: 1500; Time: 12.95s; loss: 49.9065; acc: 19211.0/19317.0=0.9945\n","Instance: 2000; Time: 12.74s; loss: 31.3058; acc: 25542.0/25669.0=0.9951\n","Instance: 2500; Time: 13.06s; loss: 41.3511; acc: 32025.0/32181.0=0.9952\n","Instance: 3000; Time: 13.89s; loss: 36.1558; acc: 38710.0/38891.0=0.9953\n","Instance: 3500; Time: 13.21s; loss: 36.5859; acc: 45197.0/45409.0=0.9953\n","Instance: 4000; Time: 13.68s; loss: 42.3206; acc: 51837.0/52090.0=0.9951\n","Instance: 4500; Time: 12.94s; loss: 42.8798; acc: 58308.0/58595.0=0.9951\n","Instance: 5000; Time: 13.38s; loss: 41.7186; acc: 64746.0/65071.0=0.9950\n","Instance: 5500; Time: 13.46s; loss: 51.5365; acc: 71340.0/71703.0=0.9949\n","Instance: 6000; Time: 13.15s; loss: 43.7911; acc: 77708.0/78108.0=0.9949\n","Instance: 6500; Time: 12.73s; loss: 37.5602; acc: 84020.0/84439.0=0.9950\n","Instance: 7000; Time: 13.22s; loss: 37.1865; acc: 90444.0/90902.0=0.9950\n","Instance: 7500; Time: 13.13s; loss: 44.0299; acc: 96840.0/97334.0=0.9949\n","Instance: 8000; Time: 13.54s; loss: 54.3297; acc: 103482.0/104027.0=0.9948\n","Instance: 8500; Time: 13.39s; loss: 27.1342; acc: 109914.0/110482.0=0.9949\n","Instance: 9000; Time: 13.38s; loss: 36.5770; acc: 116413.0/117017.0=0.9948\n","Instance: 9500; Time: 12.88s; loss: 28.3264; acc: 122714.0/123335.0=0.9950\n","Instance: 10000; Time: 13.31s; loss: 37.7843; acc: 129221.0/129878.0=0.9949\n","Instance: 10500; Time: 13.70s; loss: 33.8572; acc: 135902.0/136588.0=0.9950\n","Instance: 11000; Time: 13.30s; loss: 36.8108; acc: 142316.0/143024.0=0.9950\n","Instance: 11500; Time: 13.63s; loss: 42.7469; acc: 148846.0/149589.0=0.9950\n","Instance: 12000; Time: 13.28s; loss: 47.1439; acc: 155273.0/156058.0=0.9950\n","Instance: 12500; Time: 13.19s; loss: 33.7451; acc: 161679.0/162494.0=0.9950\n","Instance: 13000; Time: 13.76s; loss: 46.3788; acc: 168348.0/169199.0=0.9950\n","Instance: 13500; Time: 13.14s; loss: 46.4191; acc: 174790.0/175680.0=0.9949\n","Instance: 14000; Time: 13.19s; loss: 44.9205; acc: 181236.0/182165.0=0.9949\n","Instance: 14500; Time: 13.36s; loss: 44.0914; acc: 187674.0/188638.0=0.9949\n","Instance: 15000; Time: 13.97s; loss: 62.8818; acc: 194519.0/195524.0=0.9949\n","     Instance: 15473; Time: 12.54s; loss: 36.4100; acc: 200567.0/201595.0=0.9949\n","Epoch: 9 training finished. Time: 411.48s, speed: 37.60st/s,  total loss: 1290.7158203125\n","gold_num =  11549  pred_num =  11351  right_num =  10822\n","Dev: time: 69.34s, speed: 89.42st/s; acc: 0.9875, p: 0.9534, r: 0.9371, f: 0.9452\n","gold_num =  7933  pred_num =  7810  right_num =  7468\n","Test: time: 45.60s, speed: 90.69st/s; acc: 0.9884, p: 0.9562, r: 0.9414, f: 0.9487\n","Epoch: 10/25\n"," Learning rate is setted as: 0.00898105408857568\n","Instance: 500; Time: 12.68s; loss: 23.2789; acc: 6276.0/6293.0=0.9973\n","Instance: 1000; Time: 13.72s; loss: 30.1962; acc: 13019.0/13061.0=0.9968\n","Instance: 1500; Time: 13.72s; loss: 45.1836; acc: 19729.0/19812.0=0.9958\n","Instance: 2000; Time: 12.94s; loss: 40.8853; acc: 26091.0/26210.0=0.9955\n","Instance: 2500; Time: 12.91s; loss: 31.5344; acc: 32436.0/32583.0=0.9955\n","Instance: 3000; Time: 12.91s; loss: 33.2476; acc: 38717.0/38887.0=0.9956\n","Instance: 3500; Time: 13.36s; loss: 37.6049; acc: 45286.0/45487.0=0.9956\n","Instance: 4000; Time: 12.86s; loss: 38.0665; acc: 51636.0/51863.0=0.9956\n","Instance: 4500; Time: 13.46s; loss: 29.7852; acc: 58145.0/58399.0=0.9957\n","Instance: 5000; Time: 13.19s; loss: 25.4919; acc: 64524.0/64799.0=0.9958\n","Instance: 5500; Time: 13.66s; loss: 22.7579; acc: 71231.0/71519.0=0.9960\n","Instance: 6000; Time: 13.64s; loss: 30.2502; acc: 77936.0/78242.0=0.9961\n","Instance: 6500; Time: 12.82s; loss: 33.7162; acc: 84281.0/84617.0=0.9960\n","Instance: 7000; Time: 13.09s; loss: 36.9921; acc: 90670.0/91043.0=0.9959\n","Instance: 7500; Time: 13.68s; loss: 40.1864; acc: 97327.0/97734.0=0.9958\n","Instance: 8000; Time: 13.33s; loss: 49.3093; acc: 103863.0/104310.0=0.9957\n","Instance: 8500; Time: 13.33s; loss: 36.7318; acc: 110357.0/110840.0=0.9956\n","Instance: 9000; Time: 13.49s; loss: 37.4592; acc: 117056.0/117565.0=0.9957\n","Instance: 9500; Time: 13.15s; loss: 40.4995; acc: 123490.0/124027.0=0.9957\n","Instance: 10000; Time: 13.49s; loss: 35.9812; acc: 129974.0/130534.0=0.9957\n","Instance: 10500; Time: 12.66s; loss: 36.2181; acc: 136210.0/136799.0=0.9957\n","Instance: 11000; Time: 13.14s; loss: 45.0503; acc: 142633.0/143265.0=0.9956\n","Instance: 11500; Time: 13.53s; loss: 40.4088; acc: 149280.0/149952.0=0.9955\n","Instance: 12000; Time: 13.73s; loss: 33.3868; acc: 155859.0/156561.0=0.9955\n","Instance: 12500; Time: 12.90s; loss: 27.5940; acc: 162255.0/162986.0=0.9955\n","Instance: 13000; Time: 13.32s; loss: 28.5651; acc: 168679.0/169431.0=0.9956\n","Instance: 13500; Time: 13.37s; loss: 30.6842; acc: 175149.0/175935.0=0.9955\n","Instance: 14000; Time: 13.08s; loss: 29.5547; acc: 181492.0/182300.0=0.9956\n","Instance: 14500; Time: 13.34s; loss: 33.3875; acc: 188034.0/188873.0=0.9956\n","Instance: 15000; Time: 13.60s; loss: 40.7832; acc: 194710.0/195579.0=0.9956\n","     Instance: 15473; Time: 12.77s; loss: 22.1892; acc: 200712.0/201595.0=0.9956\n","Epoch: 10 training finished. Time: 410.85s, speed: 37.66st/s,  total loss: 1066.9802551269531\n","gold_num =  11549  pred_num =  11417  right_num =  10920\n","Dev: time: 70.73s, speed: 87.65st/s; acc: 0.9886, p: 0.9565, r: 0.9455, f: 0.9510\n","Exceed previous best f score: 0.9462074978204009\n","gold_num =  7933  pred_num =  7846  right_num =  7494\n","Test: time: 47.43s, speed: 87.25st/s; acc: 0.9885, p: 0.9551, r: 0.9447, f: 0.9499\n","Epoch: 11/25\n"," Learning rate is setted as: 0.008532001384146894\n","Instance: 500; Time: 12.64s; loss: 32.3020; acc: 6204.0/6226.0=0.9965\n","Instance: 1000; Time: 13.78s; loss: 31.7760; acc: 12833.0/12887.0=0.9958\n","Instance: 1500; Time: 13.14s; loss: 47.1687; acc: 19271.0/19361.0=0.9954\n","Instance: 2000; Time: 13.92s; loss: 38.6915; acc: 26031.0/26155.0=0.9953\n","Instance: 2500; Time: 13.43s; loss: 39.7305; acc: 32459.0/32613.0=0.9953\n","Instance: 3000; Time: 13.72s; loss: 26.5741; acc: 39161.0/39332.0=0.9957\n","Instance: 3500; Time: 13.16s; loss: 35.3914; acc: 45591.0/45789.0=0.9957\n","Instance: 4000; Time: 13.07s; loss: 31.6345; acc: 51995.0/52219.0=0.9957\n","Instance: 4500; Time: 13.11s; loss: 41.4302; acc: 58473.0/58722.0=0.9958\n","Instance: 5000; Time: 13.63s; loss: 30.1099; acc: 65182.0/65455.0=0.9958\n","Instance: 5500; Time: 13.19s; loss: 21.5846; acc: 71539.0/71837.0=0.9959\n","Instance: 6000; Time: 12.82s; loss: 32.2462; acc: 77762.0/78082.0=0.9959\n","Instance: 6500; Time: 13.67s; loss: 41.2004; acc: 84285.0/84641.0=0.9958\n","Instance: 7000; Time: 13.90s; loss: 42.0002; acc: 90976.0/91358.0=0.9958\n","Instance: 7500; Time: 12.72s; loss: 31.6442; acc: 97249.0/97651.0=0.9959\n","Instance: 8000; Time: 13.47s; loss: 26.8258; acc: 103738.0/104164.0=0.9959\n","Instance: 8500; Time: 13.15s; loss: 21.8730; acc: 110222.0/110660.0=0.9960\n","Instance: 9000; Time: 13.55s; loss: 30.7133; acc: 116745.0/117204.0=0.9961\n","Instance: 9500; Time: 13.14s; loss: 31.2406; acc: 123202.0/123686.0=0.9961\n","Instance: 10000; Time: 13.42s; loss: 30.1965; acc: 129782.0/130298.0=0.9960\n","Instance: 10500; Time: 13.30s; loss: 31.3401; acc: 136250.0/136790.0=0.9961\n","Instance: 11000; Time: 13.29s; loss: 35.1039; acc: 142688.0/143257.0=0.9960\n","Instance: 11500; Time: 13.79s; loss: 37.5172; acc: 149227.0/149820.0=0.9960\n","Instance: 12000; Time: 13.16s; loss: 32.7704; acc: 155645.0/156259.0=0.9961\n","Instance: 12500; Time: 13.65s; loss: 37.7952; acc: 162179.0/162827.0=0.9960\n","Instance: 13000; Time: 13.43s; loss: 34.0564; acc: 168657.0/169345.0=0.9959\n","Instance: 13500; Time: 13.13s; loss: 35.5393; acc: 175078.0/175799.0=0.9959\n","Instance: 14000; Time: 12.93s; loss: 20.5773; acc: 181361.0/182097.0=0.9960\n","Instance: 14500; Time: 13.64s; loss: 44.2671; acc: 187897.0/188660.0=0.9960\n","Instance: 15000; Time: 13.71s; loss: 25.7831; acc: 194508.0/195295.0=0.9960\n","     Instance: 15473; Time: 13.01s; loss: 21.4504; acc: 200788.0/201595.0=0.9960\n","Epoch: 11 training finished. Time: 413.69s, speed: 37.40st/s,  total loss: 1020.533935546875\n","gold_num =  11549  pred_num =  11394  right_num =  10938\n","Dev: time: 69.59s, speed: 89.09st/s; acc: 0.9894, p: 0.9600, r: 0.9471, f: 0.9535\n","Exceed previous best f score: 0.9509710006095968\n","gold_num =  7933  pred_num =  7823  right_num =  7474\n","Test: time: 45.71s, speed: 90.52st/s; acc: 0.9883, p: 0.9554, r: 0.9421, f: 0.9487\n","Epoch: 12/25\n"," Learning rate is setted as: 0.00810540131493955\n","Instance: 500; Time: 13.35s; loss: 27.2825; acc: 6582.0/6608.0=0.9961\n","Instance: 1000; Time: 13.00s; loss: 35.8417; acc: 13037.0/13096.0=0.9955\n","Instance: 1500; Time: 13.94s; loss: 21.7399; acc: 19971.0/20042.0=0.9965\n","Instance: 2000; Time: 12.87s; loss: 40.8862; acc: 26426.0/26535.0=0.9959\n","Instance: 2500; Time: 12.63s; loss: 38.4954; acc: 32858.0/32995.0=0.9958\n","Instance: 3000; Time: 12.87s; loss: 30.0092; acc: 39322.0/39483.0=0.9959\n","Instance: 3500; Time: 12.75s; loss: 22.3873; acc: 45650.0/45829.0=0.9961\n","Instance: 4000; Time: 13.16s; loss: 28.8928; acc: 52124.0/52321.0=0.9962\n","Instance: 4500; Time: 13.27s; loss: 20.5012; acc: 58586.0/58803.0=0.9963\n","Instance: 5000; Time: 13.20s; loss: 33.1398; acc: 64979.0/65224.0=0.9962\n","Instance: 5500; Time: 12.57s; loss: 28.0148; acc: 71287.0/71554.0=0.9963\n","Instance: 6000; Time: 13.08s; loss: 34.3016; acc: 77842.0/78142.0=0.9962\n","Instance: 6500; Time: 12.79s; loss: 18.8268; acc: 84239.0/84556.0=0.9963\n","Instance: 7000; Time: 13.31s; loss: 30.2821; acc: 90855.0/91198.0=0.9962\n","Instance: 7500; Time: 13.09s; loss: 30.9940; acc: 97414.0/97785.0=0.9962\n","Instance: 8000; Time: 13.40s; loss: 30.6206; acc: 104170.0/104563.0=0.9962\n","Instance: 8500; Time: 13.06s; loss: 26.4095; acc: 110721.0/111131.0=0.9963\n","Instance: 9000; Time: 12.76s; loss: 16.8372; acc: 117131.0/117549.0=0.9964\n","Instance: 9500; Time: 13.13s; loss: 18.0555; acc: 123651.0/124083.0=0.9965\n","Instance: 10000; Time: 13.40s; loss: 37.2826; acc: 130300.0/130760.0=0.9965\n","Instance: 10500; Time: 13.18s; loss: 28.9229; acc: 136986.0/137465.0=0.9965\n","Instance: 11000; Time: 12.78s; loss: 33.2170; acc: 143414.0/143928.0=0.9964\n","Instance: 11500; Time: 12.60s; loss: 27.8741; acc: 149714.0/150253.0=0.9964\n","Instance: 12000; Time: 13.65s; loss: 50.0778; acc: 156352.0/156925.0=0.9963\n","Instance: 12500; Time: 12.45s; loss: 21.8601; acc: 162616.0/163209.0=0.9964\n","Instance: 13000; Time: 12.90s; loss: 17.5961; acc: 169058.0/169666.0=0.9964\n","Instance: 13500; Time: 12.97s; loss: 35.1477; acc: 175433.0/176070.0=0.9964\n","Instance: 14000; Time: 12.64s; loss: 11.5878; acc: 181643.0/182282.0=0.9965\n","Instance: 14500; Time: 13.40s; loss: 23.3999; acc: 188308.0/188968.0=0.9965\n","Instance: 15000; Time: 13.11s; loss: 34.2928; acc: 194809.0/195501.0=0.9965\n","     Instance: 15473; Time: 12.56s; loss: 27.8983; acc: 200882.0/201595.0=0.9965\n","Epoch: 12 training finished. Time: 403.88s, speed: 38.31st/s,  total loss: 882.6751098632812\n","gold_num =  11549  pred_num =  11411  right_num =  10951\n","Dev: time: 68.94s, speed: 89.93st/s; acc: 0.9894, p: 0.9597, r: 0.9482, f: 0.9539\n","Exceed previous best f score: 0.9534934402650046\n","gold_num =  7933  pred_num =  7838  right_num =  7510\n","Test: time: 44.91s, speed: 92.14st/s; acc: 0.9890, p: 0.9582, r: 0.9467, f: 0.9524\n","Epoch: 13/25\n"," Learning rate is setted as: 0.007700131249192572\n","Instance: 500; Time: 13.23s; loss: 22.6151; acc: 6547.0/6556.0=0.9986\n","Instance: 1000; Time: 12.70s; loss: 12.1312; acc: 12927.0/12944.0=0.9987\n","Instance: 1500; Time: 12.12s; loss: 26.6727; acc: 19074.0/19113.0=0.9980\n","Instance: 2500; Time: 12.40s; loss: 19.7059; acc: 31625.0/31683.0=0.9982\n","Instance: 3000; Time: 13.42s; loss: 16.5696; acc: 38349.0/38415.0=0.9983\n","Instance: 3500; Time: 13.56s; loss: 25.5654; acc: 45014.0/45097.0=0.9982\n","Instance: 4000; Time: 13.35s; loss: 22.3583; acc: 51541.0/51639.0=0.9981\n","Instance: 4500; Time: 12.71s; loss: 35.7896; acc: 57824.0/57951.0=0.9978\n","Instance: 5000; Time: 13.59s; loss: 25.7145; acc: 64445.0/64591.0=0.9977\n","Instance: 5500; Time: 13.09s; loss: 20.7323; acc: 70733.0/70897.0=0.9977\n","Instance: 6000; Time: 13.71s; loss: 24.1165; acc: 77355.0/77537.0=0.9977\n","Instance: 6500; Time: 12.88s; loss: 16.0189; acc: 83764.0/83956.0=0.9977\n","Instance: 7000; Time: 12.46s; loss: 15.9825; acc: 90082.0/90289.0=0.9977\n","Instance: 7500; Time: 13.58s; loss: 15.5042; acc: 96831.0/97044.0=0.9978\n","Instance: 8000; Time: 13.19s; loss: 22.7032; acc: 103445.0/103675.0=0.9978\n","Instance: 8500; Time: 12.80s; loss: 13.7097; acc: 109675.0/109911.0=0.9979\n","Instance: 9000; Time: 13.23s; loss: 23.5203; acc: 116226.0/116478.0=0.9978\n","Instance: 9500; Time: 12.89s; loss: 29.9652; acc: 122694.0/122965.0=0.9978\n","Instance: 10000; Time: 13.61s; loss: 24.7976; acc: 129421.0/129712.0=0.9978\n","Instance: 10500; Time: 13.31s; loss: 35.1497; acc: 136074.0/136391.0=0.9977\n","Instance: 11000; Time: 13.01s; loss: 31.4211; acc: 142621.0/142956.0=0.9977\n","Instance: 11500; Time: 13.35s; loss: 23.4802; acc: 149253.0/149605.0=0.9976\n","Instance: 12000; Time: 13.07s; loss: 33.8549; acc: 155790.0/156174.0=0.9975\n","Instance: 12500; Time: 13.30s; loss: 27.3696; acc: 162418.0/162826.0=0.9975\n","Instance: 13000; Time: 12.91s; loss: 27.3934; acc: 168785.0/169209.0=0.9975\n","Instance: 13500; Time: 12.89s; loss: 29.9373; acc: 175234.0/175688.0=0.9974\n","Instance: 14000; Time: 13.52s; loss: 27.3829; acc: 181835.0/182314.0=0.9974\n","Instance: 14500; Time: 13.02s; loss: 22.3483; acc: 188366.0/188867.0=0.9973\n","Instance: 15000; Time: 13.53s; loss: 22.2000; acc: 195064.0/195583.0=0.9973\n","     Instance: 15473; Time: 12.37s; loss: 27.8184; acc: 201054.0/201595.0=0.9973\n","Epoch: 13 training finished. Time: 405.66s, speed: 38.14st/s,  total loss: 733.6004943847656\n","gold_num =  11549  pred_num =  11399  right_num =  10956\n","Dev: time: 67.60s, speed: 91.71st/s; acc: 0.9897, p: 0.9611, r: 0.9487, f: 0.9549\n","Exceed previous best f score: 0.9539198606271778\n","gold_num =  7933  pred_num =  7823  right_num =  7504\n","Test: time: 44.28s, speed: 93.45st/s; acc: 0.9893, p: 0.9592, r: 0.9459, f: 0.9525\n","Epoch: 14/25\n"," Learning rate is setted as: 0.007315124686732943\n","Instance: 500; Time: 13.46s; loss: 23.5914; acc: 6670.0/6706.0=0.9946\n","Instance: 1000; Time: 12.78s; loss: 19.8573; acc: 13066.0/13123.0=0.9957\n","Instance: 1500; Time: 13.36s; loss: 12.5204; acc: 19827.0/19895.0=0.9966\n","Instance: 2000; Time: 13.37s; loss: 40.6663; acc: 26492.0/26590.0=0.9963\n","Instance: 2500; Time: 12.40s; loss: 16.8529; acc: 32704.0/32812.0=0.9967\n","Instance: 3000; Time: 13.36s; loss: 14.4429; acc: 39328.0/39452.0=0.9969\n","Instance: 3500; Time: 13.25s; loss: 17.3295; acc: 45855.0/45996.0=0.9969\n","Instance: 4000; Time: 13.01s; loss: 25.6107; acc: 52310.0/52467.0=0.9970\n","Instance: 4500; Time: 12.75s; loss: 25.8527; acc: 58533.0/58715.0=0.9969\n","Instance: 5000; Time: 12.97s; loss: 25.0204; acc: 64976.0/65177.0=0.9969\n","Instance: 5500; Time: 13.65s; loss: 26.2701; acc: 71782.0/72004.0=0.9969\n","Instance: 6000; Time: 12.53s; loss: 20.5695; acc: 78098.0/78340.0=0.9969\n","Instance: 6500; Time: 13.63s; loss: 16.9316; acc: 84779.0/85033.0=0.9970\n","Instance: 7000; Time: 13.28s; loss: 26.4066; acc: 91404.0/91684.0=0.9969\n","Instance: 7500; Time: 12.94s; loss: 19.2941; acc: 97872.0/98161.0=0.9971\n","Instance: 8000; Time: 13.37s; loss: 24.1528; acc: 104420.0/104733.0=0.9970\n","Instance: 8500; Time: 13.10s; loss: 15.5515; acc: 110892.0/111214.0=0.9971\n","Instance: 9000; Time: 12.69s; loss: 23.2861; acc: 117281.0/117619.0=0.9971\n","Instance: 9500; Time: 13.38s; loss: 17.0292; acc: 123923.0/124268.0=0.9972\n","Instance: 10000; Time: 13.22s; loss: 15.6533; acc: 130428.0/130783.0=0.9973\n","Instance: 10500; Time: 13.62s; loss: 15.1519; acc: 137022.0/137395.0=0.9973\n","Instance: 11000; Time: 12.94s; loss: 19.0481; acc: 143560.0/143944.0=0.9973\n","Instance: 11500; Time: 13.07s; loss: 22.2836; acc: 150032.0/150437.0=0.9973\n","Instance: 12000; Time: 12.52s; loss: 15.7135; acc: 156302.0/156717.0=0.9974\n","Instance: 12500; Time: 12.85s; loss: 20.4401; acc: 162654.0/163088.0=0.9973\n","Instance: 13000; Time: 13.06s; loss: 29.6854; acc: 169141.0/169597.0=0.9973\n","Instance: 13500; Time: 12.95s; loss: 28.3958; acc: 175568.0/176054.0=0.9972\n","Instance: 14000; Time: 12.88s; loss: 12.9547; acc: 181850.0/182342.0=0.9973\n","Instance: 14500; Time: 12.77s; loss: 21.8975; acc: 188348.0/188855.0=0.9973\n","Instance: 15000; Time: 13.28s; loss: 15.4929; acc: 194910.0/195424.0=0.9974\n","     Instance: 15473; Time: 12.76s; loss: 10.6019; acc: 201073.0/201595.0=0.9974\n","Epoch: 14 training finished. Time: 405.20s, speed: 38.19st/s,  total loss: 638.5545654296875\n","gold_num =  11549  pred_num =  11383  right_num =  10947\n","Dev: time: 67.90s, speed: 91.31st/s; acc: 0.9898, p: 0.9617, r: 0.9479, f: 0.9547\n","gold_num =  7933  pred_num =  7823  right_num =  7512\n","Test: time: 44.62s, speed: 92.65st/s; acc: 0.9896, p: 0.9602, r: 0.9469, f: 0.9535\n","Epoch: 15/25\n"," Learning rate is setted as: 0.006949368452396296\n","Instance: 500; Time: 13.11s; loss: 19.5897; acc: 6552.0/6564.0=0.9982\n","Instance: 1000; Time: 12.48s; loss: 13.1458; acc: 12702.0/12717.0=0.9988\n","Instance: 1500; Time: 13.50s; loss: 22.1682; acc: 19444.0/19480.0=0.9982\n","Instance: 2000; Time: 12.94s; loss: 31.7772; acc: 25982.0/26039.0=0.9978\n","Instance: 2500; Time: 13.36s; loss: 13.0522; acc: 32498.0/32571.0=0.9978\n","Instance: 3000; Time: 13.08s; loss: 20.0382; acc: 39083.0/39172.0=0.9977\n","Instance: 3500; Time: 13.29s; loss: 19.1437; acc: 45636.0/45741.0=0.9977\n","Instance: 4000; Time: 12.96s; loss: 16.6416; acc: 52081.0/52200.0=0.9977\n","Instance: 4500; Time: 12.61s; loss: 21.2966; acc: 58412.0/58546.0=0.9977\n","Instance: 5000; Time: 13.04s; loss: 17.5782; acc: 64930.0/65076.0=0.9978\n","Instance: 5500; Time: 13.18s; loss: 23.8861; acc: 71367.0/71533.0=0.9977\n","Instance: 6000; Time: 12.79s; loss: 19.2068; acc: 77706.0/77882.0=0.9977\n","Instance: 6500; Time: 13.51s; loss: 16.4396; acc: 84377.0/84575.0=0.9977\n","Instance: 7000; Time: 13.13s; loss: 22.8569; acc: 90942.0/91163.0=0.9976\n","Instance: 7500; Time: 12.92s; loss: 18.7427; acc: 97452.0/97691.0=0.9976\n","Instance: 8000; Time: 12.92s; loss: 25.3577; acc: 103917.0/104178.0=0.9975\n","Instance: 8500; Time: 13.04s; loss: 11.6155; acc: 110408.0/110678.0=0.9976\n","Instance: 9000; Time: 13.18s; loss: 14.6460; acc: 116901.0/117180.0=0.9976\n","Instance: 9500; Time: 12.86s; loss: 15.8333; acc: 123323.0/123616.0=0.9976\n","Instance: 10000; Time: 13.18s; loss: 11.6786; acc: 129751.0/130054.0=0.9977\n","Instance: 10500; Time: 12.97s; loss: 11.1136; acc: 136191.0/136502.0=0.9977\n","Instance: 11000; Time: 13.22s; loss: 20.0630; acc: 142845.0/143170.0=0.9977\n","Instance: 11500; Time: 13.21s; loss: 19.5815; acc: 149292.0/149632.0=0.9977\n","Instance: 12000; Time: 13.52s; loss: 27.5543; acc: 155943.0/156302.0=0.9977\n","Instance: 12500; Time: 13.02s; loss: 46.7825; acc: 162301.0/162692.0=0.9976\n","Instance: 13000; Time: 13.06s; loss: 18.3654; acc: 168851.0/169256.0=0.9976\n","Instance: 13500; Time: 13.30s; loss: 18.9303; acc: 175427.0/175853.0=0.9976\n","Instance: 14000; Time: 13.17s; loss: 34.4082; acc: 181978.0/182423.0=0.9976\n","Instance: 14500; Time: 12.87s; loss: 11.3506; acc: 188355.0/188806.0=0.9976\n","Instance: 15000; Time: 13.57s; loss: 17.2209; acc: 195097.0/195558.0=0.9976\n","     Instance: 15473; Time: 12.25s; loss: 17.8845; acc: 201120.0/201595.0=0.9976\n","Epoch: 15 training finished. Time: 405.25s, speed: 38.18st/s,  total loss: 617.949462890625\n","gold_num =  11549  pred_num =  11331  right_num =  10905\n","Dev: time: 68.16s, speed: 90.96st/s; acc: 0.9898, p: 0.9624, r: 0.9442, f: 0.9532\n","gold_num =  7933  pred_num =  7785  right_num =  7476\n","Test: time: 44.53s, speed: 92.85st/s; acc: 0.9891, p: 0.9603, r: 0.9424, f: 0.9513\n","Epoch: 16/25\n"," Learning rate is setted as: 0.00660190002977648\n","Instance: 500; Time: 12.70s; loss: 18.3550; acc: 6501.0/6516.0=0.9977\n","Instance: 1000; Time: 13.51s; loss: 13.4077; acc: 13089.0/13114.0=0.9981\n","Instance: 1500; Time: 12.94s; loss: 26.4236; acc: 19621.0/19664.0=0.9978\n","Instance: 2000; Time: 12.67s; loss: 18.1307; acc: 25907.0/25973.0=0.9975\n","Instance: 2500; Time: 13.72s; loss: 15.3103; acc: 32618.0/32688.0=0.9979\n","Instance: 3000; Time: 13.38s; loss: 20.3319; acc: 39312.0/39404.0=0.9977\n","Instance: 3500; Time: 13.44s; loss: 15.7764; acc: 45995.0/46097.0=0.9978\n","Instance: 4000; Time: 13.04s; loss: 23.2444; acc: 52585.0/52698.0=0.9979\n","Instance: 4500; Time: 13.64s; loss: 26.3223; acc: 59220.0/59351.0=0.9978\n","Instance: 5000; Time: 12.89s; loss: 19.8916; acc: 65502.0/65647.0=0.9978\n","Instance: 5500; Time: 12.88s; loss: 12.5389; acc: 71811.0/71966.0=0.9978\n","Instance: 6000; Time: 12.70s; loss: 24.5477; acc: 78187.0/78364.0=0.9977\n","Instance: 6500; Time: 12.98s; loss: 22.5583; acc: 84718.0/84916.0=0.9977\n","Instance: 7000; Time: 12.91s; loss: 18.4902; acc: 91157.0/91374.0=0.9976\n","Instance: 7500; Time: 13.18s; loss: 17.0609; acc: 97691.0/97924.0=0.9976\n","Instance: 8000; Time: 13.05s; loss: 14.8741; acc: 104148.0/104394.0=0.9976\n","Instance: 8500; Time: 13.24s; loss: 13.9573; acc: 110631.0/110890.0=0.9977\n","Instance: 9000; Time: 13.14s; loss: 19.7390; acc: 117187.0/117459.0=0.9977\n","Instance: 9500; Time: 12.74s; loss: 15.0258; acc: 123636.0/123918.0=0.9977\n","Instance: 10000; Time: 13.32s; loss: 14.7352; acc: 130116.0/130408.0=0.9978\n","Instance: 10500; Time: 12.73s; loss: 26.0031; acc: 136445.0/136752.0=0.9978\n","Instance: 11000; Time: 12.61s; loss: 15.5164; acc: 142803.0/143117.0=0.9978\n","Instance: 11500; Time: 13.52s; loss: 21.1301; acc: 149401.0/149737.0=0.9978\n","Instance: 12000; Time: 12.93s; loss: 16.2333; acc: 155785.0/156134.0=0.9978\n","Instance: 12500; Time: 12.76s; loss: 13.7136; acc: 162179.0/162533.0=0.9978\n","Instance: 13000; Time: 13.15s; loss: 17.2253; acc: 168731.0/169095.0=0.9978\n","Instance: 13500; Time: 13.43s; loss: 17.7930; acc: 175333.0/175711.0=0.9978\n","Instance: 14000; Time: 12.83s; loss: 13.5078; acc: 181648.0/182034.0=0.9979\n","Instance: 14500; Time: 13.54s; loss: 18.8231; acc: 188415.0/188816.0=0.9979\n","Instance: 15000; Time: 13.67s; loss: 17.3741; acc: 195235.0/195649.0=0.9979\n","     Instance: 15473; Time: 11.97s; loss: 17.3895; acc: 201164.0/201595.0=0.9979\n","Epoch: 16 training finished. Time: 405.24s, speed: 38.18st/s,  total loss: 565.4307250976562\n","gold_num =  11549  pred_num =  11401  right_num =  10999\n","Dev: time: 68.25s, speed: 90.83st/s; acc: 0.9907, p: 0.9647, r: 0.9524, f: 0.9585\n","Exceed previous best f score: 0.9548544535471502\n","gold_num =  7933  pred_num =  7828  right_num =  7523\n","Test: time: 44.95s, speed: 92.05st/s; acc: 0.9896, p: 0.9610, r: 0.9483, f: 0.9546\n","Epoch: 17/25\n"," Learning rate is setted as: 0.006271805028287656\n","Instance: 500; Time: 13.12s; loss: 17.2910; acc: 6536.0/6544.0=0.9988\n","Instance: 1000; Time: 13.48s; loss: 13.1202; acc: 13160.0/13176.0=0.9988\n","Instance: 1500; Time: 12.86s; loss: 12.1750; acc: 19534.0/19563.0=0.9985\n","Instance: 2000; Time: 13.28s; loss: 17.4937; acc: 26098.0/26145.0=0.9982\n","Instance: 2500; Time: 12.71s; loss: 20.9956; acc: 32422.0/32484.0=0.9981\n","Instance: 3000; Time: 13.31s; loss: 14.9518; acc: 39008.0/39088.0=0.9980\n","Instance: 3500; Time: 13.03s; loss: 15.9613; acc: 45476.0/45573.0=0.9979\n","Instance: 4000; Time: 13.35s; loss: 11.6639; acc: 52177.0/52280.0=0.9980\n","Instance: 4500; Time: 12.84s; loss: 16.2362; acc: 58516.0/58631.0=0.9980\n","Instance: 5000; Time: 12.86s; loss: 11.4738; acc: 64883.0/65007.0=0.9981\n","Instance: 5500; Time: 13.31s; loss: 21.3959; acc: 71441.0/71579.0=0.9981\n","Instance: 6000; Time: 12.85s; loss: 8.6931; acc: 77910.0/78057.0=0.9981\n","Instance: 6500; Time: 12.94s; loss: 16.0046; acc: 84390.0/84547.0=0.9981\n","Instance: 7000; Time: 13.15s; loss: 20.8445; acc: 90912.0/91075.0=0.9982\n","Instance: 7500; Time: 13.54s; loss: 25.6412; acc: 97580.0/97765.0=0.9981\n","Instance: 8000; Time: 13.22s; loss: 20.7126; acc: 104219.0/104420.0=0.9981\n","Instance: 8500; Time: 12.95s; loss: 7.7616; acc: 110679.0/110883.0=0.9982\n","Instance: 9000; Time: 13.54s; loss: 16.9402; acc: 117402.0/117618.0=0.9982\n","Instance: 9500; Time: 13.68s; loss: 21.9357; acc: 123985.0/124213.0=0.9982\n","Instance: 10000; Time: 13.13s; loss: 22.7626; acc: 130544.0/130793.0=0.9981\n","Instance: 10500; Time: 13.15s; loss: 12.1227; acc: 137056.0/137312.0=0.9981\n","Instance: 11000; Time: 12.76s; loss: 6.1896; acc: 143401.0/143659.0=0.9982\n","Instance: 11500; Time: 13.33s; loss: 15.9900; acc: 149895.0/150164.0=0.9982\n","Instance: 12000; Time: 12.85s; loss: 15.7799; acc: 156223.0/156507.0=0.9982\n","Instance: 12500; Time: 13.22s; loss: 17.5189; acc: 162722.0/163016.0=0.9982\n","Instance: 13000; Time: 12.79s; loss: 20.1016; acc: 169164.0/169470.0=0.9982\n","Instance: 13500; Time: 12.79s; loss: 15.9873; acc: 175494.0/175810.0=0.9982\n","Instance: 14000; Time: 12.93s; loss: 15.4425; acc: 181967.0/182295.0=0.9982\n","Instance: 14500; Time: 12.88s; loss: 10.4448; acc: 188379.0/188713.0=0.9982\n","Instance: 15000; Time: 13.29s; loss: 13.8379; acc: 194975.0/195323.0=0.9982\n","     Instance: 15473; Time: 12.93s; loss: 21.2690; acc: 201230.0/201595.0=0.9982\n","Epoch: 17 training finished. Time: 406.08s, speed: 38.10st/s,  total loss: 498.73876953125\n","gold_num =  11549  pred_num =  11413  right_num =  11011\n","Dev: time: 67.85s, speed: 91.38st/s; acc: 0.9907, p: 0.9648, r: 0.9534, f: 0.9591\n","Exceed previous best f score: 0.9585185185185184\n","gold_num =  7933  pred_num =  7836  right_num =  7549\n","Test: time: 44.81s, speed: 92.36st/s; acc: 0.9903, p: 0.9634, r: 0.9516, f: 0.9574\n","Epoch: 18/25\n"," Learning rate is setted as: 0.005958214776873273\n","Instance: 500; Time: 12.72s; loss: 19.8036; acc: 6440.0/6463.0=0.9964\n","Instance: 1000; Time: 13.23s; loss: 14.4695; acc: 13013.0/13047.0=0.9974\n","Instance: 1500; Time: 13.31s; loss: 10.5887; acc: 19456.0/19503.0=0.9976\n","Instance: 2000; Time: 13.48s; loss: 16.5288; acc: 26054.0/26111.0=0.9978\n","Instance: 2500; Time: 13.42s; loss: 12.0092; acc: 32737.0/32798.0=0.9981\n","Instance: 3000; Time: 12.85s; loss: 11.6655; acc: 39061.0/39130.0=0.9982\n","Instance: 3500; Time: 13.05s; loss: 16.8210; acc: 45568.0/45651.0=0.9982\n","Instance: 4000; Time: 13.12s; loss: 11.2745; acc: 51988.0/52079.0=0.9983\n","Instance: 4500; Time: 13.11s; loss: 12.5872; acc: 58421.0/58527.0=0.9982\n","Instance: 5000; Time: 12.95s; loss: 13.1368; acc: 64767.0/64881.0=0.9982\n","Instance: 5500; Time: 13.21s; loss: 17.2240; acc: 71316.0/71445.0=0.9982\n","Instance: 6000; Time: 13.84s; loss: 15.2656; acc: 78063.0/78203.0=0.9982\n","Instance: 6500; Time: 12.76s; loss: 11.2417; acc: 84501.0/84647.0=0.9983\n","Instance: 7000; Time: 13.25s; loss: 16.6522; acc: 90939.0/91097.0=0.9983\n","Instance: 7500; Time: 13.55s; loss: 12.5356; acc: 97665.0/97835.0=0.9983\n","Instance: 8000; Time: 12.85s; loss: 12.9419; acc: 104086.0/104266.0=0.9983\n","Instance: 8500; Time: 13.32s; loss: 20.6105; acc: 110679.0/110877.0=0.9982\n","Instance: 9000; Time: 13.51s; loss: 12.9338; acc: 117347.0/117553.0=0.9982\n","Instance: 9500; Time: 12.80s; loss: 17.8674; acc: 123835.0/124057.0=0.9982\n","Instance: 10000; Time: 12.54s; loss: 8.9604; acc: 130095.0/130323.0=0.9983\n","Instance: 10500; Time: 12.78s; loss: 13.3901; acc: 136337.0/136578.0=0.9982\n","Instance: 11000; Time: 12.63s; loss: 13.8203; acc: 142556.0/142809.0=0.9982\n","Instance: 11500; Time: 13.51s; loss: 17.7162; acc: 149224.0/149488.0=0.9982\n","Instance: 12000; Time: 13.30s; loss: 14.0665; acc: 155886.0/156156.0=0.9983\n","Instance: 12500; Time: 12.79s; loss: 20.7477; acc: 162271.0/162554.0=0.9983\n","Instance: 13000; Time: 13.37s; loss: 10.2306; acc: 168877.0/169169.0=0.9983\n","Instance: 13500; Time: 13.16s; loss: 12.6111; acc: 175391.0/175693.0=0.9983\n","Instance: 14000; Time: 13.22s; loss: 13.7809; acc: 181869.0/182181.0=0.9983\n","Instance: 14500; Time: 13.04s; loss: 9.9135; acc: 188474.0/188790.0=0.9983\n","Instance: 15000; Time: 13.46s; loss: 24.4829; acc: 195110.0/195449.0=0.9983\n","     Instance: 15473; Time: 12.62s; loss: 11.3391; acc: 201253.0/201595.0=0.9983\n","Epoch: 18 training finished. Time: 406.76s, speed: 38.04st/s,  total loss: 447.21697998046875\n","gold_num =  11549  pred_num =  11380  right_num =  10979\n","Dev: time: 67.67s, speed: 91.63st/s; acc: 0.9906, p: 0.9648, r: 0.9506, f: 0.9577\n","gold_num =  7933  pred_num =  7815  right_num =  7527\n","Test: time: 44.54s, speed: 92.83st/s; acc: 0.9900, p: 0.9631, r: 0.9488, f: 0.9559\n","Epoch: 19/25\n"," Learning rate is setted as: 0.005660304038029609\n","Instance: 500; Time: 12.91s; loss: 17.0909; acc: 6475.0/6493.0=0.9972\n","Instance: 1000; Time: 12.36s; loss: 4.4235; acc: 12680.0/12700.0=0.9984\n","Instance: 1500; Time: 12.88s; loss: 16.5944; acc: 18955.0/19001.0=0.9976\n","Instance: 2000; Time: 13.11s; loss: 9.4706; acc: 25488.0/25544.0=0.9978\n","Instance: 2500; Time: 13.43s; loss: 25.7478; acc: 32089.0/32161.0=0.9978\n","Instance: 3000; Time: 12.93s; loss: 8.3698; acc: 38436.0/38510.0=0.9981\n","Instance: 3500; Time: 13.54s; loss: 11.9797; acc: 44988.0/45076.0=0.9980\n","Instance: 4000; Time: 13.65s; loss: 13.7612; acc: 51637.0/51738.0=0.9980\n","Instance: 4500; Time: 12.76s; loss: 17.2935; acc: 58089.0/58201.0=0.9981\n","Instance: 5000; Time: 13.56s; loss: 16.5370; acc: 64743.0/64866.0=0.9981\n","Instance: 5500; Time: 13.60s; loss: 16.4518; acc: 71476.0/71607.0=0.9982\n","Instance: 6000; Time: 13.16s; loss: 8.8539; acc: 77967.0/78104.0=0.9982\n","Instance: 6500; Time: 12.69s; loss: 13.0403; acc: 84386.0/84534.0=0.9982\n","Instance: 7000; Time: 14.27s; loss: 28.4741; acc: 91429.0/91599.0=0.9981\n","Instance: 7500; Time: 13.09s; loss: 13.2318; acc: 97892.0/98077.0=0.9981\n","Instance: 8000; Time: 13.12s; loss: 16.9330; acc: 104377.0/104580.0=0.9981\n","Instance: 8500; Time: 13.22s; loss: 7.6189; acc: 110898.0/111104.0=0.9981\n","Instance: 9000; Time: 12.93s; loss: 33.6088; acc: 117242.0/117468.0=0.9981\n","Instance: 9500; Time: 13.22s; loss: 28.1307; acc: 123723.0/123964.0=0.9981\n","Instance: 10000; Time: 12.98s; loss: 10.2747; acc: 130244.0/130493.0=0.9981\n","Instance: 10500; Time: 13.52s; loss: 15.3295; acc: 136870.0/137127.0=0.9981\n","Instance: 11000; Time: 12.82s; loss: 13.4023; acc: 143199.0/143476.0=0.9981\n","Instance: 11500; Time: 12.48s; loss: 7.3424; acc: 149513.0/149792.0=0.9981\n","Instance: 12000; Time: 13.53s; loss: 16.6537; acc: 156141.0/156432.0=0.9981\n","Instance: 12500; Time: 13.18s; loss: 20.0524; acc: 162591.0/162896.0=0.9981\n","Instance: 13000; Time: 13.23s; loss: 14.5063; acc: 169167.0/169489.0=0.9981\n","Instance: 13500; Time: 12.43s; loss: 12.6344; acc: 175321.0/175655.0=0.9981\n","Instance: 14000; Time: 13.05s; loss: 20.0474; acc: 181781.0/182129.0=0.9981\n","Instance: 14500; Time: 13.21s; loss: 19.9413; acc: 188342.0/188705.0=0.9981\n","Instance: 15000; Time: 13.26s; loss: 10.1877; acc: 194996.0/195363.0=0.9981\n","     Instance: 15473; Time: 12.76s; loss: 14.4011; acc: 201213.0/201595.0=0.9981\n","Epoch: 19 training finished. Time: 406.91s, speed: 38.03st/s,  total loss: 482.38470458984375\n","gold_num =  11549  pred_num =  11361  right_num =  10963\n","Dev: time: 68.55s, speed: 90.44st/s; acc: 0.9906, p: 0.9650, r: 0.9493, f: 0.9570\n","gold_num =  7933  pred_num =  7799  right_num =  7504\n","Test: time: 45.40s, speed: 91.07st/s; acc: 0.9896, p: 0.9622, r: 0.9459, f: 0.9540\n","Epoch: 20/25\n"," Learning rate is setted as: 0.005377288836128128\n","Instance: 500; Time: 13.43s; loss: 12.2329; acc: 6807.0/6818.0=0.9984\n","Instance: 1000; Time: 12.95s; loss: 15.3413; acc: 13181.0/13200.0=0.9986\n","Instance: 1500; Time: 13.21s; loss: 13.7203; acc: 19609.0/19636.0=0.9986\n","Instance: 2000; Time: 13.08s; loss: 12.0955; acc: 26065.0/26102.0=0.9986\n","Instance: 2500; Time: 13.05s; loss: 9.8826; acc: 32546.0/32595.0=0.9985\n","Instance: 3000; Time: 13.35s; loss: 11.9395; acc: 38983.0/39038.0=0.9986\n","Instance: 3500; Time: 13.10s; loss: 12.2303; acc: 45499.0/45562.0=0.9986\n","Instance: 4000; Time: 13.17s; loss: 13.4297; acc: 52027.0/52097.0=0.9987\n","Instance: 4500; Time: 13.40s; loss: 11.8077; acc: 58637.0/58718.0=0.9986\n","Instance: 5000; Time: 13.80s; loss: 16.3279; acc: 65447.0/65537.0=0.9986\n","Instance: 5500; Time: 12.85s; loss: 8.5063; acc: 71771.0/71873.0=0.9986\n","Instance: 6000; Time: 12.82s; loss: 9.3776; acc: 78106.0/78212.0=0.9986\n","Instance: 6500; Time: 13.17s; loss: 10.1685; acc: 84660.0/84772.0=0.9987\n","Instance: 7000; Time: 12.85s; loss: 8.2286; acc: 91017.0/91134.0=0.9987\n","Instance: 7500; Time: 13.18s; loss: 10.2953; acc: 97382.0/97507.0=0.9987\n","Instance: 8000; Time: 12.73s; loss: 8.1783; acc: 103815.0/103945.0=0.9987\n","Instance: 8500; Time: 13.87s; loss: 16.8357; acc: 110637.0/110783.0=0.9987\n","Instance: 9000; Time: 13.42s; loss: 19.8545; acc: 117215.0/117376.0=0.9986\n","Instance: 9500; Time: 13.17s; loss: 16.2844; acc: 123676.0/123847.0=0.9986\n","Instance: 10000; Time: 12.34s; loss: 12.1727; acc: 129850.0/130031.0=0.9986\n","Instance: 10500; Time: 13.08s; loss: 10.1667; acc: 136178.0/136364.0=0.9986\n","Instance: 11000; Time: 12.94s; loss: 27.8348; acc: 142550.0/142758.0=0.9985\n","Instance: 11500; Time: 13.24s; loss: 12.9293; acc: 149122.0/149338.0=0.9986\n","Instance: 12000; Time: 13.60s; loss: 21.2007; acc: 155774.0/156009.0=0.9985\n","Instance: 12500; Time: 13.21s; loss: 14.3740; acc: 162215.0/162458.0=0.9985\n","Instance: 13000; Time: 12.97s; loss: 14.5166; acc: 168709.0/168960.0=0.9985\n","Instance: 13500; Time: 13.54s; loss: 16.3322; acc: 175456.0/175717.0=0.9985\n","Instance: 14000; Time: 13.10s; loss: 9.3259; acc: 181960.0/182225.0=0.9985\n","Instance: 14500; Time: 13.81s; loss: 16.0784; acc: 188748.0/189027.0=0.9985\n","     Instance: 15473; Time: 12.20s; loss: 11.4335; acc: 201308.0/201595.0=0.9986\n","Epoch: 20 training finished. Time: 407.44s, speed: 37.98st/s,  total loss: 410.9130859375\n","gold_num =  11549  pred_num =  11389  right_num =  11001\n","Dev: time: 68.48s, speed: 90.56st/s; acc: 0.9909, p: 0.9659, r: 0.9526, f: 0.9592\n","Exceed previous best f score: 0.9590627994077172\n","gold_num =  7933  pred_num =  7823  right_num =  7539\n","Test: time: 44.94s, speed: 92.08st/s; acc: 0.9901, p: 0.9637, r: 0.9503, f: 0.9570\n","Epoch: 21/25\n"," Learning rate is setted as: 0.005108424394321722\n","Instance: 500; Time: 13.02s; loss: 16.9403; acc: 6379.0/6399.0=0.9969\n","Instance: 1000; Time: 12.89s; loss: 10.4607; acc: 12882.0/12904.0=0.9983\n","Instance: 1500; Time: 13.30s; loss: 12.8107; acc: 19355.0/19382.0=0.9986\n","Instance: 2000; Time: 12.75s; loss: 5.4531; acc: 25664.0/25696.0=0.9988\n","Instance: 2500; Time: 12.90s; loss: 13.7498; acc: 32110.0/32150.0=0.9988\n","Instance: 3000; Time: 13.41s; loss: 8.8414; acc: 38675.0/38720.0=0.9988\n","Instance: 3500; Time: 13.84s; loss: 7.0990; acc: 45446.0/45493.0=0.9990\n","Instance: 4000; Time: 13.90s; loss: 17.0551; acc: 52165.0/52224.0=0.9989\n","Instance: 4500; Time: 13.56s; loss: 13.3514; acc: 58834.0/58910.0=0.9987\n","Instance: 5000; Time: 13.85s; loss: 19.5995; acc: 65686.0/65785.0=0.9985\n","Instance: 5500; Time: 13.16s; loss: 15.4310; acc: 72174.0/72285.0=0.9985\n","Instance: 6000; Time: 13.01s; loss: 11.6898; acc: 78641.0/78760.0=0.9985\n","Instance: 6500; Time: 12.74s; loss: 7.2931; acc: 85012.0/85135.0=0.9986\n","Instance: 7000; Time: 13.37s; loss: 12.2969; acc: 91589.0/91722.0=0.9985\n","Instance: 7500; Time: 13.57s; loss: 10.1031; acc: 98223.0/98364.0=0.9986\n","Instance: 8000; Time: 13.02s; loss: 16.1569; acc: 104666.0/104829.0=0.9984\n","Instance: 8500; Time: 12.79s; loss: 19.0608; acc: 111098.0/111273.0=0.9984\n","Instance: 9000; Time: 12.92s; loss: 9.2611; acc: 117480.0/117659.0=0.9985\n","Instance: 9500; Time: 12.61s; loss: 9.0900; acc: 123798.0/123985.0=0.9985\n","Instance: 10000; Time: 13.50s; loss: 7.4221; acc: 130404.0/130595.0=0.9985\n","Instance: 10500; Time: 12.99s; loss: 8.2869; acc: 136838.0/137035.0=0.9986\n","Instance: 11000; Time: 13.43s; loss: 21.9387; acc: 143466.0/143675.0=0.9985\n","Instance: 11500; Time: 13.14s; loss: 11.5883; acc: 150014.0/150231.0=0.9986\n","Instance: 12000; Time: 13.25s; loss: 10.6863; acc: 156542.0/156769.0=0.9986\n","Instance: 12500; Time: 13.13s; loss: 14.4935; acc: 162952.0/163189.0=0.9985\n","Instance: 13000; Time: 13.26s; loss: 19.2531; acc: 169508.0/169755.0=0.9985\n","Instance: 13500; Time: 13.41s; loss: 21.2903; acc: 176178.0/176441.0=0.9985\n","Instance: 14000; Time: 12.50s; loss: 12.1154; acc: 182379.0/182646.0=0.9985\n","Instance: 14500; Time: 13.12s; loss: 14.4297; acc: 188922.0/189204.0=0.9985\n","Instance: 15000; Time: 13.00s; loss: 13.8632; acc: 195280.0/195570.0=0.9985\n","     Instance: 15473; Time: 12.24s; loss: 12.8052; acc: 201297.0/201595.0=0.9985\n","Epoch: 21 training finished. Time: 407.59s, speed: 37.96st/s,  total loss: 403.9161376953125\n","gold_num =  11549  pred_num =  11357  right_num =  10952\n","Dev: time: 68.66s, speed: 90.32st/s; acc: 0.9904, p: 0.9643, r: 0.9483, f: 0.9563\n","gold_num =  7933  pred_num =  7797  right_num =  7518\n","Test: time: 45.02s, speed: 91.83st/s; acc: 0.9901, p: 0.9642, r: 0.9477, f: 0.9559\n","Epoch: 22/25\n"," Learning rate is setted as: 0.004853003174605635\n","Instance: 500; Time: 12.91s; loss: 13.1582; acc: 6417.0/6429.0=0.9981\n","Instance: 1000; Time: 13.16s; loss: 9.4391; acc: 13009.0/13034.0=0.9981\n","Instance: 1500; Time: 13.11s; loss: 16.6250; acc: 19547.0/19586.0=0.9980\n","Instance: 2000; Time: 13.43s; loss: 27.9885; acc: 26101.0/26172.0=0.9973\n","Instance: 2500; Time: 12.96s; loss: 9.6743; acc: 32591.0/32664.0=0.9978\n","Instance: 3000; Time: 13.43s; loss: 18.1732; acc: 39100.0/39193.0=0.9976\n","Instance: 3500; Time: 12.77s; loss: 12.3031; acc: 45494.0/45595.0=0.9978\n","Instance: 4000; Time: 13.51s; loss: 11.8585; acc: 52147.0/52258.0=0.9979\n","Instance: 4500; Time: 13.55s; loss: 13.6736; acc: 58736.0/58865.0=0.9978\n","Instance: 5000; Time: 13.23s; loss: 10.3781; acc: 65146.0/65277.0=0.9980\n","Instance: 5500; Time: 13.00s; loss: 7.0061; acc: 71488.0/71621.0=0.9981\n","Instance: 6000; Time: 12.43s; loss: 8.9434; acc: 77677.0/77816.0=0.9982\n","Instance: 6500; Time: 13.51s; loss: 10.2058; acc: 84338.0/84487.0=0.9982\n","Instance: 7000; Time: 13.84s; loss: 6.8826; acc: 91127.0/91278.0=0.9983\n","Instance: 7500; Time: 13.52s; loss: 11.7571; acc: 97782.0/97949.0=0.9983\n","Instance: 8000; Time: 13.07s; loss: 11.3403; acc: 104153.0/104328.0=0.9983\n","Instance: 8500; Time: 13.40s; loss: 5.9178; acc: 110732.0/110911.0=0.9984\n","Instance: 9000; Time: 13.50s; loss: 14.2308; acc: 117305.0/117501.0=0.9983\n","Instance: 9500; Time: 12.97s; loss: 7.7769; acc: 123794.0/123992.0=0.9984\n","Instance: 10000; Time: 13.09s; loss: 16.0337; acc: 130317.0/130533.0=0.9983\n","Instance: 10500; Time: 13.30s; loss: 23.4779; acc: 136783.0/137018.0=0.9983\n","Instance: 11000; Time: 13.64s; loss: 6.4144; acc: 143439.0/143678.0=0.9983\n","Instance: 11500; Time: 12.99s; loss: 13.0676; acc: 149831.0/150083.0=0.9983\n","Instance: 12000; Time: 13.68s; loss: 10.8651; acc: 156502.0/156762.0=0.9983\n","Instance: 12500; Time: 12.99s; loss: 9.7579; acc: 162952.0/163231.0=0.9983\n","Instance: 13000; Time: 13.42s; loss: 15.9845; acc: 169662.0/169951.0=0.9983\n","Instance: 13500; Time: 13.02s; loss: 7.6284; acc: 176086.0/176383.0=0.9983\n","Instance: 14000; Time: 12.35s; loss: 8.6554; acc: 182354.0/182659.0=0.9983\n","Instance: 14500; Time: 12.90s; loss: 12.0096; acc: 188840.0/189156.0=0.9983\n","Instance: 15000; Time: 13.53s; loss: 10.2875; acc: 195441.0/195765.0=0.9983\n","     Instance: 15473; Time: 11.80s; loss: 17.4051; acc: 201260.0/201595.0=0.9983\n","Epoch: 22 training finished. Time: 408.01s, speed: 37.92st/s,  total loss: 378.91961669921875\n","gold_num =  11549  pred_num =  11363  right_num =  10962\n","Dev: time: 68.17s, speed: 90.97st/s; acc: 0.9904, p: 0.9647, r: 0.9492, f: 0.9569\n","gold_num =  7933  pred_num =  7802  right_num =  7525\n","Test: time: 44.87s, speed: 92.13st/s; acc: 0.9902, p: 0.9645, r: 0.9486, f: 0.9565\n","Epoch: 23/25\n"," Learning rate is setted as: 0.004610353015875353\n","Instance: 500; Time: 13.27s; loss: 10.3927; acc: 6550.0/6556.0=0.9991\n","Instance: 1000; Time: 13.18s; loss: 8.3206; acc: 12990.0/13002.0=0.9991\n","Instance: 1500; Time: 13.33s; loss: 7.8173; acc: 19558.0/19575.0=0.9991\n","Instance: 2000; Time: 13.15s; loss: 9.7490; acc: 26120.0/26144.0=0.9991\n","Instance: 2500; Time: 13.43s; loss: 8.1782; acc: 32804.0/32833.0=0.9991\n","Instance: 3000; Time: 13.20s; loss: 11.0054; acc: 39328.0/39369.0=0.9990\n","Instance: 3500; Time: 12.97s; loss: 9.6558; acc: 45559.0/45613.0=0.9988\n","Instance: 4000; Time: 13.66s; loss: 8.2815; acc: 52248.0/52307.0=0.9989\n","Instance: 4500; Time: 13.10s; loss: 6.4058; acc: 58620.0/58684.0=0.9989\n","Instance: 5000; Time: 13.12s; loss: 13.2460; acc: 65014.0/65090.0=0.9988\n","Instance: 5500; Time: 12.65s; loss: 7.2434; acc: 71307.0/71390.0=0.9988\n","Instance: 6000; Time: 13.25s; loss: 5.3121; acc: 77881.0/77968.0=0.9989\n","Instance: 6500; Time: 13.31s; loss: 17.8292; acc: 84408.0/84505.0=0.9989\n","Instance: 7000; Time: 13.58s; loss: 16.3417; acc: 91013.0/91128.0=0.9987\n","Instance: 7500; Time: 13.22s; loss: 12.6769; acc: 97563.0/97692.0=0.9987\n","Instance: 8000; Time: 12.74s; loss: 20.2875; acc: 103897.0/104035.0=0.9987\n","Instance: 8500; Time: 13.16s; loss: 20.2411; acc: 110444.0/110598.0=0.9986\n","Instance: 9000; Time: 13.20s; loss: 14.3488; acc: 116939.0/117103.0=0.9986\n","Instance: 9500; Time: 13.48s; loss: 11.0549; acc: 123670.0/123840.0=0.9986\n","Instance: 10000; Time: 13.30s; loss: 14.7498; acc: 130218.0/130398.0=0.9986\n","Instance: 10500; Time: 13.40s; loss: 16.3647; acc: 136759.0/136955.0=0.9986\n","Instance: 11000; Time: 13.01s; loss: 10.8301; acc: 143242.0/143450.0=0.9986\n","Instance: 11500; Time: 13.27s; loss: 12.8000; acc: 149899.0/150113.0=0.9986\n","Instance: 12000; Time: 12.98s; loss: 7.1647; acc: 156386.0/156606.0=0.9986\n","Instance: 12500; Time: 13.00s; loss: 7.2469; acc: 162946.0/163167.0=0.9986\n","Instance: 13000; Time: 12.89s; loss: 9.7781; acc: 169307.0/169536.0=0.9986\n","Instance: 13500; Time: 13.03s; loss: 9.5160; acc: 175771.0/176006.0=0.9987\n","Instance: 14000; Time: 13.08s; loss: 8.2645; acc: 182262.0/182505.0=0.9987\n","Instance: 14500; Time: 12.61s; loss: 12.2789; acc: 188669.0/188923.0=0.9987\n","Instance: 15000; Time: 12.43s; loss: 20.8889; acc: 194909.0/195181.0=0.9986\n","     Instance: 15473; Time: 12.78s; loss: 21.0866; acc: 201299.0/201595.0=0.9985\n","Epoch: 23 training finished. Time: 406.80s, speed: 38.04st/s,  total loss: 369.35699462890625\n","gold_num =  11549  pred_num =  11388  right_num =  11015\n","Dev: time: 67.72s, speed: 91.56st/s; acc: 0.9912, p: 0.9672, r: 0.9538, f: 0.9605\n","Exceed previous best f score: 0.9591943499869213\n","gold_num =  7933  pred_num =  7822  right_num =  7550\n","Test: time: 44.68s, speed: 92.61st/s; acc: 0.9907, p: 0.9652, r: 0.9517, f: 0.9584\n","Epoch: 24/25\n"," Learning rate is setted as: 0.004379835365081586\n","Instance: 500; Time: 12.94s; loss: 6.8700; acc: 6561.0/6567.0=0.9991\n","Instance: 1000; Time: 13.28s; loss: 20.9158; acc: 13285.0/13311.0=0.9980\n","Instance: 1500; Time: 13.08s; loss: 10.0841; acc: 19748.0/19781.0=0.9983\n","Instance: 2000; Time: 13.50s; loss: 8.7260; acc: 26465.0/26506.0=0.9985\n","Instance: 2500; Time: 13.41s; loss: 19.4552; acc: 33087.0/33143.0=0.9983\n","Instance: 3000; Time: 13.09s; loss: 8.8455; acc: 39537.0/39599.0=0.9984\n","Instance: 3500; Time: 13.39s; loss: 12.2773; acc: 46150.0/46220.0=0.9985\n","Instance: 4000; Time: 12.69s; loss: 16.6189; acc: 52535.0/52617.0=0.9984\n","Instance: 4500; Time: 13.43s; loss: 12.1272; acc: 59158.0/59249.0=0.9985\n","Instance: 5000; Time: 13.39s; loss: 10.5522; acc: 65735.0/65836.0=0.9985\n","Instance: 5500; Time: 12.79s; loss: 12.7877; acc: 72172.0/72282.0=0.9985\n","Instance: 6000; Time: 12.86s; loss: 5.5382; acc: 78543.0/78659.0=0.9985\n","Instance: 6500; Time: 13.06s; loss: 18.4393; acc: 84914.0/85046.0=0.9984\n","Instance: 7000; Time: 12.93s; loss: 6.9796; acc: 91405.0/91542.0=0.9985\n","Instance: 7500; Time: 13.62s; loss: 8.9720; acc: 98236.0/98378.0=0.9986\n","Instance: 8000; Time: 13.41s; loss: 15.6366; acc: 104826.0/104983.0=0.9985\n","Instance: 8500; Time: 12.88s; loss: 14.7397; acc: 111344.0/111510.0=0.9985\n","Instance: 9000; Time: 13.00s; loss: 6.8987; acc: 117861.0/118027.0=0.9986\n","Instance: 9500; Time: 13.09s; loss: 11.9325; acc: 124413.0/124595.0=0.9985\n","Instance: 10000; Time: 13.08s; loss: 10.5758; acc: 130909.0/131097.0=0.9986\n","Instance: 10500; Time: 12.96s; loss: 9.9604; acc: 137339.0/137535.0=0.9986\n","Instance: 11000; Time: 12.68s; loss: 18.9639; acc: 143769.0/143979.0=0.9985\n","Instance: 11500; Time: 12.88s; loss: 9.4186; acc: 150237.0/150455.0=0.9986\n","Instance: 12000; Time: 13.07s; loss: 11.4441; acc: 156729.0/156957.0=0.9985\n","Instance: 12500; Time: 13.30s; loss: 4.9398; acc: 163343.0/163573.0=0.9986\n","Instance: 13000; Time: 12.57s; loss: 14.8743; acc: 169641.0/169882.0=0.9986\n","Instance: 13500; Time: 12.35s; loss: 14.5563; acc: 175808.0/176059.0=0.9986\n","Instance: 14000; Time: 13.33s; loss: 7.0443; acc: 182381.0/182636.0=0.9986\n","Instance: 14500; Time: 12.84s; loss: 14.6180; acc: 188788.0/189059.0=0.9986\n","Instance: 15000; Time: 12.93s; loss: 12.9633; acc: 195285.0/195562.0=0.9986\n","     Instance: 15473; Time: 12.47s; loss: 7.6647; acc: 201314.0/201595.0=0.9986\n","Epoch: 24 training finished. Time: 404.30s, speed: 38.27st/s,  total loss: 365.42010498046875\n","gold_num =  11549  pred_num =  11366  right_num =  10969\n","Dev: time: 68.19s, speed: 90.94st/s; acc: 0.9906, p: 0.9651, r: 0.9498, f: 0.9574\n","gold_num =  7933  pred_num =  7808  right_num =  7522\n","Test: time: 44.43s, speed: 93.09st/s; acc: 0.9901, p: 0.9634, r: 0.9482, f: 0.9557\n"]}]},{"cell_type":"code","source":["speed, acc, p, r, f, _, right_num = evaluate(data, model, \"test\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fNUY--v-OAh-","executionInfo":{"status":"ok","timestamp":1648032306429,"user_tz":-480,"elapsed":45825,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15214000089252266369"}},"outputId":"c85b5fa2-f343-4820-cead-f1421b5aafe8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["__main__:185: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:186: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:187: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:188: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:208: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:263: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  word_var = autograd.Variable(torch.LongTensor(skip_input_[t][0]),volatile =  volatile_flag)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:156: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n","  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"]},{"output_type":"stream","name":"stdout","text":["gold_num =  7933  pred_num =  7808  right_num =  7522\n"]}]},{"cell_type":"code","source":["filename='/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/test_log.json'\n","with open(filename,'w',encoding='utf-8') as file_obj:\n","    json.dump(_,file_obj,ensure_ascii=False,indent = 4)"],"metadata":{"id":"fYDhdCxmOCWS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model,\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/final.model\")"],"metadata":{"id":"otAWHQVr1zva"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f=open('/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/log.txt','w',encoding='utf-8')\n","for i in dev_log:\n","  f.write(str(i)+'\\n')\n","f.close()"],"metadata":{"id":"CtrspepTQViW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["speed, acc, p, r, f, pred = evaluate(data, model, \"test\")"],"metadata":{"id":"ffMRllWt7um3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647792657814,"user_tz":-480,"elapsed":47944,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15214000089252266369"}},"outputId":"0359c271-ed0a-4aa3-bb1d-bfc10cde6d1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["__main__:184: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:185: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:186: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:187: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:207: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:263: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  word_var = autograd.Variable(torch.LongTensor(skip_input_[t][0]),volatile =  volatile_flag)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:156: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n","  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"]},{"output_type":"stream","name":"stdout","text":["gold_num =  7551  pred_num =  7500  right_num =  6922\n"]}]}]}