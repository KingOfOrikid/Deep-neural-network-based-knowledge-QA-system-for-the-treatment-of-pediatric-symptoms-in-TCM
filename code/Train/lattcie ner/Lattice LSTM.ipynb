{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6cdf2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "985da1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../cyx＆xy/lattice/ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8cc0ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bdb6a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\") \n",
    "warnings.warn(\"deprecated\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "226ecea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from utils.metric import get_ner_fmeasure\n",
    "from model.bilstmcrf import BiLSTM_CRF as SeqModel\n",
    "from utils.data import Data\n",
    "\n",
    "seed_num = 100\n",
    "random.seed(seed_num)\n",
    "torch.manual_seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "\n",
    "\n",
    "def data_initialization(data, gaz_file, train_file, dev_file, test_file):\n",
    "    data.build_alphabet(train_file)\n",
    "    data.build_alphabet(dev_file)\n",
    "    data.build_alphabet(test_file)\n",
    "    data.build_gaz_file(gaz_file)\n",
    "\n",
    "    #gaz_alphabet train,dev,test file在embedding中匹配到的词语\n",
    "    data.build_gaz_alphabet(train_file)\n",
    "    data.build_gaz_alphabet(dev_file)\n",
    "    data.build_gaz_alphabet(test_file)\n",
    "    data.fix_alphabet()\n",
    "    return data\n",
    "\n",
    "\n",
    "def predict_check(pred_variable, gold_variable, mask_variable):\n",
    "    \"\"\"\n",
    "        input:\n",
    "            pred_variable (batch_size, sent_len): pred tag result, in numpy format\n",
    "            gold_variable (batch_size, sent_len): gold  result variable\n",
    "            mask_variable (batch_size, sent_len): mask variable\n",
    "    \"\"\"\n",
    "    pred = pred_variable.cpu().data.numpy()\n",
    "    gold = gold_variable.cpu().data.numpy()\n",
    "    mask = mask_variable.cpu().data.numpy()\n",
    "    overlaped = (pred == gold)\n",
    "    right_token = np.sum(overlaped * mask)\n",
    "    total_token = mask.sum()\n",
    "    # print(\"right: %s, total: %s\"%(right_token, total_token))\n",
    "    return right_token, total_token\n",
    "\n",
    "\n",
    "def recover_label(pred_variable, gold_variable, mask_variable, label_alphabet, word_recover):\n",
    "    \"\"\"\n",
    "        input:\n",
    "            pred_variable (batch_size, sent_len): pred tag result\n",
    "            gold_variable (batch_size, sent_len): gold result variable\n",
    "            mask_variable (batch_size, sent_len): mask variable\n",
    "    \"\"\"\n",
    "    \n",
    "    pred_variable = pred_variable[word_recover]\n",
    "    gold_variable = gold_variable[word_recover]\n",
    "    mask_variable = mask_variable[word_recover]\n",
    "    batch_size = gold_variable.size(0)\n",
    "    seq_len = gold_variable.size(1)\n",
    "    mask = mask_variable.cpu().data.numpy()\n",
    "    pred_tag = pred_variable.cpu().data.numpy()\n",
    "    gold_tag = gold_variable.cpu().data.numpy()\n",
    "    batch_size = mask.shape[0]\n",
    "    pred_label = []\n",
    "    gold_label = []\n",
    "    for idx in range(batch_size):\n",
    "        pred = [label_alphabet.get_instance(pred_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n",
    "        gold = [label_alphabet.get_instance(gold_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n",
    "        # print \"p:\",pred, pred_tag.tolist()\n",
    "        # print \"g:\", gold, gold_tag.tolist()\n",
    "        assert(len(pred)==len(gold))\n",
    "        pred_label.append(pred)\n",
    "        gold_label.append(gold)\n",
    "    return pred_label, gold_label\n",
    "\n",
    "\n",
    "def save_data_setting(data, save_file):\n",
    "    \"\"\"\n",
    "    new_data = copy.deepcopy(data)\n",
    "    ## remove input instances\n",
    "    new_data.train_texts = []\n",
    "    new_data.dev_texts = []\n",
    "    new_data.test_texts = []\n",
    "    new_data.raw_texts = []\n",
    "    new_data.train_Ids = []\n",
    "    new_data.dev_Ids = []\n",
    "    new_data.test_Ids = []\n",
    "    new_data.raw_Ids = []\"\"\"\n",
    "    ## save data settings\n",
    "    with open(save_file, 'wb') as fp:\n",
    "        pickle.dump(data, fp)\n",
    "    print (\"Data setting saved to file: \", save_file)\n",
    "\n",
    "\n",
    "def load_data_setting(save_file):\n",
    "    with open(save_file, 'rb') as fp:\n",
    "        data = pickle.load(fp)\n",
    "    print (\"Data setting loaded from file: \", save_file)\n",
    "    data.show_data_summary()\n",
    "    return data\n",
    "\n",
    "def lr_decay(optimizer, epoch, decay_rate, init_lr):\n",
    "    lr = init_lr * ((1-decay_rate)**epoch)\n",
    "    print (\" Learning rate is setted as:\", lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(data, model, name):\n",
    "    if name == \"train\":\n",
    "        instances = data.train_Ids\n",
    "    elif name == \"dev\":\n",
    "        instances = data.dev_Ids\n",
    "    elif name == 'test':\n",
    "        instances = data.test_Ids\n",
    "    elif name == 'raw':\n",
    "        instances = data.raw_Ids\n",
    "    else:\n",
    "        print (\"Error: wrong evaluate name,\", name)\n",
    "    pred_results = []\n",
    "    gold_results = []\n",
    "    ## set model in eval model\n",
    "    model.eval()\n",
    "    batch_size = 10\n",
    "    start_time = time.time()\n",
    "    train_num = len(instances)\n",
    "    total_batch = train_num//batch_size+1\n",
    "    for batch_id in range(total_batch):\n",
    "        start = batch_id*batch_size\n",
    "        end = (batch_id+1)*batch_size \n",
    "        if end >train_num:\n",
    "            end =  train_num\n",
    "        instance = instances[start:end]\n",
    "        if not instance:\n",
    "            continue\n",
    "        gaz_list,batch_word, batch_biword, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu, True)\n",
    "        tag_seq = model(gaz_list,batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask)\n",
    "        # print \"tag:\",tag_seq\n",
    "        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet, batch_wordrecover)\n",
    "        pred_results += pred_label\n",
    "        gold_results += gold_label\n",
    "    decode_time = time.time() - start_time\n",
    "    speed = len(instances)/decode_time\n",
    "    acc, p, r, f, right_num = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)\n",
    "    return speed, acc, p, r, f, pred_results, right_num\n",
    "\n",
    "\n",
    "def batchify_with_label(input_batch_list, gpu, volatile_flag=False):\n",
    "    \"\"\"\n",
    "        input: list of words, chars and labels, various length. [[words,biwords,chars,gaz, labels],[words,biwords,chars,labels],...]\n",
    "            words: word ids for one sentence. (batch_size, sent_len) \n",
    "            chars: char ids for on sentences, various length. (batch_size, sent_len, each_word_length)\n",
    "        output:\n",
    "            zero padding for word and char, with their batch length\n",
    "            word_seq_tensor: (batch_size, max_sent_len) Variable\n",
    "            word_seq_lengths: (batch_size,1) Tensor\n",
    "            char_seq_tensor: (batch_size*max_sent_len, max_word_len) Variable\n",
    "            char_seq_lengths: (batch_size*max_sent_len,1) Tensor\n",
    "            char_seq_recover: (batch_size*max_sent_len,1)  recover char sequence order \n",
    "            label_seq_tensor: (batch_size, max_sent_len)\n",
    "            mask: (batch_size, max_sent_len) \n",
    "    \"\"\"\n",
    "    batch_size = len(input_batch_list)\n",
    "    words = [sent[0] for sent in input_batch_list]\n",
    "    biwords = [sent[1] for sent in input_batch_list]\n",
    "    chars = [sent[2] for sent in input_batch_list]\n",
    "\n",
    "\n",
    "    gazs = [sent[3] for sent in input_batch_list]\n",
    "    labels = [sent[4] for sent in input_batch_list]\n",
    "    word_seq_lengths = torch.LongTensor(list(map(len, words)))\n",
    "    max_seq_len = word_seq_lengths.max().item()\n",
    "\n",
    "    word_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile =  volatile_flag).long()\n",
    "    biword_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile =  volatile_flag).long()\n",
    "    label_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).long()\n",
    "    #mask = autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).byte()\n",
    "    mask = autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).bool()\n",
    "\n",
    "    for idx, (seq, biseq, label, seqlen) in enumerate(zip(words, biwords, labels, word_seq_lengths)):\n",
    "        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "        biword_seq_tensor[idx, :seqlen] = torch.LongTensor(biseq)\n",
    "        label_seq_tensor[idx, :seqlen] = torch.LongTensor(label)\n",
    "        mask[idx, :seqlen] = torch.Tensor([1]*seqlen.item())\n",
    "\n",
    "    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n",
    "    word_seq_tensor = word_seq_tensor[word_perm_idx]\n",
    "    biword_seq_tensor = biword_seq_tensor[word_perm_idx]\n",
    "    label_seq_tensor = label_seq_tensor[word_perm_idx]\n",
    "    mask = mask[word_perm_idx]\n",
    "\n",
    "    ### deal with char\n",
    "    # pad_chars (batch_size, max_seq_len)\n",
    "    pad_chars = [chars[idx] + [[0]] * (max_seq_len-len(chars[idx])) for idx in range(len(chars))]\n",
    "    length_list = [list(map(len, pad_char)) for pad_char in pad_chars]\n",
    "    #length_list = [len(pad_char) for pad_char in pad_chars]\n",
    "    max_word_len = max(map(max, length_list))\n",
    "    char_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len, max_word_len)), volatile =  volatile_flag).long()\n",
    "    char_seq_lengths = torch.LongTensor(length_list)\n",
    "    for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):\n",
    "        for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):\n",
    "            # print len(word), wordlen\n",
    "            char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)\n",
    "    char_seq_tensor = char_seq_tensor[word_perm_idx].view(batch_size*max_seq_len,-1)\n",
    "    char_seq_lengths = char_seq_lengths[word_perm_idx].view(batch_size*max_seq_len,)\n",
    "    char_seq_lengths, char_perm_idx = char_seq_lengths.sort(0, descending=True)\n",
    "    char_seq_tensor = char_seq_tensor[char_perm_idx]\n",
    "    _, char_seq_recover = char_perm_idx.sort(0, descending=False)\n",
    "    _, word_seq_recover = word_perm_idx.sort(0, descending=False)\n",
    "    \n",
    "    ## keep the gaz_list in orignial order\n",
    "    \n",
    "    gaz_list = [ gazs[i] for i in word_perm_idx]\n",
    "    gaz_list.append(volatile_flag)\n",
    "    if gpu:\n",
    "        word_seq_tensor = word_seq_tensor.cuda()\n",
    "        biword_seq_tensor = biword_seq_tensor.cuda()\n",
    "        word_seq_lengths = word_seq_lengths.cuda()\n",
    "        word_seq_recover = word_seq_recover.cuda()\n",
    "        label_seq_tensor = label_seq_tensor.cuda()\n",
    "        char_seq_tensor = char_seq_tensor.cuda()\n",
    "        char_seq_recover = char_seq_recover.cuda()\n",
    "        mask = mask.cuda()\n",
    "    return gaz_list, word_seq_tensor, biword_seq_tensor, word_seq_lengths, word_seq_recover, char_seq_tensor, char_seq_lengths, char_seq_recover, label_seq_tensor, mask\n",
    "\n",
    "\n",
    "def train(data, save_model_dir,save_data_set, seg=True):\n",
    "    dev_log=[]\n",
    "    #test_log=[]\n",
    "    train_log=[]\n",
    "    print (\"Training model...\")\n",
    "    data.show_data_summary()\n",
    "    #save_data_name = save_data_set\n",
    "    #save_data_setting(data, save_data_name)\n",
    "\n",
    "    save_data_name = save_data_set\n",
    "    save_data_setting(data, save_data_name)\n",
    "\n",
    "    model = SeqModel(data)\n",
    "    print (\"finished built model.\")\n",
    "    loss_function = nn.NLLLoss()\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.SGD(parameters, lr=data.HP_lr, momentum=data.HP_momentum)\n",
    "    best_dev = -1\n",
    "    #data.HP_iteration = 100#epoch次数\n",
    "    data.HP_iteration = 18#epoch次数\n",
    "    ## start training data.HP_iteration\n",
    "    for idx in range(data.HP_iteration):\n",
    "        epoch_start = time.time()\n",
    "        temp_start = epoch_start\n",
    "        print(\"Epoch: %s/%s\" %(idx,data.HP_iteration))\n",
    "        optimizer = lr_decay(optimizer, idx, data.HP_lr_decay, data.HP_lr)\n",
    "        instance_count = 0\n",
    "        sample_id = 0\n",
    "        sample_loss = 0\n",
    "        batch_loss = 0\n",
    "        total_loss = 0\n",
    "        right_token = 0\n",
    "        whole_token = 0\n",
    "        random.shuffle(data.train_Ids)\n",
    "        ## set model in train model\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        batch_size = 10 ## current only support batch size = 1 to compulate and accumulate to data.HP_batch_size update weights\n",
    "        train_num = len(data.train_Ids)\n",
    "        total_batch = train_num//batch_size+1\n",
    "        for batch_id in range(total_batch):\n",
    "            start = batch_id*batch_size\n",
    "            end = (batch_id+1)*batch_size \n",
    "            if end >train_num:\n",
    "                end = train_num\n",
    "            instance = data.train_Ids[start:end]\n",
    "            if not instance:\n",
    "                continue\n",
    "            gaz_list,  batch_word, batch_biword, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu)\n",
    "            # print \"gaz_list:\",gaz_list\n",
    "            # exit(0)\n",
    "            instance_count += 1\n",
    "            loss, tag_seq = model.neg_log_likelihood_loss(gaz_list, batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, batch_label, mask)\n",
    "            right, whole = predict_check(tag_seq, batch_label, mask)\n",
    "            right_token += right\n",
    "            whole_token += whole\n",
    "            #sample_loss += loss.data[0]\n",
    "            #total_loss += loss.data[0]\n",
    "            sample_loss += loss.data.item()\n",
    "            total_loss += loss.data.item()\n",
    "            batch_loss += loss\n",
    "\n",
    "            #if end%2000 == 0:\n",
    "            if end%1000 == 0:\n",
    "                temp_time = time.time()\n",
    "                temp_cost = temp_time - temp_start\n",
    "                temp_start = temp_time\n",
    "                print(\"Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f\"%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))\n",
    "                sys.stdout.flush()\n",
    "                sample_loss = 0\n",
    "            if end%data.HP_batch_size == 0:\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                batch_loss = 0\n",
    "        train_log.append(right_token)\n",
    "        temp_time = time.time()\n",
    "        temp_cost = temp_time - temp_start\n",
    "        print(\"     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f\"%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))       \n",
    "        epoch_finish = time.time()\n",
    "        epoch_cost = epoch_finish - epoch_start\n",
    "        print(\"Epoch: %s training finished. Time: %.2fs, speed: %.2fst/s,  total loss: %s\"%(idx, epoch_cost, train_num/epoch_cost, total_loss))\n",
    "        \n",
    "        # exit(0)\n",
    "        # continue\n",
    "        speed, acc, p, r, f, _, right_num = evaluate(data, model, \"dev\")\n",
    "        dev_log.append(right_num)\n",
    "        dev_finish = time.time()\n",
    "        dev_cost = dev_finish - epoch_finish\n",
    "\n",
    "        if seg:\n",
    "            current_score = f\n",
    "            print(\"Dev: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(dev_cost, speed, acc, p, r, f))\n",
    "        else:\n",
    "            current_score = acc\n",
    "            print(\"Dev: time: %.2fs speed: %.2fst/s; acc: %.4f\"%(dev_cost, speed, acc))\n",
    "        \n",
    "        if current_score > best_dev:\n",
    "            if seg:\n",
    "                print (\"Exceed previous best f score:\", best_dev)\n",
    "            else:\n",
    "                print (\"Exceed previous best acc score:\", best_dev)\n",
    "\n",
    "            model_name = save_model_dir\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            #model_name = save_model_dir +'.'+ str(idx) + \".model\"\n",
    "            #torch.save(model.state_dict(), model_name)\n",
    "            best_dev = current_score \n",
    "        # ## decode test\n",
    "        speed, acc, p, r, f, _, right_num = evaluate(data, model, \"test\")\n",
    "        #test_log.append(_)\n",
    "        test_finish = time.time()\n",
    "        test_cost = test_finish - dev_finish\n",
    "        if seg:\n",
    "            print(\"Test: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(test_cost, speed, acc, p, r, f))\n",
    "        else:\n",
    "            print(\"Test: time: %.2fs, speed: %.2fst/s; acc: %.4f\"%(test_cost, speed, acc))\n",
    "        \n",
    "        gc.collect()\n",
    "    return model,train_log,dev_log\n",
    "\n",
    "\n",
    "def load_model_decode(model_dir, data, name, gpu, seg=True):\n",
    "    data.HP_gpu = gpu\n",
    "    print (\"Load Model from file: \", model_dir)\n",
    "    model = SeqModel(data)\n",
    "    ## load model need consider if the model trained in GPU and load in CPU, or vice versa\n",
    "    # if not gpu:\n",
    "    #     model.load_state_dict(torch.load(model_dir), map_location=lambda storage, loc: storage)\n",
    "    #     # model = torch.load(model_dir, map_location=lambda storage, loc: storage)\n",
    "    # else:\n",
    "    model.load_state_dict(torch.load(model_dir))\n",
    "        # model = torch.load(model_dir)\n",
    "    #model = torch.load(model_dir)\n",
    "    \n",
    "    print(\"Decode %s data ...\"%(name))\n",
    "    start_time = time.time()\n",
    "    speed, acc, p, r, f, pred_results, right_num = evaluate(data, model, name)\n",
    "    end_time = time.time()\n",
    "    time_cost = end_time - start_time\n",
    "    if seg:\n",
    "        print(\"%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(name, time_cost, speed, acc, p, r, f))\n",
    "    else:\n",
    "        print(\"%s: time:%.2fs, speed:%.2fst/s; acc: %.4f\"%(name, time_cost, speed, acc))\n",
    "    return pred_results\n",
    "\n",
    "def load_model_decode_with_model(model, data, name, gpu, seg=True):\n",
    "    data.HP_gpu = gpu\n",
    "    \n",
    "    print(\"Decode %s data ...\"%(name))\n",
    "    start_time = time.time()\n",
    "    speed, acc, p, r, f, pred_results, right_num = evaluate(data, model, name)\n",
    "    end_time = time.time()\n",
    "    time_cost = end_time - start_time\n",
    "    if seg:\n",
    "        print(\"%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(name, time_cost, speed, acc, p, r, f))\n",
    "    else:\n",
    "        print(\"%s: time:%.2fs, speed:%.2fst/s; acc: %.4f\"%(name, time_cost, speed, acc))\n",
    "    return pred_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "df2061d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuDNN: True\n",
      "GPU available: True\n",
      "Status: train\n",
      "Seg:  True\n",
      "Train file: ../cyx＆xy/lattice/cyx/final/train.txt\n",
      "Dev file: ../cyx＆xy/lattice/cyx/final/dev.txt\n",
      "Test file: ../cyx＆xy/lattice/cyx/final/test.txt\n",
      "Raw file: None\n",
      "Char emb: ../cyx＆xy/lattice/ner/data/gigaword_chn.all.a2b.uni.ite50.vec\n",
      "Bichar emb: ../cyx＆xy/lattice/ner/data/gigaword_chn.all.a2b.bi.ite50.vec\n",
      "Gaz file: ../cyx＆xy/lattice/ner/data/ctb.50d.vec\n",
      "Model saved to: ../cyx＆xy/lattice/cyx/final/lattice-lstm_model_final417.model\n",
      "Load gaz file:  ../cyx＆xy/lattice/ner/data/ctb.50d.vec  total size: 704368\n",
      "gaz alphabet size: 14607\n",
      "gaz alphabet size: 16425\n",
      "gaz alphabet size: 19472\n",
      "build word pretrain emb...\n",
      "Embedding:\n",
      "     pretrain word:11327, prefect match:2949, case_match:0, oov:63, oov%:0.02090939263192831\n",
      "build biword pretrain emb...\n",
      "Embedding:\n",
      "     pretrain word:3986686, prefect match:71099, case_match:2, oov:21908, oov%:0.23554456510052682\n",
      "build gaz pretrain emb...\n",
      "Embedding:\n",
      "     pretrain word:704368, prefect match:19470, case_match:0, oov:1, oov%:5.135579293344289e-05\n",
      "Training model...\n",
      "DATA SUMMARY START:\n",
      "     Tag          scheme: BIO\n",
      "     MAX SENTENCE LENGTH: 600\n",
      "     MAX   WORD   LENGTH: -1\n",
      "     Number   normalized: True\n",
      "     Use          bigram: True\n",
      "     Word  alphabet size: 3013\n",
      "     Biword alphabet size: 93010\n",
      "     Char  alphabet size: 3013\n",
      "     Gaz   alphabet size: 19472\n",
      "     Label alphabet size: 8\n",
      "     Word embedding size: 50\n",
      "     Biword embedding size: 50\n",
      "     Char embedding size: 30\n",
      "     Gaz embedding size: 50\n",
      "     Norm     word   emb: True\n",
      "     Norm     biword emb: True\n",
      "     Norm     gaz    emb: False\n",
      "     Norm   gaz  dropout: 0.5\n",
      "     Train instance number: 23408\n",
      "     Dev   instance number: 7390\n",
      "     Test  instance number: 13326\n",
      "     Raw   instance number: 0\n",
      "     Hyperpara  iteration: 100\n",
      "     Hyperpara  batch size: 10\n",
      "     Hyperpara          lr: 0.015\n",
      "     Hyperpara    lr_decay: 0.05\n",
      "     Hyperpara     HP_clip: 5.0\n",
      "     Hyperpara    momentum: 0\n",
      "     Hyperpara  hidden_dim: 200\n",
      "     Hyperpara     dropout: 0.5\n",
      "     Hyperpara  lstm_layer: 1\n",
      "     Hyperpara      bilstm: True\n",
      "     Hyperpara         GPU: True\n",
      "     Hyperpara     use_gaz: True\n",
      "     Hyperpara fix gaz emb: False\n",
      "     Hyperpara    use_char: True\n",
      "             Char_features: LSTM\n",
      "DATA SUMMARY END.\n",
      "Data setting saved to file:  ../cyx＆xy/lattice/cyx/final/model-train-data_final417.dset\n",
      "build batched lstmcrf...\n",
      "build batched bilstm...\n",
      "build batched char bilstm...\n",
      "build LatticeLSTM...  forward , Fix emb: False  gaz drop: 0.5\n",
      "load pretrain word emb... (19472, 50)\n",
      "build LatticeLSTM...  backward , Fix emb: False  gaz drop: 0.5\n",
      "load pretrain word emb... (19472, 50)\n",
      "build batched crf...\n",
      "finished built model.\n",
      "Epoch: 0/18\n",
      " Learning rate is setted as: 0.015\n",
      "Instance: 1000; Time: 35.66s; loss: 23464.8848; acc: 11155/17966=0.6209\n",
      "Instance: 2000; Time: 38.57s; loss: 5660.0028; acc: 26791/37070=0.7227\n",
      "Instance: 3000; Time: 38.60s; loss: 3744.9554; acc: 42849/55901=0.7665\n",
      "Instance: 4000; Time: 39.04s; loss: 3234.5508; acc: 59256/74796=0.7922\n",
      "Instance: 5000; Time: 40.15s; loss: 3065.9835; acc: 76520/94411=0.8105\n",
      "Instance: 6000; Time: 41.47s; loss: 2351.8860; acc: 94487/114239=0.8271\n",
      "Instance: 7000; Time: 41.22s; loss: 2048.8559; acc: 112621/134018=0.8403\n",
      "Instance: 8000; Time: 42.45s; loss: 2169.2362; acc: 131556/154648=0.8507\n",
      "Instance: 9000; Time: 38.22s; loss: 2068.7924; acc: 148553/173116=0.8581\n",
      "Instance: 10000; Time: 40.27s; loss: 1918.6024; acc: 166665/192643=0.8651\n",
      "Instance: 11000; Time: 42.54s; loss: 2006.8583; acc: 185691/213359=0.8703\n",
      "Instance: 12000; Time: 38.55s; loss: 1673.5949; acc: 203097/232061=0.8752\n",
      "Instance: 13000; Time: 42.13s; loss: 1938.1350; acc: 221723/252350=0.8786\n",
      "Instance: 14000; Time: 39.28s; loss: 1484.8215; acc: 239772/271601=0.8828\n",
      "Instance: 15000; Time: 37.46s; loss: 1473.8883; acc: 257935/290890=0.8867\n",
      "Instance: 16000; Time: 40.98s; loss: 1763.0720; acc: 277218/311531=0.8899\n",
      "Instance: 17000; Time: 45.28s; loss: 1889.7087; acc: 297608/333349=0.8928\n",
      "Instance: 18000; Time: 43.88s; loss: 1454.8961; acc: 317619/354589=0.8957\n",
      "Instance: 19000; Time: 41.17s; loss: 1402.0959; acc: 336360/374447=0.8983\n",
      "Instance: 20000; Time: 43.90s; loss: 4888.4418; acc: 355093/395770=0.8972\n",
      "Instance: 21000; Time: 40.83s; loss: 2557.9154; acc: 373497/415869=0.8981\n",
      "Instance: 22000; Time: 37.74s; loss: 1755.7705; acc: 390958/434720=0.8993\n",
      "Instance: 23000; Time: 39.70s; loss: 1750.4084; acc: 408713/453839=0.9006\n",
      "     Instance: 23408; Time: 15.46s; loss: 548.9085; acc: 415752/461291=0.9013\n",
      "Epoch: 0 training finished. Time: 944.55s, speed: 24.78st/s,  total loss: 76316.26570129395\n",
      "gold_num =  13630  pred_num =  13410  right_num =  11043\n",
      "Dev: time: 127.41s, speed: 58.05st/s; acc: 0.9533, p: 0.8235, r: 0.8102, f: 0.8168\n",
      "Exceed previous best f score: -1\n",
      "gold_num =  24150  pred_num =  23525  right_num =  19450\n",
      "Test: time: 224.10s, speed: 59.53st/s; acc: 0.9523, p: 0.8268, r: 0.8054, f: 0.8159\n",
      "Epoch: 1/18\n",
      " Learning rate is setted as: 0.014249999999999999\n",
      "Instance: 1000; Time: 37.71s; loss: 1259.4826; acc: 17495/18524=0.9445\n",
      "Instance: 2000; Time: 41.24s; loss: 1182.2105; acc: 36597/38601=0.9481\n",
      "Instance: 3000; Time: 39.97s; loss: 1252.0941; acc: 55413/58431=0.9483\n",
      "Instance: 4000; Time: 39.77s; loss: 1101.1308; acc: 74311/78234=0.9499\n",
      "Instance: 5000; Time: 38.39s; loss: 1106.1505; acc: 91849/96633=0.9505\n",
      "Instance: 6000; Time: 41.29s; loss: 1438.7953; acc: 110820/116709=0.9495\n",
      "Instance: 7000; Time: 40.01s; loss: 1250.2548; acc: 129282/136189=0.9493\n",
      "Instance: 8000; Time: 42.70s; loss: 1364.2806; acc: 148861/156848=0.9491\n",
      "Instance: 9000; Time: 41.85s; loss: 1353.8946; acc: 168135/177152=0.9491\n",
      "Instance: 10000; Time: 37.22s; loss: 1062.5415; acc: 185480/195392=0.9493\n",
      "Instance: 11000; Time: 37.48s; loss: 2124.5157; acc: 202490/213849=0.9469\n",
      "Instance: 12000; Time: 38.39s; loss: 1205.2744; acc: 220329/232579=0.9473\n",
      "Instance: 13000; Time: 41.53s; loss: 1357.1272; acc: 239615/252996=0.9471\n",
      "Instance: 14000; Time: 39.06s; loss: 1221.6120; acc: 258351/272657=0.9475\n",
      "Instance: 15000; Time: 40.59s; loss: 1196.0775; acc: 277717/292892=0.9482\n",
      "Instance: 16000; Time: 40.84s; loss: 1297.5932; acc: 297218/313356=0.9485\n",
      "Instance: 17000; Time: 36.67s; loss: 1061.6896; acc: 314738/331722=0.9488\n",
      "Instance: 18000; Time: 41.12s; loss: 2356.8930; acc: 333585/351986=0.9477\n",
      "Instance: 19000; Time: 42.20s; loss: 1520.5285; acc: 353494/373057=0.9476\n",
      "Instance: 20000; Time: 43.13s; loss: 1236.8052; acc: 373647/394071=0.9482\n",
      "Instance: 21000; Time: 40.18s; loss: 1118.7347; acc: 392855/414089=0.9487\n",
      "Instance: 22000; Time: 41.84s; loss: 1197.3879; acc: 412537/434617=0.9492\n",
      "Instance: 23000; Time: 38.41s; loss: 1017.3092; acc: 430837/453672=0.9497\n",
      "     Instance: 23408; Time: 15.34s; loss: 398.9218; acc: 438145/461291=0.9498\n",
      "Epoch: 1 training finished. Time: 936.95s, speed: 24.98st/s,  total loss: 30681.305236816406\n",
      "gold_num =  13630  pred_num =  13191  right_num =  11580\n",
      "Dev: time: 124.67s, speed: 59.33st/s; acc: 0.9654, p: 0.8779, r: 0.8496, f: 0.8635\n",
      "Exceed previous best f score: 0.8167899408284022\n",
      "gold_num =  24150  pred_num =  23278  right_num =  19956\n",
      "Test: time: 216.85s, speed: 61.52st/s; acc: 0.9593, p: 0.8573, r: 0.8263, f: 0.8415\n",
      "Epoch: 2/18\n",
      " Learning rate is setted as: 0.0135375\n",
      "Instance: 1000; Time: 39.51s; loss: 964.7823; acc: 19285/20051=0.9618\n",
      "Instance: 2000; Time: 41.99s; loss: 817.5827; acc: 39242/40637=0.9657\n",
      "Instance: 3000; Time: 37.58s; loss: 993.4033; acc: 57583/59745=0.9638\n",
      "Instance: 4000; Time: 41.86s; loss: 880.8025; acc: 77590/80430=0.9647\n",
      "Instance: 5000; Time: 38.89s; loss: 851.6404; acc: 96310/99839=0.9647\n",
      "Instance: 6000; Time: 38.41s; loss: 951.8991; acc: 114900/119123=0.9645\n",
      "Instance: 7000; Time: 38.82s; loss: 901.7935; acc: 133678/138547=0.9649\n",
      "Instance: 8000; Time: 39.18s; loss: 830.5261; acc: 152531/158042=0.9651\n",
      "Instance: 9000; Time: 37.42s; loss: 871.3558; acc: 170599/176825=0.9648\n",
      "Instance: 10000; Time: 38.88s; loss: 835.8098; acc: 189670/196540=0.9650\n",
      "Instance: 11000; Time: 39.04s; loss: 855.2142; acc: 208837/216345=0.9653\n",
      "Instance: 12000; Time: 37.77s; loss: 868.9222; acc: 227227/235421=0.9652\n",
      "Instance: 13000; Time: 40.10s; loss: 781.3892; acc: 246618/255384=0.9657\n",
      "Instance: 14000; Time: 38.69s; loss: 792.6589; acc: 265176/274546=0.9659\n",
      "Instance: 15000; Time: 41.53s; loss: 930.2795; acc: 284999/295045=0.9660\n",
      "Instance: 16000; Time: 40.95s; loss: 807.3016; acc: 304934/315566=0.9663\n",
      "Instance: 17000; Time: 42.17s; loss: 1018.5868; acc: 324919/336322=0.9661\n",
      "Instance: 18000; Time: 36.51s; loss: 671.5189; acc: 342896/354846=0.9663\n",
      "Instance: 19000; Time: 35.92s; loss: 834.1418; acc: 360547/373097=0.9664\n",
      "Instance: 20000; Time: 38.43s; loss: 707.7147; acc: 379345/392454=0.9666\n",
      "Instance: 21000; Time: 37.89s; loss: 894.8198; acc: 397821/411628=0.9665\n",
      "Instance: 22000; Time: 41.54s; loss: 805.6811; acc: 417785/432196=0.9667\n",
      "Instance: 23000; Time: 41.69s; loss: 905.6680; acc: 437806/452915=0.9666\n",
      "     Instance: 23408; Time: 16.88s; loss: 377.1776; acc: 445881/461291=0.9666\n",
      "Epoch: 2 training finished. Time: 921.64s, speed: 25.40st/s,  total loss: 20150.669799804688\n",
      "gold_num =  13630  pred_num =  13188  right_num =  11971\n",
      "Dev: time: 124.59s, speed: 59.37st/s; acc: 0.9726, p: 0.9077, r: 0.8783, f: 0.8928\n",
      "Exceed previous best f score: 0.8635024794004699\n",
      "gold_num =  24150  pred_num =  23440  right_num =  20507\n",
      "Test: time: 216.57s, speed: 61.60st/s; acc: 0.9668, p: 0.8749, r: 0.8492, f: 0.8618\n",
      "Epoch: 3/18\n",
      " Learning rate is setted as: 0.012860624999999997\n",
      "Instance: 1000; Time: 37.12s; loss: 583.6309; acc: 18397/18882=0.9743\n",
      "Instance: 2000; Time: 36.13s; loss: 754.0306; acc: 36263/37336=0.9713\n",
      "Instance: 3000; Time: 37.83s; loss: 673.2646; acc: 55246/56851=0.9718\n",
      "Instance: 4000; Time: 42.25s; loss: 684.8184; acc: 75442/77604=0.9721\n",
      "Instance: 5000; Time: 41.11s; loss: 659.0667; acc: 95160/97829=0.9727\n",
      "Instance: 6000; Time: 41.26s; loss: 710.7737; acc: 114880/118137=0.9724\n",
      "Instance: 7000; Time: 40.99s; loss: 691.4247; acc: 134697/138480=0.9727\n",
      "Instance: 8000; Time: 39.79s; loss: 646.4395; acc: 154326/158574=0.9732\n",
      "Instance: 9000; Time: 37.62s; loss: 752.8334; acc: 173269/178034=0.9732\n",
      "Instance: 10000; Time: 38.22s; loss: 669.6995; acc: 191900/197201=0.9731\n",
      "Instance: 11000; Time: 39.21s; loss: 697.7754; acc: 211067/216875=0.9732\n",
      "Instance: 12000; Time: 40.05s; loss: 685.5751; acc: 230437/236730=0.9734\n",
      "Instance: 13000; Time: 37.94s; loss: 744.3866; acc: 248827/255698=0.9731\n",
      "Instance: 14000; Time: 42.73s; loss: 733.1800; acc: 269180/276670=0.9729\n",
      "Instance: 15000; Time: 38.04s; loss: 559.1060; acc: 287755/295647=0.9733\n",
      "Instance: 16000; Time: 41.05s; loss: 726.5753; acc: 307784/316274=0.9732\n",
      "Instance: 17000; Time: 37.98s; loss: 671.2151; acc: 326544/335496=0.9733\n",
      "Instance: 18000; Time: 41.45s; loss: 763.0140; acc: 346558/356121=0.9731\n",
      "Instance: 19000; Time: 41.72s; loss: 661.0620; acc: 366248/376293=0.9733\n",
      "Instance: 20000; Time: 38.36s; loss: 648.1631; acc: 384954/395505=0.9733\n",
      "Instance: 21000; Time: 37.82s; loss: 652.3312; acc: 403489/414550=0.9733\n",
      "Instance: 22000; Time: 40.17s; loss: 651.6819; acc: 422566/434112=0.9734\n",
      "Instance: 23000; Time: 45.16s; loss: 563.9754; acc: 441134/453092=0.9736\n",
      "     Instance: 23408; Time: 27.29s; loss: 264.1313; acc: 449112/461291=0.9736\n",
      "Epoch: 3 training finished. Time: 941.30s, speed: 24.87st/s,  total loss: 15848.154327392578\n",
      "gold_num =  13630  pred_num =  13775  right_num =  12443\n",
      "Dev: time: 165.27s, speed: 44.74st/s; acc: 0.9741, p: 0.9033, r: 0.9129, f: 0.9081\n",
      "Exceed previous best f score: 0.8927585949735252\n",
      "gold_num =  24150  pred_num =  24534  right_num =  21058\n",
      "Test: time: 217.55s, speed: 61.32st/s; acc: 0.9673, p: 0.8583, r: 0.8720, f: 0.8651\n",
      "Epoch: 4/18\n",
      " Learning rate is setted as: 0.012217593749999998\n",
      "Instance: 1000; Time: 39.55s; loss: 541.6091; acc: 19276/19692=0.9789\n",
      "Instance: 2000; Time: 41.41s; loss: 555.3102; acc: 39590/40464=0.9784\n",
      "Instance: 3000; Time: 38.68s; loss: 656.2211; acc: 58580/59934=0.9774\n",
      "Instance: 4000; Time: 43.18s; loss: 764.4971; acc: 79573/81528=0.9760\n",
      "Instance: 5000; Time: 34.78s; loss: 531.6354; acc: 96957/99322=0.9762\n",
      "Instance: 6000; Time: 41.25s; loss: 783.9219; acc: 116852/119795=0.9754\n",
      "Instance: 7000; Time: 40.95s; loss: 534.8630; acc: 136680/140031=0.9761\n",
      "Instance: 8000; Time: 39.53s; loss: 600.9576; acc: 155730/159587=0.9758\n",
      "Instance: 9000; Time: 38.72s; loss: 544.6056; acc: 175134/179415=0.9761\n",
      "Instance: 10000; Time: 37.46s; loss: 604.7200; acc: 193353/198075=0.9762\n",
      "Instance: 11000; Time: 38.17s; loss: 520.5651; acc: 211924/217083=0.9762\n",
      "Instance: 12000; Time: 40.50s; loss: 655.3801; acc: 231666/237311=0.9762\n",
      "Instance: 13000; Time: 36.27s; loss: 553.6780; acc: 249535/255619=0.9762\n",
      "Instance: 14000; Time: 39.98s; loss: 564.1447; acc: 268944/275473=0.9763\n",
      "Instance: 15000; Time: 38.21s; loss: 710.1641; acc: 287455/294459=0.9762\n",
      "Instance: 16000; Time: 41.26s; loss: 539.7300; acc: 307206/314669=0.9763\n",
      "Instance: 17000; Time: 38.01s; loss: 580.6509; acc: 325891/333792=0.9763\n",
      "Instance: 18000; Time: 40.06s; loss: 527.5725; acc: 345454/353803=0.9764\n",
      "Instance: 19000; Time: 41.66s; loss: 645.4041; acc: 365449/374354=0.9762\n",
      "Instance: 20000; Time: 36.98s; loss: 522.8488; acc: 383546/392811=0.9764\n",
      "Instance: 21000; Time: 39.81s; loss: 584.9972; acc: 402940/412626=0.9765\n",
      "Instance: 22000; Time: 44.13s; loss: 564.7034; acc: 424245/434375=0.9767\n",
      "Instance: 23000; Time: 37.52s; loss: 573.9872; acc: 442439/452977=0.9767\n",
      "     Instance: 23408; Time: 17.14s; loss: 202.3519; acc: 450586/461291=0.9768\n",
      "Epoch: 4 training finished. Time: 925.22s, speed: 25.30st/s,  total loss: 13864.519287109375\n",
      "gold_num =  13630  pred_num =  13463  right_num =  12466\n",
      "Dev: time: 122.54s, speed: 60.36st/s; acc: 0.9786, p: 0.9259, r: 0.9146, f: 0.9202\n",
      "Exceed previous best f score: 0.9080824667031564\n",
      "gold_num =  24150  pred_num =  23924  right_num =  20847\n",
      "Test: time: 214.26s, speed: 62.27st/s; acc: 0.9690, p: 0.8714, r: 0.8632, f: 0.8673\n",
      "Epoch: 5/18\n",
      " Learning rate is setted as: 0.011606714062499995\n",
      "Instance: 1000; Time: 38.76s; loss: 574.3839; acc: 19111/19541=0.9780\n",
      "Instance: 2000; Time: 38.41s; loss: 573.7314; acc: 38033/38913=0.9774\n",
      "Instance: 3000; Time: 39.68s; loss: 562.9095; acc: 57270/58579=0.9777\n",
      "Instance: 4000; Time: 36.78s; loss: 493.1319; acc: 75596/77311=0.9778\n",
      "Instance: 5000; Time: 37.95s; loss: 476.1725; acc: 94620/96684=0.9787\n",
      "Instance: 6000; Time: 45.37s; loss: 818.9454; acc: 116158/118885=0.9771\n",
      "Instance: 7000; Time: 37.45s; loss: 639.3013; acc: 134265/137483=0.9766\n",
      "Instance: 8000; Time: 37.87s; loss: 542.2962; acc: 152720/156392=0.9765\n",
      "Instance: 9000; Time: 42.48s; loss: 682.2765; acc: 173243/177414=0.9765\n",
      "Instance: 10000; Time: 39.59s; loss: 580.5112; acc: 192416/197018=0.9766\n",
      "Instance: 11000; Time: 39.46s; loss: 576.6286; acc: 211482/216546=0.9766\n",
      "Instance: 12000; Time: 39.67s; loss: 448.1620; acc: 230845/236266=0.9771\n",
      "Instance: 13000; Time: 39.17s; loss: 563.5513; acc: 250112/255993=0.9770\n",
      "Instance: 14000; Time: 40.25s; loss: 531.8662; acc: 269688/275975=0.9772\n",
      "Instance: 15000; Time: 42.33s; loss: 554.6266; acc: 290230/296931=0.9774\n",
      "Instance: 16000; Time: 38.36s; loss: 482.2591; acc: 309193/316353=0.9774\n",
      "Instance: 17000; Time: 35.67s; loss: 463.6451; acc: 326996/334512=0.9775\n",
      "Instance: 18000; Time: 43.70s; loss: 610.2499; acc: 348068/356043=0.9776\n",
      "Instance: 19000; Time: 38.68s; loss: 578.9105; acc: 367059/375452=0.9776\n",
      "Instance: 20000; Time: 39.25s; loss: 471.9341; acc: 385662/394437=0.9778\n",
      "Instance: 21000; Time: 36.58s; loss: 585.2499; acc: 403546/412782=0.9776\n",
      "Instance: 22000; Time: 41.98s; loss: 572.0912; acc: 424052/433705=0.9777\n",
      "Instance: 23000; Time: 39.69s; loss: 505.1032; acc: 443087/453134=0.9778\n",
      "     Instance: 23408; Time: 16.36s; loss: 157.9880; acc: 451125/461291=0.9780\n",
      "Epoch: 5 training finished. Time: 925.51s, speed: 25.29st/s,  total loss: 13045.925476074219\n",
      "gold_num =  13630  pred_num =  13565  right_num =  12567\n",
      "Dev: time: 126.29s, speed: 58.57st/s; acc: 0.9784, p: 0.9264, r: 0.9220, f: 0.9242\n",
      "Exceed previous best f score: 0.9202376997748496\n",
      "gold_num =  24150  pred_num =  24186  right_num =  20930\n",
      "Test: time: 218.26s, speed: 61.12st/s; acc: 0.9673, p: 0.8654, r: 0.8667, f: 0.8660\n",
      "Epoch: 6/18\n",
      " Learning rate is setted as: 0.011026378359374997\n",
      "Instance: 1000; Time: 41.68s; loss: 425.6110; acc: 20322/20656=0.9838\n",
      "Instance: 2000; Time: 39.63s; loss: 381.1935; acc: 40046/40646=0.9852\n",
      "Instance: 3000; Time: 41.49s; loss: 497.3341; acc: 60234/61220=0.9839\n",
      "Instance: 4000; Time: 38.98s; loss: 448.8683; acc: 79477/80820=0.9834\n",
      "Instance: 5000; Time: 39.33s; loss: 926.7291; acc: 98287/100274=0.9802\n",
      "Instance: 6000; Time: 38.10s; loss: 677.8249; acc: 116644/119167=0.9788\n",
      "Instance: 7000; Time: 34.88s; loss: 619.7935; acc: 133619/136657=0.9778\n",
      "Instance: 8000; Time: 42.07s; loss: 516.0688; acc: 153961/157401=0.9781\n",
      "Instance: 9000; Time: 38.94s; loss: 566.9788; acc: 172836/176724=0.9780\n",
      "Instance: 10000; Time: 39.44s; loss: 533.0615; acc: 192406/196682=0.9783\n",
      "Instance: 11000; Time: 40.77s; loss: 658.3716; acc: 212086/216798=0.9783\n",
      "Instance: 12000; Time: 38.50s; loss: 484.9984; acc: 231063/236166=0.9784\n",
      "Instance: 13000; Time: 38.76s; loss: 480.5577; acc: 249966/255418=0.9787\n",
      "Instance: 14000; Time: 39.80s; loss: 460.2681; acc: 269668/275466=0.9790\n",
      "Instance: 15000; Time: 43.00s; loss: 552.6115; acc: 290401/296609=0.9791\n",
      "Instance: 16000; Time: 41.10s; loss: 555.4240; acc: 310332/316943=0.9791\n",
      "Instance: 17000; Time: 36.30s; loss: 454.8428; acc: 328366/335335=0.9792\n",
      "Instance: 18000; Time: 37.26s; loss: 427.6299; acc: 346760/354043=0.9794\n",
      "Instance: 19000; Time: 40.19s; loss: 630.0986; acc: 366216/373958=0.9793\n",
      "Instance: 20000; Time: 41.85s; loss: 542.4515; acc: 386543/394665=0.9794\n",
      "Instance: 21000; Time: 39.07s; loss: 569.0795; acc: 405518/414068=0.9794\n",
      "Instance: 22000; Time: 39.72s; loss: 534.4827; acc: 424809/433811=0.9792\n",
      "Instance: 23000; Time: 39.43s; loss: 451.4727; acc: 444278/453580=0.9795\n",
      "     Instance: 23408; Time: 15.22s; loss: 194.4586; acc: 451811/461291=0.9794\n",
      "Epoch: 6 training finished. Time: 925.52s, speed: 25.29st/s,  total loss: 12590.211151123047\n",
      "gold_num =  13630  pred_num =  13598  right_num =  12633\n",
      "Dev: time: 122.73s, speed: 60.27st/s; acc: 0.9791, p: 0.9290, r: 0.9269, f: 0.9279\n",
      "Exceed previous best f score: 0.9242140099282956\n",
      "gold_num =  24150  pred_num =  24459  right_num =  20963\n",
      "Test: time: 217.44s, speed: 61.36st/s; acc: 0.9680, p: 0.8571, r: 0.8680, f: 0.8625\n",
      "Epoch: 7/18\n",
      " Learning rate is setted as: 0.010475059441406245\n",
      "Instance: 1000; Time: 37.74s; loss: 365.9148; acc: 18947/19204=0.9866\n",
      "Instance: 2000; Time: 39.22s; loss: 511.3763; acc: 38324/38956=0.9838\n",
      "Instance: 3000; Time: 43.55s; loss: 461.9849; acc: 59950/60903=0.9844\n",
      "Instance: 4000; Time: 37.31s; loss: 505.5392; acc: 78686/79984=0.9838\n",
      "Instance: 5000; Time: 38.49s; loss: 404.0081; acc: 97879/99489=0.9838\n",
      "Instance: 6000; Time: 38.59s; loss: 427.1241; acc: 116874/118782=0.9839\n",
      "Instance: 7000; Time: 36.20s; loss: 412.6562; acc: 134909/137124=0.9838\n",
      "Instance: 8000; Time: 41.12s; loss: 401.6810; acc: 154767/157311=0.9838\n",
      "Instance: 9000; Time: 36.41s; loss: 350.2450; acc: 172882/175675=0.9841\n",
      "Instance: 10000; Time: 40.42s; loss: 340.7339; acc: 192651/195695=0.9844\n",
      "Instance: 11000; Time: 38.47s; loss: 403.8188; acc: 211560/214924=0.9843\n",
      "Instance: 12000; Time: 39.46s; loss: 462.7618; acc: 230922/234665=0.9840\n",
      "Instance: 13000; Time: 43.37s; loss: 438.4368; acc: 251601/255663=0.9841\n",
      "Instance: 14000; Time: 37.74s; loss: 382.4713; acc: 270398/274751=0.9842\n",
      "Instance: 15000; Time: 37.18s; loss: 395.2335; acc: 288616/293267=0.9841\n",
      "Instance: 16000; Time: 41.29s; loss: 347.1685; acc: 308818/313736=0.9843\n",
      "Instance: 17000; Time: 40.88s; loss: 462.1182; acc: 328670/333963=0.9842\n",
      "Instance: 18000; Time: 37.72s; loss: 454.9626; acc: 347504/353066=0.9842\n",
      "Instance: 19000; Time: 38.09s; loss: 347.1156; acc: 366171/372010=0.9843\n",
      "Instance: 20000; Time: 40.35s; loss: 525.1641; acc: 385713/391972=0.9840\n",
      "Instance: 21000; Time: 41.01s; loss: 448.6245; acc: 405645/412277=0.9839\n",
      "Instance: 22000; Time: 42.62s; loss: 469.7498; acc: 426459/433448=0.9839\n",
      "Instance: 23000; Time: 37.80s; loss: 432.3769; acc: 445014/452324=0.9838\n",
      "     Instance: 23408; Time: 18.17s; loss: 189.5453; acc: 453846/461291=0.9839\n",
      "Epoch: 7 training finished. Time: 923.21s, speed: 25.36st/s,  total loss: 9940.811340332031\n",
      "gold_num =  13630  pred_num =  13296  right_num =  12569\n",
      "Dev: time: 123.17s, speed: 60.05st/s; acc: 0.9797, p: 0.9453, r: 0.9222, f: 0.9336\n",
      "Exceed previous best f score: 0.9279418245923314\n",
      "gold_num =  24150  pred_num =  23936  right_num =  20727\n",
      "Test: time: 215.78s, speed: 61.83st/s; acc: 0.9666, p: 0.8659, r: 0.8583, f: 0.8621\n",
      "Epoch: 8/18\n",
      " Learning rate is setted as: 0.009951306469335933\n",
      "Instance: 1000; Time: 36.96s; loss: 479.5065; acc: 18507/18856=0.9815\n",
      "Instance: 2000; Time: 38.47s; loss: 439.0051; acc: 37386/38110=0.9810\n",
      "Instance: 3000; Time: 40.48s; loss: 362.2741; acc: 57459/58443=0.9832\n",
      "Instance: 4000; Time: 41.84s; loss: 427.9583; acc: 77560/78835=0.9838\n",
      "Instance: 5000; Time: 41.12s; loss: 343.0782; acc: 97894/99471=0.9841\n",
      "Instance: 6000; Time: 39.01s; loss: 415.3548; acc: 116767/118662=0.9840\n",
      "Instance: 7000; Time: 38.34s; loss: 397.6345; acc: 135483/137636=0.9844\n",
      "Instance: 8000; Time: 42.02s; loss: 429.0456; acc: 155683/158132=0.9845\n",
      "Instance: 9000; Time: 37.55s; loss: 336.7123; acc: 174527/177221=0.9848\n",
      "Instance: 10000; Time: 38.61s; loss: 367.4309; acc: 193498/196472=0.9849\n",
      "Instance: 11000; Time: 39.27s; loss: 396.8563; acc: 212926/216152=0.9851\n",
      "Instance: 12000; Time: 38.41s; loss: 400.5120; acc: 231998/235510=0.9851\n",
      "Instance: 13000; Time: 39.47s; loss: 360.2847; acc: 251525/255289=0.9853\n",
      "Instance: 14000; Time: 42.94s; loss: 560.9956; acc: 272437/276568=0.9851\n",
      "Instance: 15000; Time: 38.54s; loss: 264.4573; acc: 291517/295832=0.9854\n",
      "Instance: 16000; Time: 39.17s; loss: 322.0415; acc: 310910/315482=0.9855\n",
      "Instance: 17000; Time: 36.05s; loss: 283.7431; acc: 328831/333624=0.9856\n",
      "Instance: 18000; Time: 38.12s; loss: 364.4051; acc: 347950/353034=0.9856\n",
      "Instance: 19000; Time: 42.00s; loss: 440.0372; acc: 368470/373904=0.9855\n",
      "Instance: 20000; Time: 37.59s; loss: 356.9929; acc: 387099/392803=0.9855\n",
      "Instance: 21000; Time: 38.42s; loss: 368.7145; acc: 406202/412219=0.9854\n",
      "Instance: 22000; Time: 42.07s; loss: 562.0031; acc: 426625/433053=0.9852\n",
      "Instance: 23000; Time: 40.82s; loss: 406.7634; acc: 446801/453550=0.9851\n",
      "     Instance: 23408; Time: 15.59s; loss: 165.9277; acc: 454428/461291=0.9851\n",
      "Epoch: 8 training finished. Time: 922.85s, speed: 25.37st/s,  total loss: 9251.734832763672\n",
      "gold_num =  13630  pred_num =  13542  right_num =  12698\n",
      "Dev: time: 123.61s, speed: 59.83st/s; acc: 0.9810, p: 0.9377, r: 0.9316, f: 0.9346\n",
      "Exceed previous best f score: 0.9335957810294883\n",
      "gold_num =  24150  pred_num =  24283  right_num =  21019\n",
      "Test: time: 217.27s, speed: 61.40st/s; acc: 0.9691, p: 0.8656, r: 0.8704, f: 0.8680\n",
      "Epoch: 9/18\n",
      " Learning rate is setted as: 0.009453741145869136\n",
      "Instance: 1000; Time: 37.69s; loss: 441.6796; acc: 18576/18928=0.9814\n",
      "Instance: 2000; Time: 38.20s; loss: 314.1413; acc: 37464/38046=0.9847\n",
      "Instance: 3000; Time: 40.93s; loss: 387.9765; acc: 57416/58265=0.9854\n",
      "Instance: 4000; Time: 40.73s; loss: 407.3470; acc: 77122/78262=0.9854\n",
      "Instance: 5000; Time: 40.12s; loss: 326.4370; acc: 96812/98219=0.9857\n",
      "Instance: 6000; Time: 40.32s; loss: 256.2495; acc: 116669/118273=0.9864\n",
      "Instance: 7000; Time: 38.43s; loss: 385.6787; acc: 135727/137565=0.9866\n",
      "Instance: 8000; Time: 40.55s; loss: 354.2689; acc: 155576/157662=0.9868\n",
      "Instance: 9000; Time: 38.11s; loss: 305.9565; acc: 174837/177148=0.9870\n",
      "Instance: 10000; Time: 42.92s; loss: 403.5942; acc: 195574/198167=0.9869\n",
      "Instance: 11000; Time: 39.56s; loss: 351.2306; acc: 215104/217964=0.9869\n",
      "Instance: 12000; Time: 38.40s; loss: 313.9086; acc: 234180/237310=0.9868\n",
      "Instance: 13000; Time: 39.00s; loss: 357.3325; acc: 252982/256400=0.9867\n",
      "Instance: 14000; Time: 37.21s; loss: 341.0691; acc: 271621/275342=0.9865\n",
      "Instance: 15000; Time: 38.64s; loss: 309.4883; acc: 290637/294581=0.9866\n",
      "Instance: 16000; Time: 39.11s; loss: 378.2773; acc: 310299/314497=0.9867\n",
      "Instance: 17000; Time: 43.91s; loss: 320.0768; acc: 331445/335880=0.9868\n",
      "Instance: 18000; Time: 38.13s; loss: 321.7791; acc: 350633/355345=0.9867\n",
      "Instance: 19000; Time: 40.08s; loss: 315.6252; acc: 369964/374901=0.9868\n",
      "Instance: 20000; Time: 40.21s; loss: 319.3544; acc: 389674/394843=0.9869\n",
      "Instance: 21000; Time: 37.57s; loss: 290.6417; acc: 408233/413628=0.9870\n",
      "Instance: 22000; Time: 37.15s; loss: 325.8658; acc: 426837/432508=0.9869\n",
      "Instance: 23000; Time: 42.19s; loss: 436.7339; acc: 447677/453648=0.9868\n",
      "     Instance: 23408; Time: 15.24s; loss: 157.1190; acc: 455225/461291=0.9868\n",
      "Epoch: 9 training finished. Time: 924.41s, speed: 25.32st/s,  total loss: 8121.831390380859\n",
      "gold_num =  13630  pred_num =  13542  right_num =  12706\n",
      "Dev: time: 123.79s, speed: 59.75st/s; acc: 0.9813, p: 0.9383, r: 0.9322, f: 0.9352\n",
      "Exceed previous best f score: 0.9346385985573383\n",
      "gold_num =  24150  pred_num =  24213  right_num =  20996\n",
      "Test: time: 216.58s, speed: 61.60st/s; acc: 0.9692, p: 0.8671, r: 0.8694, f: 0.8683\n",
      "Epoch: 10/18\n",
      " Learning rate is setted as: 0.00898105408857568\n",
      "Instance: 1000; Time: 38.25s; loss: 286.9780; acc: 19420/19623=0.9897\n",
      "Instance: 2000; Time: 39.11s; loss: 335.8839; acc: 38541/38949=0.9895\n",
      "Instance: 3000; Time: 37.09s; loss: 260.1705; acc: 56891/57483=0.9897\n",
      "Instance: 4000; Time: 39.87s; loss: 380.9027; acc: 76815/77669=0.9890\n",
      "Instance: 5000; Time: 41.70s; loss: 345.7024; acc: 97396/98468=0.9891\n",
      "Instance: 6000; Time: 41.51s; loss: 311.7789; acc: 117580/118899=0.9889\n",
      "Instance: 7000; Time: 36.51s; loss: 443.7390; acc: 135688/137313=0.9882\n",
      "Instance: 8000; Time: 41.27s; loss: 319.7560; acc: 156079/157996=0.9879\n",
      "Instance: 9000; Time: 42.38s; loss: 391.0137; acc: 176507/178735=0.9875\n",
      "Instance: 10000; Time: 39.09s; loss: 276.4437; acc: 195784/198206=0.9878\n",
      "Instance: 11000; Time: 36.42s; loss: 317.3367; acc: 214210/216836=0.9879\n",
      "Instance: 12000; Time: 39.81s; loss: 311.1819; acc: 233637/236474=0.9880\n",
      "Instance: 13000; Time: 38.99s; loss: 286.2344; acc: 253054/256101=0.9881\n",
      "Instance: 14000; Time: 42.31s; loss: 284.2552; acc: 273806/277068=0.9882\n",
      "Instance: 15000; Time: 40.35s; loss: 309.6700; acc: 293583/297082=0.9882\n",
      "Instance: 16000; Time: 38.28s; loss: 241.0145; acc: 312487/316181=0.9883\n",
      "Instance: 17000; Time: 40.24s; loss: 232.7756; acc: 332279/336136=0.9885\n",
      "Instance: 18000; Time: 38.86s; loss: 252.8983; acc: 351463/355525=0.9886\n",
      "Instance: 19000; Time: 39.63s; loss: 303.7933; acc: 371066/375387=0.9885\n",
      "Instance: 20000; Time: 38.11s; loss: 316.8598; acc: 389950/394556=0.9883\n",
      "Instance: 21000; Time: 39.22s; loss: 304.5226; acc: 409216/414024=0.9884\n",
      "Instance: 22000; Time: 38.50s; loss: 257.8867; acc: 428355/433360=0.9885\n",
      "Instance: 23000; Time: 38.61s; loss: 381.0402; acc: 447666/452956=0.9883\n",
      "     Instance: 23408; Time: 16.80s; loss: 104.6673; acc: 455900/461291=0.9883\n",
      "Epoch: 10 training finished. Time: 922.91s, speed: 25.36st/s,  total loss: 7256.505157470703\n",
      "gold_num =  13630  pred_num =  13569  right_num =  12743\n",
      "Dev: time: 167.74s, speed: 44.08st/s; acc: 0.9814, p: 0.9391, r: 0.9349, f: 0.9370\n",
      "Exceed previous best f score: 0.9352274400117767\n",
      "gold_num =  24150  pred_num =  24307  right_num =  21044\n",
      "Test: time: 226.52s, speed: 58.89st/s; acc: 0.9695, p: 0.8658, r: 0.8714, f: 0.8686\n",
      "Epoch: 11/18\n",
      " Learning rate is setted as: 0.008532001384146894\n",
      "Instance: 1000; Time: 39.91s; loss: 327.5818; acc: 19892/20165=0.9865\n",
      "Instance: 2000; Time: 40.37s; loss: 249.7758; acc: 39651/40098=0.9889\n",
      "Instance: 3000; Time: 41.75s; loss: 275.4932; acc: 60095/60754=0.9892\n",
      "Instance: 4000; Time: 38.27s; loss: 301.7017; acc: 78844/79753=0.9886\n",
      "Instance: 5000; Time: 41.01s; loss: 319.4971; acc: 98841/99998=0.9884\n",
      "Instance: 6000; Time: 39.13s; loss: 275.3375; acc: 118242/119636=0.9883\n",
      "Instance: 7000; Time: 41.26s; loss: 367.9500; acc: 138146/139787=0.9883\n",
      "Instance: 8000; Time: 38.54s; loss: 257.9496; acc: 157405/159267=0.9883\n",
      "Instance: 9000; Time: 40.23s; loss: 338.3552; acc: 177318/179405=0.9884\n",
      "Instance: 10000; Time: 40.94s; loss: 405.9902; acc: 197630/199998=0.9882\n",
      "Instance: 11000; Time: 40.68s; loss: 342.2198; acc: 217770/220415=0.9880\n",
      "Instance: 12000; Time: 36.70s; loss: 275.7981; acc: 236066/238925=0.9880\n",
      "Instance: 13000; Time: 40.60s; loss: 282.3045; acc: 256253/259321=0.9882\n",
      "Instance: 14000; Time: 41.31s; loss: 300.7255; acc: 276136/279408=0.9883\n",
      "Instance: 15000; Time: 37.81s; loss: 246.7983; acc: 294745/298158=0.9886\n",
      "Instance: 16000; Time: 41.19s; loss: 325.0992; acc: 314844/318469=0.9886\n",
      "Instance: 17000; Time: 37.85s; loss: 242.2625; acc: 332970/336787=0.9887\n",
      "Instance: 18000; Time: 61.48s; loss: 223.2762; acc: 351784/355785=0.9888\n",
      "Instance: 19000; Time: 67.80s; loss: 291.9650; acc: 371657/375890=0.9887\n",
      "Instance: 20000; Time: 49.07s; loss: 255.7780; acc: 390473/394906=0.9888\n",
      "Instance: 21000; Time: 36.43s; loss: 250.3700; acc: 408835/413424=0.9889\n",
      "Instance: 22000; Time: 41.02s; loss: 240.7239; acc: 429007/433778=0.9890\n",
      "Instance: 23000; Time: 38.62s; loss: 260.5714; acc: 447849/452834=0.9890\n",
      "     Instance: 23408; Time: 17.13s; loss: 153.2202; acc: 456212/461291=0.9890\n",
      "Epoch: 11 training finished. Time: 989.10s, speed: 23.67st/s,  total loss: 6810.7447509765625\n",
      "gold_num =  13630  pred_num =  13582  right_num =  12755\n",
      "Dev: time: 124.72s, speed: 59.31st/s; acc: 0.9813, p: 0.9391, r: 0.9358, f: 0.9375\n",
      "Exceed previous best f score: 0.9370197433729182\n",
      "gold_num =  24150  pred_num =  24335  right_num =  21091\n",
      "Test: time: 216.97s, speed: 61.49st/s; acc: 0.9696, p: 0.8667, r: 0.8733, f: 0.8700\n",
      "Epoch: 12/18\n",
      " Learning rate is setted as: 0.00810540131493955\n",
      "Instance: 1000; Time: 41.58s; loss: 310.5710; acc: 20076/20280=0.9899\n",
      "Instance: 2000; Time: 41.86s; loss: 212.7252; acc: 40468/40851=0.9906\n",
      "Instance: 3000; Time: 40.31s; loss: 247.5601; acc: 60179/60759=0.9905\n",
      "Instance: 4000; Time: 37.26s; loss: 216.8041; acc: 78549/79296=0.9906\n",
      "Instance: 5000; Time: 36.98s; loss: 257.2106; acc: 96865/97765=0.9908\n",
      "Instance: 6000; Time: 38.09s; loss: 245.1202; acc: 115600/116655=0.9910\n",
      "Instance: 7000; Time: 37.66s; loss: 271.0848; acc: 134215/135445=0.9909\n",
      "Instance: 8000; Time: 36.63s; loss: 239.8819; acc: 152491/153886=0.9909\n",
      "Instance: 9000; Time: 40.39s; loss: 243.8798; acc: 171904/173506=0.9908\n",
      "Instance: 10000; Time: 40.39s; loss: 253.7157; acc: 191548/193311=0.9909\n",
      "Instance: 11000; Time: 41.98s; loss: 261.1555; acc: 212533/214521=0.9907\n",
      "Instance: 12000; Time: 39.31s; loss: 318.3946; acc: 232156/234377=0.9905\n",
      "Instance: 13000; Time: 40.74s; loss: 340.1154; acc: 251924/254418=0.9902\n",
      "Instance: 14000; Time: 39.02s; loss: 261.8277; acc: 271122/273780=0.9903\n",
      "Instance: 15000; Time: 39.18s; loss: 233.3023; acc: 290826/293668=0.9903\n",
      "Instance: 16000; Time: 40.42s; loss: 245.3703; acc: 310515/313577=0.9902\n",
      "Instance: 17000; Time: 43.57s; loss: 317.4879; acc: 331383/334646=0.9902\n",
      "Instance: 18000; Time: 37.73s; loss: 304.9944; acc: 350042/353511=0.9902\n",
      "Instance: 19000; Time: 51.70s; loss: 342.0558; acc: 368386/372127=0.9899\n",
      "Instance: 20000; Time: 61.77s; loss: 289.7505; acc: 387313/391307=0.9898\n",
      "Instance: 21000; Time: 61.87s; loss: 254.5634; acc: 406741/410950=0.9898\n",
      "Instance: 22000; Time: 53.69s; loss: 269.8723; acc: 427929/432297=0.9899\n",
      "Instance: 23000; Time: 41.88s; loss: 332.6677; acc: 448450/453051=0.9898\n",
      "     Instance: 23408; Time: 16.22s; loss: 134.1171; acc: 456596/461291=0.9898\n",
      "Epoch: 12 training finished. Time: 1000.22s, speed: 23.40st/s,  total loss: 6404.228210449219\n",
      "gold_num =  13630  pred_num =  13544  right_num =  12753\n",
      "Dev: time: 165.31s, speed: 44.73st/s; acc: 0.9824, p: 0.9416, r: 0.9357, f: 0.9386\n",
      "Exceed previous best f score: 0.9374540643833602\n",
      "gold_num =  24150  pred_num =  24290  right_num =  21081\n",
      "Test: time: 225.18s, speed: 59.24st/s; acc: 0.9698, p: 0.8679, r: 0.8729, f: 0.8704\n",
      "Epoch: 13/18\n",
      " Learning rate is setted as: 0.007700131249192572\n",
      "Instance: 1000; Time: 37.78s; loss: 205.3608; acc: 19280/19424=0.9926\n",
      "Instance: 2000; Time: 38.34s; loss: 241.4301; acc: 38308/38644=0.9913\n",
      "Instance: 3000; Time: 39.17s; loss: 249.4446; acc: 57856/58376=0.9911\n",
      "Instance: 4000; Time: 43.04s; loss: 275.6495; acc: 78956/79675=0.9910\n",
      "Instance: 5000; Time: 34.92s; loss: 234.2225; acc: 96669/97552=0.9909\n",
      "Instance: 6000; Time: 40.50s; loss: 329.9604; acc: 116365/117487=0.9905\n",
      "Instance: 7000; Time: 41.60s; loss: 271.8781; acc: 136995/138335=0.9903\n",
      "Instance: 8000; Time: 41.00s; loss: 292.2615; acc: 157259/158799=0.9903\n",
      "Instance: 9000; Time: 37.88s; loss: 271.7961; acc: 176190/177919=0.9903\n",
      "Instance: 10000; Time: 38.94s; loss: 249.3253; acc: 195611/197531=0.9903\n",
      "Instance: 11000; Time: 38.71s; loss: 204.1531; acc: 214865/216946=0.9904\n",
      "Instance: 12000; Time: 40.15s; loss: 355.8385; acc: 234853/237159=0.9903\n",
      "Instance: 13000; Time: 40.69s; loss: 217.4441; acc: 254899/257382=0.9904\n",
      "Instance: 14000; Time: 41.13s; loss: 377.4162; acc: 275214/277962=0.9901\n",
      "Instance: 15000; Time: 38.47s; loss: 212.5969; acc: 294365/297265=0.9902\n",
      "Instance: 16000; Time: 40.61s; loss: 204.4954; acc: 314179/317241=0.9903\n",
      "Instance: 17000; Time: 37.08s; loss: 228.0900; acc: 333197/336407=0.9905\n",
      "Instance: 18000; Time: 42.11s; loss: 333.1485; acc: 353695/357160=0.9903\n",
      "Instance: 19000; Time: 39.55s; loss: 276.3107; acc: 372801/376463=0.9903\n",
      "Instance: 20000; Time: 52.58s; loss: 220.8853; acc: 391819/395662=0.9903\n",
      "Instance: 21000; Time: 64.31s; loss: 261.9752; acc: 411086/415126=0.9903\n",
      "Instance: 22000; Time: 63.19s; loss: 188.4841; acc: 430342/434508=0.9904\n",
      "Instance: 23000; Time: 35.85s; loss: 249.8134; acc: 448242/452609=0.9904\n",
      "     Instance: 23408; Time: 17.08s; loss: 87.5710; acc: 456860/461291=0.9904\n",
      "Epoch: 13 training finished. Time: 984.67s, speed: 23.77st/s,  total loss: 6039.551330566406\n",
      "gold_num =  13630  pred_num =  13530  right_num =  12779\n",
      "Dev: time: 125.37s, speed: 59.00st/s; acc: 0.9821, p: 0.9445, r: 0.9376, f: 0.9410\n",
      "Exceed previous best f score: 0.9386177964230514\n",
      "gold_num =  24150  pred_num =  24291  right_num =  21024\n",
      "Test: time: 218.17s, speed: 61.15st/s; acc: 0.9695, p: 0.8655, r: 0.8706, f: 0.8680\n",
      "Epoch: 14/18\n",
      " Learning rate is setted as: 0.007315124686732943\n",
      "Instance: 1000; Time: 40.75s; loss: 281.3970; acc: 20136/20331=0.9904\n",
      "Instance: 2000; Time: 36.67s; loss: 280.9323; acc: 38411/38795=0.9901\n",
      "Instance: 3000; Time: 41.87s; loss: 290.3637; acc: 58806/59407=0.9899\n",
      "Instance: 4000; Time: 37.88s; loss: 239.2343; acc: 78157/78913=0.9904\n",
      "Instance: 5000; Time: 38.23s; loss: 224.4919; acc: 97209/98113=0.9908\n",
      "Instance: 6000; Time: 40.19s; loss: 263.4263; acc: 117010/118135=0.9905\n",
      "Instance: 7000; Time: 39.84s; loss: 262.2826; acc: 136464/137762=0.9906\n",
      "Instance: 8000; Time: 40.67s; loss: 231.1264; acc: 156285/157777=0.9905\n",
      "Instance: 9000; Time: 43.57s; loss: 222.9755; acc: 177504/179165=0.9907\n",
      "Instance: 10000; Time: 37.12s; loss: 230.6262; acc: 195804/197630=0.9908\n",
      "Instance: 11000; Time: 39.99s; loss: 237.0952; acc: 215641/217653=0.9908\n",
      "Instance: 12000; Time: 37.69s; loss: 205.4435; acc: 234318/236504=0.9908\n",
      "Instance: 13000; Time: 40.92s; loss: 249.1536; acc: 254197/256550=0.9908\n",
      "Instance: 14000; Time: 38.63s; loss: 216.1084; acc: 273204/275779=0.9907\n",
      "Instance: 15000; Time: 41.06s; loss: 207.8270; acc: 293517/296261=0.9907\n",
      "Instance: 16000; Time: 37.32s; loss: 230.2657; acc: 312027/314928=0.9908\n",
      "Instance: 17000; Time: 37.10s; loss: 211.5969; acc: 330771/333792=0.9909\n",
      "Instance: 18000; Time: 37.07s; loss: 219.9350; acc: 349526/352703=0.9910\n",
      "Instance: 19000; Time: 40.19s; loss: 257.4627; acc: 369501/372860=0.9910\n",
      "Instance: 20000; Time: 43.85s; loss: 252.0212; acc: 390925/394459=0.9910\n",
      "Instance: 21000; Time: 39.31s; loss: 213.9435; acc: 410406/414087=0.9911\n",
      "Instance: 22000; Time: 41.49s; loss: 227.9691; acc: 430623/434494=0.9911\n",
      "Instance: 23000; Time: 38.45s; loss: 267.5744; acc: 449666/453753=0.9910\n",
      "     Instance: 23408; Time: 15.23s; loss: 115.9362; acc: 457125/461291=0.9910\n",
      "Epoch: 14 training finished. Time: 925.08s, speed: 25.30st/s,  total loss: 5639.188720703125\n",
      "gold_num =  13630  pred_num =  13420  right_num =  12623\n",
      "Dev: time: 123.50s, speed: 59.89st/s; acc: 0.9812, p: 0.9406, r: 0.9261, f: 0.9333\n",
      "gold_num =  24150  pred_num =  23860  right_num =  20897\n",
      "Test: time: 216.72s, speed: 61.54st/s; acc: 0.9698, p: 0.8758, r: 0.8653, f: 0.8705\n",
      "Epoch: 15/18\n",
      " Learning rate is setted as: 0.006949368452396296\n",
      "Instance: 1000; Time: 42.88s; loss: 221.7570; acc: 21198/21361=0.9924\n",
      "Instance: 2000; Time: 40.46s; loss: 242.0018; acc: 41366/41684=0.9924\n",
      "Instance: 3000; Time: 39.50s; loss: 205.8290; acc: 60953/61445=0.9920\n",
      "Instance: 4000; Time: 39.29s; loss: 172.7940; acc: 80796/81415=0.9924\n",
      "Instance: 5000; Time: 40.99s; loss: 195.1537; acc: 101243/102036=0.9922\n",
      "Instance: 6000; Time: 41.67s; loss: 209.4736; acc: 121668/122634=0.9921\n",
      "Instance: 7000; Time: 38.73s; loss: 244.8433; acc: 141186/142320=0.9920\n",
      "Instance: 8000; Time: 38.86s; loss: 228.3068; acc: 160491/161774=0.9921\n",
      "Instance: 9000; Time: 35.76s; loss: 203.0669; acc: 178632/180035=0.9922\n",
      "Instance: 10000; Time: 40.85s; loss: 260.8625; acc: 198808/200376=0.9922\n",
      "Instance: 11000; Time: 40.49s; loss: 234.7777; acc: 218996/220711=0.9922\n",
      "Instance: 12000; Time: 36.44s; loss: 168.1838; acc: 237583/239411=0.9924\n",
      "Instance: 13000; Time: 39.12s; loss: 241.8859; acc: 256952/258949=0.9923\n",
      "Instance: 14000; Time: 40.04s; loss: 203.3456; acc: 276580/278751=0.9922\n",
      "Instance: 15000; Time: 39.72s; loss: 242.7209; acc: 296370/298715=0.9921\n",
      "Instance: 16000; Time: 37.13s; loss: 280.9193; acc: 315054/317578=0.9921\n",
      "Instance: 17000; Time: 36.78s; loss: 173.0543; acc: 333362/336059=0.9920\n",
      "Instance: 18000; Time: 38.25s; loss: 202.6766; acc: 352413/355303=0.9919\n",
      "Instance: 19000; Time: 39.47s; loss: 257.3010; acc: 371724/374810=0.9918\n",
      "Instance: 20000; Time: 38.05s; loss: 269.9412; acc: 390592/393893=0.9916\n",
      "Instance: 21000; Time: 41.00s; loss: 148.3384; acc: 410819/414235=0.9918\n",
      "Instance: 22000; Time: 40.06s; loss: 233.8511; acc: 430651/434251=0.9917\n",
      "Instance: 23000; Time: 38.04s; loss: 230.0599; acc: 449663/453441=0.9917\n",
      "     Instance: 23408; Time: 15.84s; loss: 128.0301; acc: 457434/461291=0.9916\n",
      "Epoch: 15 training finished. Time: 919.42s, speed: 25.46st/s,  total loss: 5199.17431640625\n",
      "gold_num =  13630  pred_num =  13461  right_num =  12762\n",
      "Dev: time: 122.93s, speed: 60.17st/s; acc: 0.9829, p: 0.9481, r: 0.9363, f: 0.9422\n",
      "Exceed previous best f score: 0.9410162002945508\n",
      "gold_num =  24150  pred_num =  24119  right_num =  20981\n",
      "Test: time: 216.83s, speed: 61.53st/s; acc: 0.9696, p: 0.8699, r: 0.8688, f: 0.8693\n",
      "Epoch: 16/18\n",
      " Learning rate is setted as: 0.00660190002977648\n",
      "Instance: 1000; Time: 38.96s; loss: 222.3416; acc: 19860/20013=0.9924\n",
      "Instance: 2000; Time: 38.79s; loss: 195.7567; acc: 39197/39513=0.9920\n",
      "Instance: 3000; Time: 38.27s; loss: 185.1353; acc: 58202/58613=0.9930\n",
      "Instance: 4000; Time: 45.70s; loss: 292.9456; acc: 80523/81131=0.9925\n",
      "Instance: 5000; Time: 60.21s; loss: 185.7419; acc: 101372/102095=0.9929\n",
      "Instance: 6000; Time: 72.92s; loss: 195.6913; acc: 120423/121292=0.9928\n",
      "Instance: 7000; Time: 78.63s; loss: 256.3854; acc: 140405/141463=0.9925\n",
      "Instance: 8000; Time: 37.19s; loss: 186.4464; acc: 158834/160025=0.9926\n",
      "Instance: 9000; Time: 39.42s; loss: 216.8428; acc: 178656/180019=0.9924\n",
      "Instance: 10000; Time: 39.20s; loss: 241.5707; acc: 198058/199645=0.9921\n",
      "Instance: 11000; Time: 39.82s; loss: 201.8261; acc: 217795/219547=0.9920\n",
      "Instance: 12000; Time: 40.58s; loss: 193.9949; acc: 237834/239734=0.9921\n",
      "Instance: 13000; Time: 37.22s; loss: 194.3689; acc: 256968/259020=0.9921\n",
      "Instance: 14000; Time: 40.54s; loss: 175.1731; acc: 277046/279249=0.9921\n",
      "Instance: 15000; Time: 40.09s; loss: 179.7941; acc: 296572/298918=0.9922\n",
      "Instance: 16000; Time: 39.48s; loss: 202.4114; acc: 316154/318655=0.9922\n",
      "Instance: 17000; Time: 37.48s; loss: 249.4869; acc: 334895/337571=0.9921\n",
      "Instance: 18000; Time: 40.02s; loss: 233.0018; acc: 354708/357563=0.9920\n",
      "Instance: 19000; Time: 36.37s; loss: 169.3873; acc: 373044/376008=0.9921\n",
      "Instance: 20000; Time: 42.28s; loss: 247.8073; acc: 393417/396580=0.9920\n",
      "Instance: 21000; Time: 40.09s; loss: 267.6016; acc: 412972/416333=0.9919\n",
      "Instance: 22000; Time: 38.28s; loss: 231.6251; acc: 431940/435480=0.9919\n",
      "Instance: 23000; Time: 37.02s; loss: 184.6996; acc: 450442/454099=0.9919\n",
      "     Instance: 23408; Time: 14.56s; loss: 82.2642; acc: 457554/461291=0.9919\n",
      "Epoch: 16 training finished. Time: 1013.13s, speed: 23.10st/s,  total loss: 4992.2996826171875\n",
      "gold_num =  13630  pred_num =  13627  right_num =  12832\n",
      "Dev: time: 124.51s, speed: 59.41st/s; acc: 0.9819, p: 0.9417, r: 0.9415, f: 0.9416\n",
      "gold_num =  24150  pred_num =  24512  right_num =  21089\n",
      "Test: time: 216.97s, speed: 61.47st/s; acc: 0.9694, p: 0.8604, r: 0.8733, f: 0.8668\n",
      "Epoch: 17/18\n",
      " Learning rate is setted as: 0.006271805028287656\n",
      "Instance: 1000; Time: 39.31s; loss: 166.0830; acc: 19685/19811=0.9936\n",
      "Instance: 2000; Time: 40.06s; loss: 205.4558; acc: 39400/39662=0.9934\n",
      "Instance: 3000; Time: 40.17s; loss: 214.0040; acc: 59485/59899=0.9931\n",
      "Instance: 4000; Time: 36.55s; loss: 181.4402; acc: 77835/78376=0.9931\n",
      "Instance: 5000; Time: 40.94s; loss: 208.8069; acc: 98096/98785=0.9930\n",
      "Instance: 6000; Time: 38.05s; loss: 152.6100; acc: 117462/118270=0.9932\n",
      "Instance: 7000; Time: 39.96s; loss: 196.4283; acc: 137518/138473=0.9931\n",
      "Instance: 8000; Time: 35.65s; loss: 215.2560; acc: 156497/157621=0.9929\n",
      "Instance: 9000; Time: 37.50s; loss: 192.4406; acc: 175266/176574=0.9926\n",
      "Instance: 10000; Time: 39.02s; loss: 183.0162; acc: 194483/195936=0.9926\n",
      "Instance: 11000; Time: 37.95s; loss: 243.4229; acc: 213357/214971=0.9925\n",
      "Instance: 12000; Time: 37.80s; loss: 158.8268; acc: 232386/234100=0.9927\n",
      "Instance: 13000; Time: 40.55s; loss: 252.0027; acc: 252299/254197=0.9925\n",
      "Instance: 14000; Time: 40.19s; loss: 207.9019; acc: 272212/274269=0.9925\n",
      "Instance: 15000; Time: 41.85s; loss: 264.7604; acc: 293096/295314=0.9925\n",
      "Instance: 16000; Time: 39.63s; loss: 261.3214; acc: 312513/314907=0.9924\n",
      "Instance: 17000; Time: 37.04s; loss: 207.0186; acc: 331120/333698=0.9923\n",
      "Instance: 18000; Time: 38.33s; loss: 238.2703; acc: 350369/353111=0.9922\n",
      "Instance: 19000; Time: 40.58s; loss: 228.6003; acc: 370466/373364=0.9922\n",
      "Instance: 20000; Time: 40.91s; loss: 217.4259; acc: 390656/393730=0.9922\n",
      "Instance: 21000; Time: 39.89s; loss: 176.1657; acc: 410280/413486=0.9922\n",
      "Instance: 22000; Time: 42.36s; loss: 318.5469; acc: 430751/434240=0.9920\n",
      "Instance: 23000; Time: 39.28s; loss: 208.4128; acc: 449710/453360=0.9919\n",
      "     Instance: 23408; Time: 16.02s; loss: 108.1279; acc: 457562/461291=0.9919\n",
      "Epoch: 17 training finished. Time: 919.62s, speed: 25.45st/s,  total loss: 5006.345550537109\n",
      "gold_num =  13630  pred_num =  13674  right_num =  12812\n",
      "Dev: time: 123.38s, speed: 59.95st/s; acc: 0.9810, p: 0.9370, r: 0.9400, f: 0.9385\n",
      "gold_num =  24150  pred_num =  24573  right_num =  21171\n",
      "Test: time: 215.80s, speed: 61.81st/s; acc: 0.9696, p: 0.8616, r: 0.8766, f: 0.8690\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Tuning with bi-directional LSTM-CRF')\n",
    "    parser.add_argument('--embedding',  help='Embedding for words', default='None')\n",
    "    parser.add_argument('--status', choices=['train', 'test', 'decode'], help='update algorithm', default='train')\n",
    "    \n",
    "    parser.add_argument('--savemodel', default=\"../cyx＆xy/lattice/cyx/final/lattice-lstm_model_final417.model\")\n",
    "    parser.add_argument('--savedset', help='Dir of saved data setting', default=\"../cyx＆xy/lattice/cyx/final/model-train-data_final417.dset\")\n",
    "\n",
    "\n",
    "    parser.add_argument('--train', default=\"../cyx＆xy/lattice/cyx/final/train.txt\")\n",
    "    parser.add_argument('--dev', default=\"../cyx＆xy/lattice/cyx/final/dev.txt\" )\n",
    "    parser.add_argument('--test', default=\"../cyx＆xy/lattice/cyx/final/test.txt\")\n",
    "\n",
    "\n",
    "    parser.add_argument('--seg', default=\"True\") \n",
    "    parser.add_argument('--extendalphabet', default=\"True\") \n",
    "    parser.add_argument('--raw') \n",
    "    \n",
    "    parser.add_argument('--loadmodel',default=\"../cyx＆xy/lattice/cyx/final/lattice-lstm_model_final417.model\")\n",
    "    parser.add_argument('--output')\n",
    "    #args = parser.parse_args()\n",
    "    args = parser.parse_args(args=[])\n",
    "   \n",
    "    train_file = args.train\n",
    "    dev_file = args.dev\n",
    "    test_file = args.test\n",
    "    raw_file = args.raw\n",
    "    model_dir = args.loadmodel\n",
    "    dset_dir = args.savedset\n",
    "    output_file = args.output\n",
    "    if args.seg.lower() == \"true\":\n",
    "        seg = True \n",
    "    else:\n",
    "        seg = False\n",
    "    status = args.status.lower()\n",
    "\n",
    "    save_model_dir = args.savemodel\n",
    "    gpu = torch.cuda.is_available()\n",
    "\n",
    "    char_emb = \"../cyx＆xy/lattice/ner/data/gigaword_chn.all.a2b.uni.ite50.vec\"\n",
    "    \n",
    "    bichar_emb = '../cyx＆xy/lattice/ner/data/gigaword_chn.all.a2b.bi.ite50.vec'\n",
    "    \n",
    "    gaz_file = \"../cyx＆xy/lattice/ner/data/ctb.50d.vec\"\n",
    "    # gaz_file = None\n",
    "    # char_emb = None\n",
    "    #bichar_emb = None\n",
    "\n",
    "    print (\"CuDNN:\", torch.backends.cudnn.enabled)\n",
    "    # gpu = False\n",
    "    print (\"GPU available:\", gpu)\n",
    "    print (\"Status:\", status)\n",
    "    print (\"Seg: \", seg)\n",
    "    print (\"Train file:\", train_file)\n",
    "    print (\"Dev file:\", dev_file)\n",
    "    print (\"Test file:\", test_file)\n",
    "    print (\"Raw file:\", raw_file)\n",
    "    print (\"Char emb:\", char_emb)\n",
    "    print (\"Bichar emb:\", bichar_emb)\n",
    "    print (\"Gaz file:\",gaz_file)\n",
    "    if status == 'train':\n",
    "        print (\"Model saved to:\", save_model_dir)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if status == 'train':\n",
    "        data = Data()\n",
    "        data.HP_gpu = gpu\n",
    "        data.HP_use_char = True\n",
    "        data.HP_batch_size = 10\n",
    "        data.use_bigram = True\n",
    "        data.gaz_dropout = 0.5\n",
    "        data.norm_gaz_emb = False\n",
    "        data.HP_fix_gaz_emb = False\n",
    "        data_initialization(data, gaz_file, train_file, dev_file, test_file)\n",
    "\n",
    "        data.generate_instance_with_gaz(train_file,'train')\n",
    "        data.generate_instance_with_gaz(dev_file,'dev')\n",
    "        data.generate_instance_with_gaz(test_file,'test')\n",
    "\n",
    "        data.build_word_pretrain_emb(char_emb)\n",
    "        data.build_biword_pretrain_emb(bichar_emb)\n",
    "        data.build_gaz_pretrain_emb(gaz_file)\n",
    "        #data = load_data_setting(dset_dir)\n",
    "        model,train_log,dev_log = train(data, save_model_dir,dset_dir, seg)\n",
    "    elif status == 'test':      \n",
    "        data = load_data_setting(dset_dir)\n",
    "        data.generate_instance_with_gaz(dev_file,'dev')\n",
    "        load_model_decode(model_dir, data , 'dev', gpu, seg)\n",
    "        data.generate_instance_with_gaz(test_file,'test')\n",
    "        load_model_decode(model_dir, data, 'test', gpu, seg)\n",
    "    elif status == 'decode':       \n",
    "        data = load_data_setting(dset_dir)\n",
    "        data.generate_instance_with_gaz(raw_file,'raw')\n",
    "        decode_results = load_model_decode(model_dir, data, 'raw', gpu, seg)\n",
    "        data.write_decoded_results(output_file, decode_results, 'raw')\n",
    "    else:\n",
    "        print (\"Invalid argument! Please use valid arguments! (train/test/decode)\")\n",
    "\n",
    "torch.save(model,\"../cyx＆xy/lattice/cyx/final/417final.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00910f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eaf898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c49d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6ad3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa959fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd4fbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1073e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "483d46bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='../cyx＆xy/lattice/cyx/final/train_log.json'\n",
    "train_log_lst = []\n",
    "for i in train_log:\n",
    "    train_log_lst.append(str(i))\n",
    "with open(filename,'w',encoding='utf-8') as file_obj:\n",
    "    json.dump(train_log_lst,file_obj,ensure_ascii=False,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a2503a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='../cyx＆xy/lattice/cyx/final/dev_log.json'\n",
    "dev_log_lst = []\n",
    "for i in dev_log:\n",
    "    dev_log_lst.append(str(i))\n",
    "with open(filename,'w',encoding='utf-8') as file_obj:\n",
    "    json.dump(dev_log_lst,file_obj,ensure_ascii=False,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "49d3f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, model, name):\n",
    "    if name == \"train\":\n",
    "        instances = data.train_Ids\n",
    "    elif name == \"dev\":\n",
    "        instances = data.dev_Ids\n",
    "    elif name == 'test':\n",
    "        instances = data.test_Ids\n",
    "    elif name == 'raw':\n",
    "        instances = data.raw_Ids\n",
    "    else:\n",
    "        print (\"Error: wrong evaluate name,\", name)\n",
    "    pred_results = []\n",
    "    gold_results = []\n",
    "    ## set model in eval model\n",
    "    model.eval()\n",
    "    batch_size = 10\n",
    "    start_time = time.time()\n",
    "    train_num = len(instances)\n",
    "    total_batch = train_num//batch_size+1\n",
    "    for batch_id in range(total_batch):\n",
    "        start = batch_id*batch_size\n",
    "        end = (batch_id+1)*batch_size \n",
    "        if end >train_num:\n",
    "            end =  train_num\n",
    "        instance = instances[start:end]\n",
    "        if not instance:\n",
    "            continue\n",
    "        gaz_list,batch_word, batch_biword, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu, True)\n",
    "        tag_seq = model(gaz_list,batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask)\n",
    "        # print \"tag:\",tag_seq\n",
    "        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet, batch_wordrecover)\n",
    "        pred_results += pred_label\n",
    "        gold_results += gold_label\n",
    "    #decode_time = time.time() - start_time\n",
    "    #speed = len(instances)/decode_time\n",
    "    #acc, p, r, f = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)\n",
    "    return pred_results\n",
    "\n",
    "def load_model_decode_with_model_predict(model, data, name, gpu, seg=True):\n",
    "    data.HP_gpu = gpu\n",
    "    \n",
    "    print(\"Decode %s data ...\"%(name))\n",
    "    start_time = time.time()\n",
    "    pred_results = predict(data, model, name)\n",
    "    end_time = time.time()\n",
    "    time_cost = end_time - start_time\n",
    "    return pred_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "210fa474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold_num =  24150  pred_num =  24573  right_num =  21171\n"
     ]
    }
   ],
   "source": [
    "speed, acc, p, r, f, _, right_num = evaluate(data, model, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "629a89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='../cyx＆xy/lattice/cyx/final/test_log.json'\n",
    "with open(filename,'w',encoding='utf-8') as file_obj:\n",
    "    json.dump(_,file_obj,ensure_ascii=False,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "72b12a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[]\n",
    "for i in train_log:\n",
    "    train_loss.append(461291-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec3dfec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loss=[]\n",
    "for i in dev_log:\n",
    "    dev_loss.append(13630 -i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "748dc71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Lattice LSTM 训练曲线')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEeCAYAAAC30gOQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABERElEQVR4nO3deXxU1fn48c+TfSEJBEISw75kUZEgiFr0J2pVROvSupWqYP2WVtFSa+vy/dZq3apWAW0VxRUUxK1WalVccakCgqCIQAibBFkCYQkJ2Z/fH/ckDCEJySSTScjzfr3ua+6ce++Zcycz8+Qs91xRVYwxxpiWFhLsAhhjjDk8WYAxxhgTEBZgjDHGBIQFGGOMMQFhAcYYY0xAWIAxxhgTEBZgjPEhIieLyKpgl8OYw4HYdTCmvRGR9cD/qOr7TTxuHvCCqj7lk6bAQFXNbdFCHvi6fYB1QLiqVtTa1hmYBIwGYoHNwDPALOA7n11jgWKg+gt7NnA1MBa4QFXf8MlzMvA74CpVfa7W60UAH9ZVTlU9SUSeAI6qY/P1wPHA5XVsewZYDjxUx7Ylqnp9Xa9nDn9hwS6AMR3cZLzgkQXsBtKBo1X1e6BT9U4uEA72DYQicjWQA1wJvOHSwoBLgDX1vF4IsF5VDwgUIvKqW+2qqifV2nYdkAD0AcbVKsPRwEXAD8BzvsG7Vr6mA7ImMnPYEJEuIvKmiOSLyE633sNtuwc4GfiHiOwVkX+IyCfu0K9d2qUiMlJE8nzy7Cki/3R57hCRf/hs+6WIrHCvNVdEevtR7OOAWaq6U1WrVHWlqjblR/nfwEki0sU9HwV8A2zxoyzGtCgLMOZwEgI8C/QGegH7gH8AqOr/AZ8C16lqJ1W9TlX/nztusEt7yTczEQkF3gQ24P33ngbMdtvOB/4X+CmQ5PJ+0Y8yzwfuEZGrRGSgH8eX4NVeLnPPrwRm+JGPMS3OAow5bKjqDlV9TVWLVbUQuAc4pRlZDgeOAP6oqkWqWqKqn7ltvwH+qqorXL/KvUC2H7WY64GZwHXAdyKSKyJnNzGPGcCVrj/nFOBfTTzemICwAGMOGyISIyJPiMgGEdkDfAJ0djURf/QENtTumHd6Aw+LyC4R2QUUAIJXy2k0Vd2nqveq6lCgK/Ay8IqIJDYhj8/walH/B7ypqvuaUgZjAsUCjDmc3AhkAMerajxQ3QQm7rGpQyY3Ar1cx3ld236tqp19lmhV/dyvkgOqugevJhQL9G3i4S/gnb81j5k2wwKMaa/CRSTKZwkD4vD6XXa5GsDttY7ZCvRrRFq1hXjDhu8TkVj3OiPctseBW0XkKAARSRCRiw9R5shaZQ4RkdtE5DgRiRCRKGAisAto6rU4jwBn4NXajGkTLMCY9uotvGBSvdwBTAGige14nefv1DrmYeAiN+rrEZd2BzDdNXVd4ruzqlYCPwEGAN8DecClbtvrwP3AbNcc9y3etSkN2VurzKfh1aqedWX+AS9InKOqexv3NtSUtUBVP1C7sM20IXYdjGl3VLVPA5tH1nr+hM9xX+BdZ+Kb1+N4tRFfPXy2fw9cUE85ngeeb0R517O/ma6294G7G5HHQcer6rgG9j+pvm3GtBYLMMZ0PGe4WQ18VV+9n1THtjTgV259poj4DiKIBf7j1v8oIrWv9C9vZllNO2ZTxRhjjAkI64MxxhgTEBZgjDHGBETAA4yIhIrIEhF50z1/TkTWichSt2S7dBGRR9yVzN+IyLE+eYwVkdVuGeuTPlRElrljHhGR+jpSjTHGtLLW6OSfCKwA4n3S/ljHhH5nAwPdcjwwFTje53qGYXhDOheLyBxV3en2+RWwAG/Y6ijg7YYK061bN+1TVASxsZCWBuHhzT5BY4w53C1evHi7qiY15ZiABhg3k+05eHNC/f4Qu58PzHDj+OeLSGcRScUbdvqeqha4PN8DRrmRLvGqOt+lz8AbTtpggOm9azeL4uPgiitg8mS/z80YYzoSEdnQ1GMC3UQ2BbgJqKqVfo9rBpssIpEuLQ1v+o1qeS6tofS8OtIbtC7xCBg3DnbsaMJpGGOMaaqABRgRORfYpqqLa226FcjEuw9GInBzoMrgU5bxIrJIRBYVhUaw/c6/wgybsskYYwIpkDWYEcB54t3edjZwmoi8oKqb1VOKN0XGcLf/JrzZa6v1cGkNpfeoI/0gqjpNVYep6jCAVVsKm3tuxhhjDiFgfTCqeitebQURGQn8QVUvF5FUVd3sRnxdgDeHE8Ac4DoRmY3Xyb/b7TcXuNfnjn1nAreqaoGI7BGRE/A6+a8E/t6Ysq3cUsiIAd1a5DyNMcFXXl5OXl4eJSUlwS5KuxcVFUWPHj0Ib4EBUMGYKmamiCThzc20FO/GTeCNAhsN5ALFwFXgTeInIncBX7r97qzu8AeuBZ7Dm+DwbQ7RwQ8QFiLkWA3GmMNKXl4ecXFx9OnTB7tawX+qyo4dO8jLy6Nv36beMeJgrRJgVHUeMM+tn1bPPgpMqGfbM8AzdaQvAo5uSlkiw0JZudUCjDGHk5KSEgsuLUBE6Nq1K/n5+S2SX4e7kj8qPITVWwupqrI52Iw5nFhwaRkt+T52wAATSnFZJRt3Fge7KMYYc1jrkAEGvI5+Y0wHtnkzjBoFW7Y0O6tdu3bx2GOPNfm40aNHs2vXriYfN27cOF59tfZkKG1PBwww3inbUGVjOrgHHoCFC+H++5udVX0BpqKiosHj3nrrLTp37tzs12+rOtwNx0JE6JUYwyrr6DfmsPSXfy/nux/21Lv9+etPJaK81HuyZAkMGQJTplAWHskVf/+ozmOOPCKe239yVJ3bAG655RbWrFlDdnY24eHhREVF0aVLF1auXElOTg4XXHABGzdupKSkhIkTJzJ+/HgA+vTpw6JFi9i7dy9nn302J510Ep9//jlpaWm88cYbREdHH/J8P/jgA/7whz9QUVHBcccdx9SpU4mMjOSWW25hzpw5hIWFceaZZ/Lggw/yyiuv8Je//IXQ0FASEhL45JNPDpl/c3S4AAOQnhxnNRhjOqjr736Fy+c8wfCtq4nMzqZ0QDoLU9J5/ie/9jvP++67j2+//ZalS5cyb948zjnnHL799tuaob7PPPMMiYmJ7Nu3j+OOO46f/exndO3a9YA8Vq9ezYsvvsiTTz7JJZdcwmuvvcbll9e+QeiBSkpKGDduHB988AHp6elceeWVTJ06lSuuuILXX3+dlStXIiI1zXB33nknc+fOJS0tza+muabqkAEmMyWOj1Zto7Siksiw0GAXxxjTghqqadT44QOY8TkkJxNZXs7J547m5JvObbEyDB8+/IDrSB555BFef/11ADZu3Mjq1asPCjB9+/YlOzsbgKFDh7J+/fpDvs6qVavo27cv6enpAIwdO5ZHH32U6667jqioKK6++mrOPfdczj3XO7cRI0Ywbtw4LrnkEn7605+2wJk2rMP1wQBkpMRRWaXkbtsb7KIYY4KhoMCb9PbddwMy+W1sbGzN+rx583j//ff54osv+PrrrxkyZEidMw5ERkbWrIeGhh6y/6YhYWFhLFy4kIsuuog333yTUaNGAfD4449z9913s3HjRoYOHcqOAE/622FrMOB19B91REKQS2OMaXW+k91OmtTs7OLi4igsrLvZfffu3XTp0oWYmBhWrlzJ/Pnzm/161TIyMli/fj25ubkMGDCA559/nlNOOYW9e/dSXFzM6NGjGTFiBP369QNgzZo1HH/88Rx//PG8/fbbbNy48aCaVEvqkAGmT7dYIkJDrB/GGNMiunbtyogRIzj66KOJjo4mOTm5ZtuoUaN4/PHHycrKIiMjgxNOOKHFXjcqKopnn32Wiy++uKaT/ze/+Q0FBQWcf/75lJSUoKpMckH0j3/8I6tXr0ZVOf300xk8eHCLlaUu4s3Q0nEMGzZMFy1axNkPf0pyfCTPXTX80AcZY9q0FStWkJWVFexiHDbqej9FZHH1jPSN1SH7YAAykjtZDcYYYwKo4waYlHg27y5hd3F5sItijDF1mjBhAtnZ2Qcszz77bLCL1Wgdsg8GfDr6txYyvG9ikEtjjDEHe/TRR4NdhGbpwDWY6pFk9V/xa4wxxn8dNsCkJkQRFxVmk14aY0yABDzAiEioiCwRkTfd874iskBEckXkJRGJcOmR7nmu297HJ49bXfoqETnLJ32US8sVkVuaWC4yU+LIsTnJjDEmIFqjBjMRWOHz/H5gsqoOAHYCV7v0q4GdLn2y2w8RORK4DDgKGAU85oJWKPAocDZwJPBzt2+jZaTEsXJLIR1tqLYxxrSGgAYYEekBnAM85Z4LcBpQfSOD6cAFbv189xy3/XS3//nAbFUtVdV1QC4w3C25qrpWVcuA2W7fRstIjqOwpILNuw+etsEYc/iaNauKrKxCQkO9x1mzqlo0/zvuuIMHH3ywRfJqL/d+qUugazBTgJuA6r9eV2CXqlZPspMHpLn1NGAjgNu+2+1fk17rmPrSGy0jJR6we8MY05HMmlXFTTdtY/z485g7N5Lx48/jppu2tXiQMQEcpiwi5wLbVHWxiIwM1Os0sizjgfEAvXr1qknPSPZGkq3cUsipmd2DUjZjTAt7+xbYsqzezXf9+RFuvPF/GDJkHgBDhszjxht/zl03P8WY0t/WfVDKIDj7vgZf9p577mH69Ol0796dnj17MnToUNasWcOECRPIz88nJiaGJ598ktTUVI455hjWrVtHSEgIRUVFZGZmsnbtWsLDwxt8jbZ875e6BPI6mBHAeSIyGogC4oGHgc4iEuZqKT2ATW7/TUBPIE9EwoAEYIdPejXfY+pLP4CqTgOmgTdVTHV6Qkw4qQlRNlTZmA4k54c+DBr02QFpgwZ9Rs4PffzOc/HixcyePZulS5dSUVHBsccey9ChQxk/fjyPP/44AwcOZMGCBVx77bV8+OGHZGdn8/HHH3Pqqafy5ptvctZZZx0yuLT1e7/UJWABRlVvBW4FcDWYP6jqL0TkFeAivD6TscAb7pA57vkXbvuHqqoiMgeYJSKTgCOAgcBCQICBItIXL7BcBoxpajmrO/qNMYeJQ9Q00h8oZNmyk2pqMADLlp1EenoxXPUfv17y008/5cILLyQmJgaA8847j5KSEj7//HMuvvjimv1KS707aV566aW89NJLnHrqqcyePZtrr732kK/R1u/9UpdgXAdzM/B7EcnF62N52qU/DXR16b8HbgFQ1eXAy8B3wDvABFWtdDWg64C5eKPUXnb7NklGShxr84sor7T2V2M6gttui+Whh15kyZKRVFSEsWTJSB566EVuuy320Ac3QVVVFZ07d2bp0qU1y4oV3oDa8847j3feeYeCggIWL17Maaed5vfrtJV7v9RJVTvUMnToUPX12uKN2vvmNzVnyx41xrRP3333XZP2nzmzUjMz92hIiPc4c2Zls15/8eLFOmjQIC0uLtY9e/bogAED9G9/+5ueeOKJ+vLLL6uqalVVlS5durTmmIsuukgvv/xyveaaaxrMe+zYsfrKK6/ovn37tGfPnrp69eqa9ClTpmhhYaFu3bpVVVV37dqliYmJqqqam5tbk8ewYcN0yZIljT6fut5PYJE28fe2w85FVq16ypiVWwoZ6Dr9jTGHtzFjQhgzpvr73vzv/bHHHsull17K4MGD6d69O8cddxwAM2fO5JprruHuu++mvLycyy67rOYeLJdeeikXX3wx8+bNa9RrtPV7v9Slw94PplppRSVH/nku15zSnz+clRHEkhlj/GX3g2lZdj+YFhIZFkrfbrHW0W+MMS2swzeRgddM9k3ermAXwxjTgU2YMIH//ve/B6RNnDiRq666Kkglaj4LMEBmchz/+WYzRaUVxEbaW2KMaX3t/d4vdenwTWSwv6PfZlY2pv3qaP3JgdKS76MFGHxvPmYBxpj2KCoqih07dliQaSZVZceOHURFRbVIftYeBPTsEkNMRKh19BvTTvXo0YO8vDzy8/ODXZR2Lyoqih49erRIXhZggJAQYWBynNVgjGmnwsPD6du3b7CLYWqxJjInMzmOVVvt5mPGGNNSLMA4GSlxFBSVsX1vWbCLYowxhwULME6mdfQbY0yLsgDjpNfMSWb3hjHGmJZgAcbp1imSbp0irAZjjDEtxAKMj4wUr6PfGGNM81mA8ZGRHE/O1kIqq2wkmTHGNFfAAoyIRInIQhH5WkSWi8hfXPpzIrJORJa6Jduli4g8IiK5IvKNiBzrk9dYEVntlrE+6UNFZJk75hERkeaUOTMljpLyKjYWFDcnG2OMMQS2BlMKnKaqg4FsYJSInOC2/VFVs92y1KWdDQx0y3hgKoCIJAK3A8cDw4HbRaSLO2Yq8Cuf40Y1p8C+Nx8zxhjTPAELMO4um3vd03C3NNT2dD4wwx03H+gsIqnAWcB7qlqgqjuB9/CCVSoQr6rz3e08ZwAXNKfMA5M7IWJDlY0xpiUEtA9GREJFZCmwDS9ILHCb7nHNYJNFJNKlpQEbfQ7Pc2kNpefVke63mIgweiXGsGqrDVU2xpjmCmiAUdVKVc0GegDDReRo4FYgEzgOSARuDmQZAERkvIgsEpFFh5oMLyM5zprIjDGmBbTKKDJV3QV8BIxS1c2uGawUeBavXwVgE9DT57AeLq2h9B51pNf1+tNUdZiqDktKSmqwrJkpcazfXkRJeWVjT88YY0wdAjmKLElEOrv1aOAMYKXrO8GN+LoA+NYdMge40o0mOwHYraqbgbnAmSLSxXXunwnMddv2iMgJLq8rgTeaW+6MlHiqFHK37T30zsYYY+oVyOn6U4HpIhKKF8heVtU3ReRDEUkCBFgK/Mbt/xYwGsgFioGrAFS1QETuAr50+92pqgVu/VrgOSAaeNstzeJ787Gj0xKam50xxnRYAQswqvoNMKSO9NPq2V+BCfVsewZ4po70RcDRzSvpgfp0jSEiLMSu6DfGmGayK/lrCQsNYUBSJ+voN8aYZrIAU4fMlDhW2azKxhjTLBZg6pCREsfWPaXsKrabjxljjL8swNTBpowxxpjmswBTh8yUeAByrKPfGGP8ZgGmDsnxkSREh1sNxhhjmsECTB1ExLv5mAUYY4zxmwWYemQkx5GzpRDv8hxjjDFNZQGmHhkpcRSWVrBp175gF8UYY9olCzD1yPSZMsYYY0zTWYCpR3p1gLGRZMYY4xcLMPWIjwonrXO01WCMMcZPFmAaYCPJjDHGfxZgGpCeHMea/L2UV1YFuyjGGNPuWIBpQGZKHOWVytr8omAXxRhj2h0LMA3YPyeZzaxsjDFNFchbJkeJyEIR+VpElovIX1x6XxFZICK5IvKSiES49Ej3PNdt7+OT160ufZWInOWTPsql5YrILS19Dv2TOhEWIjYnmTHG+CGQNZhS4DRVHQxkA6NE5ATgfmCyqg4AdgJXu/2vBna69MluP0TkSOAy4ChgFPCYiIS6WzE/CpwNHAn83O3bYiLCQuiXFGsd/cYY44eABRj17HVPw92iwGnAqy59OnCBWz/fPcdtP11ExKXPVtVSVV0H5ALD3ZKrqmtVtQyY7fZtURkp8TbppTHG+CGgfTCuprEU2Aa8B6wBdqlqhdslD0hz62nARgC3fTfQ1Te91jH1pbeozJQ48nbuY29pxaF3NsYYUyOgAUZVK1U1G+iBV+PIDOTr1UdExovIIhFZlJ+f36Rj05NtyhhjjPFHq4wiU9VdwEfAiUBnEQlzm3oAm9z6JqAngNueAOzwTa91TH3pdb3+NFUdpqrDkpKSmlR2m5PMGGP8E8hRZEki0tmtRwNnACvwAs1FbrexwBtufY57jtv+oXpz5c8BLnOjzPoCA4GFwJfAQDcqLQJvIMCclj6PtM7RxEaEssqGKhtjTJOEHXoXv6UC091orxDgZVV9U0S+A2aLyN3AEuBpt//TwPMikgsU4AUMVHW5iLwMfAdUABNUtRJARK4D5gKhwDOqurylTyIkREhPibNJL40xpokCFmBU9RtgSB3pa/H6Y2qnlwAX15PXPcA9daS/BbzV7MIeQmZKHO98uwVVxRvYZowx5lDsSv5GyEiOY2dxOfmFpcEuijHGtBsWYBohvWbKGGsmM8aYxrIA0wiZKfGAjSQzxpimsADTCImxESTFRVoNxhhjmsACTCNlpsTZpJfGGNMEFmAaKSPZCzCVVRrsohhjTLtgAaaRMlLiKK2oYsMOu/mYMcY0hgWYRsqwKWOMMaZJLMA00sDucYjYUGVjjGksCzCNFB0RSp+udvMxY4xpLAswTVDd0W+MMebQLMA0QUZKHOt3FFFSXhnsohhjTJtnAaYJMlPiqFJYvXXvoXc2xpgOzgJME2TUzElm94YxxphDsQDTBL27xhIZFmId/cYY0wgWYJogNEQYmNzJbj5mjDGNEMhbJvcUkY9E5DsRWS4iE136HSKySUSWumW0zzG3ikiuiKwSkbN80ke5tFwRucUnva+ILHDpL7lbJwdURnK81WCMMaYRAlmDqQBuVNUjgROACSJypNs2WVWz3fIWgNt2GXAUMAp4TERC3S2XHwXOBo4Efu6Tz/0urwHATuDqAJ4P4HX0byssZWdRWaBfyhhj2rWABRhV3ayqX7n1QmAFkNbAIecDs1W1VFXXAbl4t1YeDuSq6lpVLQNmA+eLd+/i04BX3fHTgQsCcjI+MuzmY8YY0yit0gcjIn2AIcACl3SdiHwjIs+ISBeXlgZs9Dksz6XVl94V2KWqFbXSAyqzZk4yG0lmjDENCXiAEZFOwGvA71R1DzAV6A9kA5uBh1qhDONFZJGILMrPz29WXklxkXSOCbeOfmOMOYSABhgRCccLLjNV9Z8AqrpVVStVtQp4Eq8JDGAT0NPn8B4urb70HUBnEQmrlX4QVZ2mqsNUdVhSUlJzz4mM5DhrIjPGmENoVIARkYkiEi+ep0XkKxE58xDHCPA0sEJVJ/mkp/rsdiHwrVufA1wmIpEi0hcYCCwEvgQGuhFjEXgDAeaoqgIfARe548cCbzTmfJorMyWOnC2FVNnNx4wxpl6NrcH80jVvnQl0Aa4A7jvEMSPcfqfVGpL8gIgsE5FvgFOBGwBUdTnwMvAd8A4wwdV0KoDrgLl4AwVedvsC3Az8XkRy8fpknm7k+TRLRko8RWWVbNq1rzVezhhj2qWwQ+8CgLjH0cDzqrrc1VDqpaqf+Rzn660GjrkHuKeO9LfqOk5V17K/ia3V+N58rGdiTGu/vDHGtAuNrcEsFpF38QLMXBGJA6oCV6y2rSbAWEe/McbUq7EB5mrgFuA4VS0GwoGrAlaqNq5TZBiDQ4s588ZxsGVLsItjjDFtUmMDzInAKlXdJSKXA38CdgeuWG3fxK/+Rf9138H99we7KMYY0yY1NsBMBYpFZDBwI7AGmBGwUrVl0dEgwmnvzCLkow9hyhQQ8dKNMcbUaGyAqXDDgs8H/qGqjwJxgStWG7Z2LYwbR2V6BmRne49XXQXr1gW7ZMYY06Y0dhRZoYjcijfs+GQRCcHrh+l4UlMhIYGQ/G2Udk0ivKoSjY9HUlKCXTJjjGlTGluDuRQoxbseZgveVfN/C1ip2rqCAmTcOD79x0xeyBzJ1vU/BLtExhjT5jSqBqOqW0RkJnCciJwLLFTVjtkHAzDDO/WRlVWcsSmCWWEhvFWlhIQ0eGmQMcZ0KI2dKuYSvGlbLgYuARaIyEUNH3X4CwsNYeLpA1m5pZB3lttwZWOM8dXYJrL/w7sGZqyqXol39fxtgStW+/GTwUfQPymWKe/n2Nxkxhjjo7EBJkRVt/k839GEYw9roSHC736cTs7Wvfxn2eZgF8cYY9qMxgaJd0RkroiME5FxwH9oYE6xjuacQamkJ3diyvs5VFotxhhjgEYGGFX9IzANOMYt01T15kAWrD0JcbWYNflF/PtrG1FmjDHQ+OtgUNXX8G4eZuow6qgUMlPiePiD1Zx7TCphodaCaIzp2Br8FRSRQhHZU8dSKCJ2U3ofISHCDWeks257EW8stVqMMcY0GGBUNU5V4+tY4lQ1vrUK2V6ceWQyRx0RzyMfrqa8ssPezcAYY4AAjgQTkZ4i8pGIfCciy0VkoktPFJH3RGS1e+zi0kVEHhGRXBH5RkSO9clrrNt/tYiM9Ukf6u6OmeuODeqVjiLCDT9OZ8OOYl7/alMwi2KMMUEXyI6CCuBGVT0SOAGYICJH4t1X5gNVHQh84J4DnA0MdMt4vBmcEZFE4HbgeLzrb26vDkpun1/5HDcqgOfTKKdndeeYHgk88uFqyiqsFmOM6bgCFmBUdbOqfuXWC4EVQBrejMzT3W7TgQvc+vnADPXMBzqLSCpwFvCeqhao6k7gPWCU2xavqvPdTM8zfPIKGhGvLyZv5z5eXZwX7OIYY0zQtMpQJxHpAwwBFgDJqlp9ReIWINmtpwEbfQ7Lc2kNpefVkR50I9OTGNKrM49+lEtpRWWwi2OMMUER8AAjIp3whjf/TlUPGHnmah4BvzJRRMaLyCIRWZSfnx/ol6vpi9m0ax8vL7JajDGmYwpogBGRcLzgMlNV/+mSt7rmLdxj9RQ0m4CePof3cGkNpfeoI/0gqjpNVYep6rCkpKTmnVQjnTywG8N6d+HRD3MpKbdajDGm4wnkKDIBngZWqOokn01zgOqRYGOBN3zSr3SjyU4AdrumtLnAmSLSxXXunwnMddv2iMgJ7rWu9Mkr6ESE35+RzpY9Jcxe+H2wi2OMMa0ukDWYEXh3wDxNRJa6ZTRwH3CGiKwGfuyegze32VogF3gSuBZAVQuAu4Av3XKnS8Pt85Q7Zg3wdgDPp8lO7N+V4/sm8ui8NVaLMcZ0OOJ1g3Qcw4YN00WLFrXa681fu4PLps3nT+dk8T8n92u11zXGmJYkIotVdVhTjrEJswLshH5d+VH/rjz+8RqKyyqCXRxjjGk1FmBawQ1npLN9bxkvzN8Q7KIYY0yrsQDTCo7rk8jJA7vx+MdrKSq1WowxpmOwANNKbjgjnYKiMqZ/sT7YRTHGmFZhAaaVHNurCyMzkpj2yVoKS8qDXRxjjAk4CzCt6IYfp7OruJzpn68PdlGMMSbgLMC0osE9O/PjrO5M+2Qte6wWY4w5zFmAaWW/+3E6e0oqeOazdcEuijHGBJQFmFZ2dFoCZx2VzNOfrmN3sdVijDGHLwswQfC7H6dTWFrBU5+tDXZRjDEmYCzABEFWajyjB6Xw7H/Xs7OoLNjFMcaYgLAAEyQTT0+nqKyCJz+1Wowx5vBkASZIMlLiOPeYI3ju8/Xs2Fsa7OIYY0yLswATRBNPH8C+8kqmfWK1GGPM4ccCTBAN6B7H+YOPYPoX68kvtFqMMebwYgEmyH57+kDKKqp44fX5MGoUbNkS7CIZY0yLsAATZP2SOnHhkB4kP/4wunAh3H9/sItkjDEtImABRkSeEZFtIvKtT9odIrKp1i2Uq7fdKiK5IrJKRM7ySR/l0nJF5Baf9L4issClvyQiEYE6l4CKjuahS7MZ88XryIcfwpQpIALR0cEumTHGNEsgazDPAaPqSJ+sqtlueQtARI4ELgOOcsc8JiKhIhIKPAqcDRwJ/NztC3C/y2sAsBO4OoDnEjhr18K4cZQNTIfsbMoGplM5dhyss6lkjDHtW8ACjKp+AhQ0cvfzgdmqWqqq64BcYLhbclV1raqWAbOB80VEgNOAV93x04ELWrL8rSY1FRISCN+eT3lSd8Ly83n/hxJ2dOoS7JIZY0yzBKMP5joR+cY1oVX/iqYBG332yXNp9aV3BXapakWt9DqJyHgRWSQii/Lz81vqPFpOQQEybhzh77/H+p9chOzYwfmP/peVW/YEu2TGGOO31g4wU4H+QDawGXioNV5UVaep6jBVHZaUlNQaL9k0M2bApEkweDD9Zkwj+V8vU1ZRxc8e+5x3l9uoMmNM+9SqAUZVt6pqpapWAU/iNYEBbAJ6+uzaw6XVl74D6CwiYbXSDwuDe3bm39efxIDunRj//GL+8eFqVDXYxTLGmCZp1QAjIqk+Ty8EqkeYzQEuE5FIEekLDAQWAl8CA92IsQi8gQBz1Pu1/Qi4yB0/FnijNc6htSTHR/HSr0/k/OwjePDdHH47eyn7yiqDXSxjjGm0sEPv4h8ReREYCXQTkTzgdmCkiGQDCqwHfg2gqstF5GXgO6ACmKCqlS6f64C5QCjwjKoudy9xMzBbRO4GlgBPB+pcgiUqPJQpl2aTkRLH3+auYv32IqZdOZTUBBvCbIxp+6SjNb0MGzZMFy1aFOxiNNn7321l4uwlxESGMe2KoQzpZaPMjDGtR0QWq+qwphxjV/K3Ez8+MpnXJ4wgOjyUS6fN5/UlecEukjHGNMgCTDuSnhzHvyaM4Nhenbnhpa/561srqKzqWDVQY0z7YQGmnUmMjeD5q4/nF8f34olP1vKrGYsoLCkPdrGMMeYgFmDaofDQEO65cBB3XXA0H+fkc+Fjn7N+e1Gwi2WMMQewANOOXXFCb56/ejjb95Zy/qP/5fPc7cEukjHG1LAA0879qH833pgwgu5xkVzxzEJmfLHeuyhz82a7v4wxJqgswBwGeneN5Z/X/oiR6Un8+Y3l/N+/vqXyvvvB7i9jjAkiuw7mMFJZpWh0NGFl7vbLS5bAkCHeelQU7NsXvMIZY9o1uw6mgwsNEcLWr2PjeZewr/9AyM6mdEA6Gy+4lJKc3GAXzxjTwViAOdykptKzbypRBdupSOpO+PZ85m0u47inv+V/X1/GV9/vtIkzjTGtImBzkZkgcveXCRs7Fn3uOUZv+IElWcn886s8Zi34nv5JsVw0tCcXDkkjJSEq2KU1xhymrA+mAyksKeftZVt4ZfFGvly/kxCBkwcmcdHQHpxxZDJR4aHBLqIxpo3ypw/GAkwHtX57Ea99lcdri/P4YXcJ8VFhnJd9BBcN7cngHgl4d6U2xhiPBZhGsABzoKoq5fM1O3h18Ube/nYLpRVVDOjeiYuG9uDCIWkkx0d519RcdRU89xykpAS7yMaYILAA0wgWYOq3p6Sct77ZzCuL81i8wWtCOyU9ids/fIre/3kVGTsWJk8OdjGNMUFgAaYRLMA0ztr8vfTq0a3Oa2oqIiL51+e59O4aQ+/EGJLiIq1JzZjDnD8BJpB3tHwGOBfYpqpHu7RE4CWgD94dLS9R1Z3i/To9DIwGioFxqvqVO2Ys8CeX7d2qOt2lDwWeA6KBt4CJ2tGiZQD1S+oE69dRdev/ovO/INRdUzMvsT+3DR/Dtle+rtk3JiKUXokx9EqMoXfXGHp1jaV3Ygx9usZyROcowkJrjYa3JjdjOoRADlN+DvgHMMMn7RbgA1W9T0Rucc9vBs4GBrrleGAqcLwLSLcDw/Bus7xYROao6k63z6+ABXgBZhTwdgDPp+NJTSWkcwJs2wbJyUSWl3PWuaM59W9j2LRrHxt2FLFhRzEbdhTzfUER67YX8XFOPqUVVTVZhIUIaV2ia4JP78RYznr6fnouXEjVX+8j9OEpwTs/Y0xABSzAqOonItKnVvL5wEi3Ph2YhxdgzgdmuBrIfBHpLCKpbt/3VLUAQETeA0aJyDwgXlXnu/QZwAVYgGl5BQUwbhyMHQvTp8P27USEhdC3Wyx9u8UetHtVlbK1sMQLOjuK2VBQxHq3/tRvhhFZUebtuGQJoUOGwCMPUx4RyVNzl5OZEkdGShypCVHW5GbMYaC1L7RMVtXNbn0LkOzW04CNPvvlubSG0vPqSK+TiIwHxgP06tWrGcXvgGb4VEAnTTrk7iEhQmpCNKkJ0ZzQr+uBGy9aT9lNtxD25UJCsrMpH5jOl6kZ3DPiCpa/s7Jmt7ioMDKSvWDjBZ14MpLjSIgJr/tFrcnNmDYpaFfyq6qKSKv0majqNGAaeJ38rfGapg6pqUR07QLbtkJyMuHl5fzonNH8596L2V1czqqthazassc9FjLn6x+YuaCi5vCU+CifoOMt/ZM6EfXAA/tnjrZRbsa0Ga0dYLaKSKqqbnZNYNtc+iagp89+PVzaJvY3qVWnz3PpPerY37R1dTS5ASTEhDO8byLD+ybW7KqqbN5dUhNwVm0pZOWWQr5Ys4OyyipWPvRTonya3BgyBKZMsZmjjWkjWjvAzAHGAve5xzd80q8Tkdl4nfy7XRCaC9wrIl3cfmcCt6pqgYjsEZET8Dr5rwT+3ponYvzUhCY3EeGIztEc0TmaUzO616RXVFaxfkcRn54xnx5/u5t+a74lMjubff0Hsn7gMfScOplOgSq/MabRAjabsoi8CHwBZIhInohcjRdYzhCR1cCP3XPwRoGtBXKBJ4FrAVzn/l3Al265s7rD3+3zlDtmDdbB32GEhYYwoHscZ5w+hKzMnkTuyKcqOZnIgu0s2lnFyTNW8MTHa9hXVhnsohrTodmFlqZ9u/JK6Natpslt18bN/PbsG/gkJ5+kuEiuP20Alx7Xk8gwm8jTmOawK/kbwQJMx7Bg7Q4eejeHhesLSOsczcTTB/LTY9MOvujTGNModkdLY5zj+3XlpV+fwPRfDqdrpwhueu0bzpz8CXO+/oGqqo71T5UxwWIBxhy2RIRT0pN4Y8IInrhiKOGhIfz2xSWMfuRT3vtuq93Z05gAswBjDnsiwllHpfDWxJN5+LJsSsor+dWMRVzw2Od8ujrfAo0xAWIBxnQYoSHC+dlpvPf7U7j/Z4PI31PCFU8v5LJp81m0vmD/jps3w6hRsGVL8AprzGHAAozpcMJDQ7j0uF589MeR3PGTI1mTX8RFj3/BuGcX8u2m3eA7M4Axxm82isx0eMVlFUz/fAO/POPIAybjrL7/jUZFITYzgOng2tT9YIxpL2IiwrhmZH8Kc1az9pob6JO7jGg3M8A7cX158JQr6TT5E28G6SRvFul+bjbpxNiIhmd+tok4TQdmAcYYJ65vL7Iye6ILP0KTk4kqL+eo4f+P0WcMYd32InK2FfL+iq1U+Axzjo8Ko29Sp5qA47vERoYd2NxmE3GaDsaayIzxVWtmALZvP2D+tIrKKvJ27mPd9iLWbi9i3fa9rNtexLr8In7YXVKzX50TcWLNbab9siv5G8ECjAmUfWWVrN/h3dlz66p1HPPYAxy1cQVRq1exb0B6TXNb9/Q+HJOWwKAenTmmRwL9kzoRGmI3WDNtm/XBGBNE0RGhZKXGk5UaD4NS4bM+sPSTmua2wSeewtk/HsI3m3bzyuI8pn+xwTsuPJSj0+IZlOYFnEE9EujbNZaQuoKO9emYdsQCjDGB4u59I665rd/27fzp3CMBqKxS1m3fyzd5u/kmbzfLNu1m1sINPPPfKgA6RYZxdFo8x/TozKC0BI7pkUCvxBjE+nRMO2JNZMa0ERWVVeTme0FnWd5uvtm0mxU/7Kn/5mpYn45pPdYH0wgWYEx7UlZRRc7WQlZ/nUOfSfeSteG7A/p0/nrSFcT27kF6cicykuNIT4kjIzmOPt1iCbeZo00Lsj4YYw4zEWEhHJ2WwNFpx8G7veGrj2v6dI790Uh+ft5wcrYWsmprIe99t5XqEdThoUL/pE5kpMSRnuwFnYyUONI6Rx/Yt2N9OiaAghJgRGQ9UAhUAhWqOkxEEoGXgD7AeuASVd0p3lVsDwOjgWJgnKp+5fIZC/zJZXu3qk5vzfMwplXV6tPpvX07N5yRXrO5pLySNfl7vYCzZS+rtuxh0fqdvLH0h5p9YiJCGZgcR0ZyJ9KT4xj9zAOkLlxIyV33Ev7IlObdL8eClaklKE1kLsAMU9XtPmkPAAWqep+I3AJ0UdWbRWQ0cD1egDkeeFhVj3cBaREwDFBgMTBUVXc29NrWRGY6msKScnK2VgeeQnK2FvLstafUOS1OaVgEo/46l8TYCLrERJAYG05ibCSJseF0iYmga6fqdG/pFBm2fyaDG27wrh0aO9YGIByG2k0fTD0BZhUwUlU3i0gqME9VM0TkCbf+ou9+1Yuq/tqlH7BffSzAGANs3kzJH28mfNFCQleupCIjk9X9jubfl17H9xHx7CwuY8feMnYWl1FQVEZ5Zd2/E+GhwrcPXFhnsKqMjOSLZXl0j4+ke1wkCdHhDU+rU0cZrUbUdrSnPhgF3hURBZ5Q1WlAsqpudtu3AMluPQ3Y6HNsnkurL/0gIjIeGA/Qq1evljoHY9qv1FSiuiXCtm2QnExYeTlZo3uSNW7kQbuqKntLK9hZVE5BcRkFRaUUFJXXPP6tz1v8+IWHyf5hFVE+c7jdO+IK8p9eUJNPRFgISZ0iawJOcnwU3eMi6R4XRZJL6x4XRdfYCK+fqKWHZFvAanXBCjAnqeomEekOvCciK303qqq64NMiXACbBl4NpqXyNaZdc306B0yLUwcRIS4qnLiocHp1jaljj0xY/m9Y9hkkJxNdXs45o89m8G0XsK2w1Fv2lJBfvV5Ywpr8Ir5Ys4M9JRUH5VbnkOwpU6iIiOS5978jITqczjERJESHH7BEhYc0XEOya4haXVACjKpuco/bROR1YDiwVURSfZrItrndNwE9fQ7v4dI24TWT+abPC3DRjTl8+MyxxqRJzcurVrCK2L6dfkmd6JfUqcHDSsorXeApYdseLwA9c8x7nPD0JI76fgWR2dmU9B/IB537c8eJvyD/PyvqzSsiLOSAgNPZPd5/5QmEl5V6O/kErKqoKLZv20lCdDiRYaFNO1+rDTVKq/fBiEgsEKKqhW79PeBO4HRgh08nf6Kq3iQi5wDXsb+T/xFVHe46+RcDx7qsv8Lr5C+o/Zq+rA/GmHbgd7/zAmB4OJSXw9ixVD00icKSCnbvK2f3vnJ27SurWa9ZissPSgvfuoWJHz3HWYXric7NqbmG6N4RV5DfqQvgTddTE5xiwg8OVLXS+t17G/Evz0KvvJKQKVOaf77tIGC1lz6YZOB1V5UNA2ap6jsi8iXwsohcDWwALnH7v4UXXHLxhilfBaCqBSJyF/Cl2+/OQwUXY0w7UUfzXUiIeD/0MeFNzq5q4gLk+UVUuWuIhp98Grf96jQXlPYHql0uQG0sKOZbl1ZcVlmTT+3mOxkyBB5+mNLwCC5/5KOaEXZdYiNI9BltV/28S2z4gSPvqrX1/qbNm8kSyWjqYXYlvzHm8HeI2zA0pKyiqiYAFW3YSPJ9d9Lt2yWErVpJeXomy3tn8dIF17A+PL5m1N3O4vpH3kWEhtDFDfuec9OZRJTXar4DKiIiefmTHCLDQogMDyEyLNRbDwshIsw9Dw9xad56RKj3XERafsj4DTcw7O9/Z1FFRZOm/bYAY4wxTVFH813tH3FVpbC0gp1F+wNOQVG597y4jIK93qP+8AOX/etxRmzPrbf5rinqm7OuNCyCE//8JgDVEWJ/JUoOeO67/ZPbz6kZgj4sK4tF333XpABjU8UYY0xTNGL0nYgQHxVOfFQ4vbvGNpzflg9hxnxwzXfnjj6bkX+9hLLKKkrLqyitqKS0wj2WV7l13/QqSsu99enZH3DSs5MZuO5bIrKzKRuYzoreR/HOmN9yTpduKF6ForpeUV292F/POHD73x57i3Nm/52jNq6AmLpGEDbMAowxxjRFS46+gwMClkyfTvj27XSJjfAzswEw/1X48iNITiaivJzB5/Rm8FUj/S/f8n/Dkk/QNblNbu6yAGOMMcEUwIDV0PVNTc1v1eTJ9Y8Rr0eH64MRGaaZmR9x222xjBlj05kbY0xj+DNMucP9wg4cuJjx48/jppu2MWtWVbCLY4wxh60OF2BEYMiQedx448+569bv4cN7YOks2PAFFG7x7e2q16xZVWRlFRIa6j1aoDLGmIN12D6YQYM+I2djT/j0QVCfABEeA136QJe+kNjXW0/sC4n9IKEns14K5aabtnHjjT9n0KDPWLbsJG666UWguzW5GWOMjw7XB5ORIfrEE7BkyUimTZvDimWRsOt72LkOCtYd+LhzPVSU7D9YQsma9gXj//AbhgyZV5O8ZMlIpk19jRXfhEFknO8A80aZNauKu+4qIicnlvT0IusfMsa0Oe1lqpigUvUCwkMPvcgDD8RCWAh0G+AttVVVwd4tPoFnLTl3DmDQoM8O2G3QoM/IyU2A+xIhPBbikiEuFTq5x7qeR8aDCLNmVVmNyBhzWOpwNZjmjiLLyipk/PjzDq7B/ONFVsx4CQq3QuFm2OseC7dAefHBGYXHQKdksh58jfG//5+D83v8dVasTGhybcgYYwLBajCNMHQoLFoU5/fxt90Wy003vXhAjcOrDXWHERMPPkAVSgt9As5Wr1ZU6C05P/Spu0a0Og7uToaEHtC5JyT0hM69vOcJPb20+DQIPXjiv5Zscmvp5ru23hzY1stnTLuiqh1qGTp0qDbXzJmVmpm5R0NCvMeZMyv9ziszc49OmjRSP/qImmXSpJGa2XeT6tw/qb48VnXaaap/G6h6e3ytJUH1wUzVp85QfeWXqu/drjPvfF/TjsjTSZNG6nvvhemkSSM1Le0HnfnUdtXdP6ju2aJauFV1b77q3u2qRTtUi3eq7tulWrJHtaRQtbRItaxYZ84o1bS0zbXy2uz3+c6cWdmi+VXn2VJ/i5YuX0uWra3n15bL1tL5teWyBUJ1+by7oTTt97bDNZG1tcku6+qDqa4RHfSfc3kJ7NnkDUrYnQe7N8Kuje7xe9iziaypnzD+pusObnJ74B+s+PWPmlS2rCc+rzuvvz3GihvOg9AICIvwHkMjICxy/3rNtsia9azrb2T8xLEH5/f3mayYNQdCQiEkzKuVhYS5JRRCaj1322fN6cZNf43hxj/8Yv979+BMHri5kDE/2QZVFVBV6R7Laz33WSq9x6zLL2T89ZfX3fw55wsIi9q/hFevR0JYtPcYHu2db0hI0/6ujdCW82vLZWuR/FShsgzKipg1S7npjjJuvHGMT16zeOCOMMb8XNxnI9r7nAbhXKvzbMkWjOryTZ06j5wctdmUG9LWAgy04AeiqpLQcGHu3EjCwvbfiraiIoyzziqhcsF078uiVe56H591rTroeejIicydG1V3Xv+60fvSVZR6j9VLRRlUlvqsl9XsF3rzSua+W0d+Z5ZQ+efEJp9uvQHQj2AKEHpnQcuULzSSrKkfM/6P1x5ctoeeYMUtv6BmztqaPraGn2fd8xzjbxx/cH6TnmTFHRO8HzQJ2b8c8DzUy6cmLZSsG29j/O+uOji/Kc+x4u8P1np9ObBc1Wlue9a119f9j8PD01kx9bF63qT6f3eyrplQT34zWDFtmjsH2X9++KyL7N/m0rOuvLjufxwefo4Vk/8KZUVQvs8txe6xVpq7lKHRn7mQcC/QhEfvDzp1PGb94fa6/w5/f4EVL/zT/XMV6vNPVziEhu1fDwk74PmsfyfV/U/X/5Yy5sJdPt97Pfg3oCa9qmY966wjGX/tJQwZMo9f/xpWrWpagOlwfTBt0ZgxIYwZU90v5H//ECGhpKcXsmzZSQd8YJctO4n09GIY9ssmZddgXj+Z0uTipT9TX3574aZ1rnZR7lO7qITK8oNrHG7JuSur7v6rbVkw9s1atSDfmlGoz7b9S/rsvXWXb+AeuHa+92NTUQoV7rHmecn+pdx7zLkjve6ybe4P3Y90KdVT2mo9z6l5nrO5X935/dDXqymqeu+XloNWuh+NKpem+9OqvMecjT3rzm9jD8h5d395qn94asqlB5ZXlZzv76s7r+/TvIuY61PPT1XO9/fWk98RsPDJWv8MVR1YxrryW399Pfn1gA3/9QbchEd7jzGJ7rlPWnWgiIgl567Mej5zmXD2A+4zUdLwY9leKNoOFfvq/zusT4F3/1T/e1ePu574nBtv+lXNZ3jIkHnc+IdfcNfd/2BM/sgm55ezpuCg8jVFh6vBiEghsKqFsusGNHMmuZbOr1tiaGhq79TUdSHR0XvZt68Tmzf3raqs3LwBtjfxjp8tmVfL5ydy1KC0tA0RMTF7a9KKizuxaVPvMtXly4JZvpYuW1vOry2XraXza8tl8wwdOnDg4gMGn6rC6tVDgcWLm1O+LVtg926rwRzKKm3iULv6iMiilsorcPkVtuC5tkxegcuvLb93bf1z0pG+E3au/ubX1GNs/KUxxpiAsABjjDEmIDpigJnWRvNq6/m15bK19fzactlaOr+2XLaWzq8tl61N5NfhOvmNMca0jg7VyS8iUXi1thJV3zn6m5yPAKF4Abq8hcoWhvf3aIny+eZVqqqVLVTGaKBKVUubmU91+arfw72HOORQ+YUDkXgDX4ta6L0LdfmVNudv7D4rkS6/clUta4G8/P6M1PfZFZGuQJTLd4e/ebnvWGe3y7amlLGe/DoBnfDOeZ+q7mxmfnFAjEsvU9VGjdps6DvvythZVfOaWbYYvOsUQvHOd7uqltSfy6HL576zXfE+y3sb8/7VU7bO7P8ch+F9ljcfsmBNvfS/PS7uTbkK2AgsBa5vRl4hwBvAbuD96rRmlm8IsAj4FvgS+FUz8uoFfAd8BSwG/tBC72FPYBPwrHsufuZzhHvvvgSWAa83s1wDgPeBHOATIL0ZeZ3k8lkALHflHN2M/GKAvwIr3efu8Wa8b9HAPXhD7BcA/+NHHgd9dl36Me7zsgz4J5DQxLw+cGlRwG/c52QP0MfPslXn1xl4FvjGnfNMvB9yf/NLAB5x34sFwKtAmj/vW/V33p3zTLzg16jvRT1liweeBLYAXwDv4M3N0py/axww2X2W/wv8zs+ydQPedOWaD2wF3vN9H+rNz98vT3ta8H4cc/CieJL7gPXyMy8BTgBOBb5o7IfqEHn2ALLcenWASGlG+SLcehe8wNW/meWLBv4CvAA815xzBtKAFS30d+0MvIxPEABiWyjvFGAtENqMPI6rPlf3Q/Qv4Kd+5nUGsMCtdwXeBo7w47NR/dmd7/MZeRq4yD2/GXjoUH/jer4HEe7zmwm8BfTzs2zV+UUDJ/jscx/w18Z8/urJL8T37wn8EZjq57lWdy+cCHyE9+PbqM9KPfnFA/cCl/jx2agrv3BgLPCCz36R/uRVxz5TgZ83pmwdpZP/GGCZeu9OMfA68DN/MlLPfKAIr2bUbKqap6or3NNCvB+2pGaUr8w1Gx0BlAJ+N8s4mcBQvP8mY5uZVwUQLiKxItJJRJrzHnYHjlbVt0QkXkSiVLWomeXDleksYLk2r3mxAghxzUZpeD9wuX7m1RPvv26AfXg1oouakkE9n90oYATwmnv+L2C0n3lVqOr3wDa84NDoi/Lqya/EpVX/TdbiPn/uu9zU/BSoEpEEEcnE+37kHCq/uvJSVRWR7sCNwAS8//Ibdb618qvupqhy6/1EpLtrsmwUn/z2+uQXD/wUmCwiR4jIEdqIpu26zlUct94H+Aneb+ghdZQA0w2v2g7ej20+3oerOSoIzPt3NNBXVf24itfj2ktXAPOAN1V1YzPyigf+F+9LFI33RWiOErwftQ+Bd/GaLv2VAhSLyHN45/qQa3Nurii8H9nnm5nPMrxa307gY2Cjqn7jZ14rgVPd+aUCo/DznxAO/OwqkOjzA7sTr4bkT17Vqj8j/nw/6vtedQfGALP9zc+dYwTwZ7wa4FDgCX/yEpEI4Dy8z10OXo2mov5D681PfNbLgCvwmqgeEJHUJuZXyYF/1/54/0i/CjwtIqc3sWw1fwcXUKtrN9+oakl10GlIRwkwVRx4rm4yo2Zp8QAjIr2BvwO/ak4+qrpLVQcAGcDpIpLejOxGAXnA93gBJlpEwg71H2QD9gBDVPV4vP/ArxWR4/zMKxwYBNyhqsfi/U1u9TMvX3HASLz+iOY4Avh/qhoN9AOOEJExfuY1H3gUry19Kl7A8vfz5/vDVrt2W07T/omoHazA+6ET/BtEdND3ynWi3wEsUdXP/c1PRERVS1X1RlXtC7wC3O1n2ZKBiXj/QGQBESJypIg05W/im18Z8ICqHqWqJwLrgYeakFd1fr4BKxxv0MuPgNuA5/wsW7VI4EJgBhy6FkkdGRyu8oC+br0K6ANsaGaeNSM1mvFjW0NEEvCaoB7w40tUn914bcOnNSOPE/D+C8oFHsb74X3Y38xcFTzfPd2C11Y/xM/s1gPfq+p69/xNvOZQv7n/yk4HPvPjP9La/h/7m8QqgP8Aw/3JSFWrVPVxVT1WVc90yd/6Wa5y9v8QlQJbRaS6NtQTbzBMYz/XdX0PivGCjD+jDQ/Iz43qm4DXuX9Dc/Orte0DvL6txvL9PCTjNdd9gdcPmIL3A97Jz7JVqeoen23/BI48+JBDls/3H4ccvAEw4H1Wyl1zbVPLVv2+xQKn4DWjNkpHCTALgXRXQ0gCzsGrhjZHKN5/LeGuuuw311/yNF6H/MuujfjgW1U2Lq8kEenmjk/B66z72t+yqervVbWXqvbHa6L4QFUn+JufiMSISBdXvjS8H/Olfma3EdgkIhmuKe8s9vdT+CsMuBhoYBrgRtsAjHDDTxPwfsz8vleEiKSKSKSIDMEL9C/7mVUoXj9YKF6t43XgOhFJw2uybErToO/3INL9XXvgnW+miPTwo2wRIhLmfgyvAH4N3CAifUQkuTn5ichQ9/3oAfyC/T/AjRHi8goHlqpqP1XNUtUsYIuqDq8VJJpatnSfsl0BvNeEvKrLF+6CsuAFqZ+6vqLTgB3ayGHP1P37NhpY2IQ8OsYoMheAz8Yb4rkCuKaZeb2M16dT3SF/eTPzG4n338dCvOGiC4BT/cwrG1jis/y2Bd/DU4EXm5nHQLxhp1/hBZYJzcxvMN6Q56/wqu4Rzcyvs/vbhrfQe3YH3qjApcDfm5FPCF7z2Lfu83Gsn/nU/uz+Aq9JcI77uzxP40dD1c7rCrwmlHV4wXUlMNvPsq0BxrnzXYTXRPgZXg3f3/zG4vXhLHb5PQVE+3mul/tsi3RlS2hm2Z71OdcnacSor0P8XQWvj2kRXiDNbGZeU4GfNeXzZlfyG2OMCYiO0kRmjDGmlVmAMcYYExAWYIwxxgSEBRhjjDEBYQHGGGNMQFiAMaadEJGRIvJmsMthTGNZgDHGGBMQFmCMaWEicrmILBSRpSLyhIiEisheEZksIstF5IPqqVlEJFtE5ovINyLyuoh0cekDROR9EflaRL4Skf4u+04i8qqIrBSRmY2ZcNCYYLEAY0wLEpEs4FJghKpm483J9Qu8eZwWqepReBNV3u4OmQHcrKrH4M2+XJ0+E3hUVQcDPwKq7x44BPgd3jxV/fCm2jemTepQt0w2phWcjjcN/JeuchGNd3+UKuAlt88LwD/dBKedVfVjlz4deEW82/qmqerrAOrmfnL5LVR3a14RWYo3cetnAT8rY/xgAcaYliXAdFU94LYBInJbrf38naPJd4biSuw7bNowayIzpmV9AFzkZrBFRBLdLN4h7L8D5Ri82wHsBnaKyMku/QrgY1UtBPJE5AKXR6SbkdmYdsX++zGmBanqdyLyJ+Bdd/Opcrz7mRQBw922bXj9NODNovu4CyBr2X+HzyuAJ0TkTpfHxa14Gsa0CJtN2ZhWICJ7VbUpN6Mypt2zJjJjjDEBYTUYY4wxAWE1GGOMMQFhAcYYY0xAWIAxxhgTEBZgjDHGBIQFGGOMMQFhAcYYY0xA/H8ymmXs4vQ0AAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    " \n",
    "names = range(0,18)\n",
    "names = [str(x) for x in list(names)]\n",
    " \n",
    "x = range(len(names))\n",
    "#plt.plot(x, y, 'ro-')\n",
    "#plt.plot(x, y1, 'bo-')\n",
    "#pl.xlim(-1, 11)  # 限定横轴的范围\n",
    "#pl.ylim(-1, 110)  # 限定纵轴的范围\n",
    " \n",
    "plt.plot(x, train_loss, marker='*', mec='r', mfc='w',label='train_loss')\n",
    "plt.plot(x, dev_loss, marker='o', mec='b', mfc='y',label='dev_loss')\n",
    "plt.legend()  # 让图例生效\n",
    "plt.xticks(x, names, rotation=1)\n",
    " \n",
    "plt.margins(0)\n",
    "plt.subplots_adjust(bottom=0.10)\n",
    "plt.xlabel('epoch') #X轴标签\n",
    "plt.ylabel(\"loss\") #Y轴标签\n",
    "#pyplot.yticks([0.750,0.800,0.850])\n",
    "plt.title(\"Lattice LSTM 训练曲线\")\n",
    "#plt.savefig('D:\\\\f1.jpg',dpi = 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "29c71b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('../cyx＆xy/lattice/cyx/final/test.txt','r',encoding='utf-8')\n",
    "test=f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e6867d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=[]\n",
    "test_str=[]\n",
    "tt=[]\n",
    "tt2=[]\n",
    "for i in test:\n",
    "    temp=i.split()\n",
    "    if len(temp)==0:\n",
    "        if len(tt)>600:\n",
    "            pass\n",
    "        else:\n",
    "            test_data.append(tt)\n",
    "            test_str.append(tt2)\n",
    "        tt=[]\n",
    "        tt2=[]\n",
    "    else:\n",
    "        tt.append(temp[-1])\n",
    "        tt2.append(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "54aa9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre=[]\n",
    "for i in _:\n",
    "    pre.extend(i)\n",
    "\n",
    "gold=[]\n",
    "for i in test_data:\n",
    "    gold.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fa1be4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "48c0999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-MED     0.9797    0.9311    0.9548       363\n",
      "       B-SYM     0.9175    0.9345    0.9259     23787\n",
      "       E-MED     0.9943    0.9559    0.9747       363\n",
      "       E-SYM     0.9262    0.9422    0.9341     23227\n",
      "       I-MED     0.9963    0.9182    0.9557       587\n",
      "       I-SYM     0.9131    0.9227    0.9179     32066\n",
      "           O     0.9925    0.9864    0.9895    180150\n",
      "\n",
      "    accuracy                         0.9696    260543\n",
      "   macro avg     0.9599    0.9416    0.9504    260543\n",
      "weighted avg     0.9700    0.9696    0.9698    260543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(gold, pre, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6888d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
