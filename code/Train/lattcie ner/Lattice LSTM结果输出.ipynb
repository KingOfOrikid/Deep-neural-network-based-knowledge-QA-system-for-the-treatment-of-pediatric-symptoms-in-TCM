{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lattice LSTM结果输出.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Bn-edxP_VExJR6SGfOwEn7JrkGbSSg-q","authorship_tag":"ABX9TyO0aAjPXhGrj/gPX7efib10"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import sys\n","from google.colab import drive\n","drive.mount('/content/drive')\n","sys.path.append('/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER')\n","\n","sys.argv=['']\n","del sys"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3CPtTaOdGD3U","executionInfo":{"status":"ok","timestamp":1648207735134,"user_tz":-480,"elapsed":16448,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15214000089252266369"}},"outputId":"e6bb310c-2338-4eb5-a716-9b51931b5dc7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import time\n","import sys\n","import argparse\n","import random\n","import copy\n","import torch\n","import gc\n","import pickle\n","import torch.autograd as autograd\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from utils.metric import get_ner_fmeasure\n","from model.bilstmcrf import BiLSTM_CRF as SeqModel\n","from utils.data import Data"],"metadata":{"id":"PIVMeYU9F3vD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(data, model, name):\n","    if name == \"train\":\n","        instances = data.train_Ids\n","    elif name == \"dev\":\n","        instances = data.dev_Ids\n","    elif name == 'test':\n","        instances = data.test_Ids\n","    elif name == 'raw':\n","        instances = data.raw_Ids\n","    else:\n","        print (\"Error: wrong evaluate name,\", name)\n","    pred_results = []\n","    gold_results = []\n","    ## set model in eval model\n","    model.eval()\n","    batch_size = 10\n","    start_time = time.time()\n","    train_num = len(instances)\n","    total_batch = train_num//batch_size+1\n","    for batch_id in range(total_batch):\n","        start = batch_id*batch_size\n","        end = (batch_id+1)*batch_size \n","        if end >train_num:\n","            end =  train_num\n","        instance = instances[start:end]\n","        if not instance:\n","            continue\n","        gaz_list,batch_word, batch_biword, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu, True)\n","        tag_seq = model(gaz_list,batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask)\n","        # print \"tag:\",tag_seq\n","        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet, batch_wordrecover)\n","        pred_results += pred_label\n","        gold_results += gold_label\n","    decode_time = time.time() - start_time\n","    speed = len(instances)/decode_time\n","    acc, p, r, f = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)\n","    return speed, acc, p, r, f, pred_results  \n","\n","def batchify_with_label(input_batch_list, gpu, volatile_flag=False):\n","    \"\"\"\n","        input: list of words, chars and labels, various length. [[words,biwords,chars,gaz, labels],[words,biwords,chars,labels],...]\n","            words: word ids for one sentence. (batch_size, sent_len) \n","            chars: char ids for on sentences, various length. (batch_size, sent_len, each_word_length)\n","        output:\n","            zero padding for word and char, with their batch length\n","            word_seq_tensor: (batch_size, max_sent_len) Variable\n","            word_seq_lengths: (batch_size,1) Tensor\n","            char_seq_tensor: (batch_size*max_sent_len, max_word_len) Variable\n","            char_seq_lengths: (batch_size*max_sent_len,1) Tensor\n","            char_seq_recover: (batch_size*max_sent_len,1)  recover char sequence order \n","            label_seq_tensor: (batch_size, max_sent_len)\n","            mask: (batch_size, max_sent_len) \n","    \"\"\"\n","    batch_size = len(input_batch_list)\n","    words = [sent[0] for sent in input_batch_list]\n","    biwords = [sent[1] for sent in input_batch_list]\n","    chars = [sent[2] for sent in input_batch_list]\n","\n","\n","    gazs = [sent[3] for sent in input_batch_list]\n","    labels = [sent[4] for sent in input_batch_list]\n","    word_seq_lengths = torch.LongTensor(list(map(len, words)))\n","    max_seq_len = word_seq_lengths.max().item()\n","\n","    word_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile =  volatile_flag).long()\n","    biword_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile =  volatile_flag).long()\n","    label_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).long()\n","    mask = autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).byte()\n","\n","    for idx, (seq, biseq, label, seqlen) in enumerate(zip(words, biwords, labels, word_seq_lengths)):\n","        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n","        biword_seq_tensor[idx, :seqlen] = torch.LongTensor(biseq)\n","        label_seq_tensor[idx, :seqlen] = torch.LongTensor(label)\n","        mask[idx, :seqlen] = torch.Tensor([1]*seqlen.item())\n","\n","    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n","    word_seq_tensor = word_seq_tensor[word_perm_idx]\n","    biword_seq_tensor = biword_seq_tensor[word_perm_idx]\n","    label_seq_tensor = label_seq_tensor[word_perm_idx]\n","    mask = mask[word_perm_idx]\n","\n","    ### deal with char\n","    # pad_chars (batch_size, max_seq_len)\n","    pad_chars = [chars[idx] + [[0]] * (max_seq_len-len(chars[idx])) for idx in range(len(chars))]\n","    length_list = [list(map(len, pad_char)) for pad_char in pad_chars]\n","    #length_list = [len(pad_char) for pad_char in pad_chars]\n","    max_word_len = max(map(max, length_list))\n","    char_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len, max_word_len)), volatile =  volatile_flag).long()\n","    char_seq_lengths = torch.LongTensor(length_list)\n","    for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):\n","        for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):\n","            # print len(word), wordlen\n","            char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)\n","    char_seq_tensor = char_seq_tensor[word_perm_idx].view(batch_size*max_seq_len,-1)\n","    char_seq_lengths = char_seq_lengths[word_perm_idx].view(batch_size*max_seq_len,)\n","    char_seq_lengths, char_perm_idx = char_seq_lengths.sort(0, descending=True)\n","    char_seq_tensor = char_seq_tensor[char_perm_idx]\n","    _, char_seq_recover = char_perm_idx.sort(0, descending=False)\n","    _, word_seq_recover = word_perm_idx.sort(0, descending=False)\n","    \n","    ## keep the gaz_list in orignial order\n","    \n","    gaz_list = [ gazs[i] for i in word_perm_idx]\n","    gaz_list.append(volatile_flag)\n","    if gpu:\n","        word_seq_tensor = word_seq_tensor.cuda()\n","        biword_seq_tensor = biword_seq_tensor.cuda()\n","        word_seq_lengths = word_seq_lengths.cuda()\n","        word_seq_recover = word_seq_recover.cuda()\n","        label_seq_tensor = label_seq_tensor.cuda()\n","        char_seq_tensor = char_seq_tensor.cuda()\n","        char_seq_recover = char_seq_recover.cuda()\n","        mask = mask.cuda()\n","    return gaz_list, word_seq_tensor, biword_seq_tensor, word_seq_lengths, word_seq_recover, char_seq_tensor, char_seq_lengths, char_seq_recover, label_seq_tensor, mask\n","\n","def recover_label(pred_variable, gold_variable, mask_variable, label_alphabet, word_recover):\n","    \"\"\"\n","        input:\n","            pred_variable (batch_size, sent_len): pred tag result\n","            gold_variable (batch_size, sent_len): gold result variable\n","            mask_variable (batch_size, sent_len): mask variable\n","    \"\"\"\n","    \n","    pred_variable = pred_variable[word_recover]\n","    gold_variable = gold_variable[word_recover]\n","    mask_variable = mask_variable[word_recover]\n","    batch_size = gold_variable.size(0)\n","    seq_len = gold_variable.size(1)\n","    mask = mask_variable.cpu().data.numpy()\n","    pred_tag = pred_variable.cpu().data.numpy()\n","    gold_tag = gold_variable.cpu().data.numpy()\n","    batch_size = mask.shape[0]\n","    pred_label = []\n","    gold_label = []\n","    for idx in range(batch_size):\n","        pred = [label_alphabet.get_instance(pred_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n","        gold = [label_alphabet.get_instance(gold_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n","        # print \"p:\",pred, pred_tag.tolist()\n","        # print \"g:\", gold, gold_tag.tolist()\n","        assert(len(pred)==len(gold))\n","        pred_label.append(pred)\n","        gold_label.append(gold)\n","    return pred_label, gold_label\n","\n","def predict(data, model, name):\n","    if name == \"train\":\n","        instances = data.train_Ids\n","    elif name == \"dev\":\n","        instances = data.dev_Ids\n","    elif name == 'test':\n","        instances = data.test_Ids\n","    elif name == 'raw':\n","        instances = data.raw_Ids\n","    else:\n","        print (\"Error: wrong evaluate name,\", name)\n","    pred_results = []\n","    gold_results = []\n","    ## set model in eval model\n","    model.eval()\n","    batch_size = 10\n","    start_time = time.time()\n","    train_num = len(instances)\n","    total_batch = train_num//batch_size+1\n","    for batch_id in range(total_batch):\n","        start = batch_id*batch_size\n","        end = (batch_id+1)*batch_size \n","        if end >train_num:\n","            end =  train_num\n","        instance = instances[start:end]\n","        if not instance:\n","            continue\n","        gaz_list,batch_word, batch_biword, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu, True)\n","        tag_seq = model(gaz_list,batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask)\n","        # print \"tag:\",tag_seq\n","        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet, batch_wordrecover)\n","        pred_results += pred_label\n","        gold_results += gold_label\n","    #decode_time = time.time() - start_time\n","    #speed = len(instances)/decode_time\n","    #acc, p, r, f = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)\n","    return pred_results\n","\n","def load_model_decode_with_model_predict(model, data, name, gpu, seg=True):\n","    data.HP_gpu = gpu\n","    \n","    print(\"Decode %s data ...\"%(name))\n","    start_time = time.time()\n","    pred_results = predict(data, model, name)\n","    end_time = time.time()\n","    time_cost = end_time - start_time\n","    return pred_results\n"],"metadata":{"id":"gDTlGTzSF0xs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3dM-JNjPFYtF"},"outputs":[],"source":["model_test=(torch.load(\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/321final.model\"))\n","with open(\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/model-train-data_final.dset\", 'rb') as fp:\n","  data = pickle.load(fp)\n","gpu = torch.cuda.is_available()"]},{"cell_type":"code","source":["ques=input()\n","f=open('/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/ques.txt','w',encoding='utf-8')\n","for i in ques:\n","  f.write(i+'\t'+'O'+'\\n')\n","f.write('\\n')\n","f.close()\n","data.generate_instance_with_gaz('/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/final/ques.txt','test')\n","load_model_decode_with_model_predict(model_test, data, 'test', gpu, True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04DG0vW9MKRC","executionInfo":{"status":"ok","timestamp":1648207808988,"user_tz":-480,"elapsed":28032,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15214000089252266369"}},"outputId":"25912c9e-fe85-415d-f9e6-bb0b84fea5f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["伤寒怎么能治好，升麻汤有用吗？\n","Decode test data ...\n"]},{"output_type":"stream","name":"stderr","text":["__main__:65: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:66: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:67: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:68: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:88: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:263: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  word_var = autograd.Variable(torch.LongTensor(skip_input_[t][0]),volatile =  volatile_flag)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:156: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n","  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"]},{"output_type":"execute_result","data":{"text/plain":["[['B-SYM',\n","  'E-SYM',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'B-MED',\n","  'I-MED',\n","  'E-MED',\n","  'O',\n","  'O',\n","  'O',\n","  'O']]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[""],"metadata":{"id":"LnsdkYnwFray"},"execution_count":null,"outputs":[]}]}