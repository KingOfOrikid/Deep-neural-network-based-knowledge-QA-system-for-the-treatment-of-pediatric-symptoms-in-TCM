{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpc1JwvTRFm5","executionInfo":{"status":"ok","timestamp":1646046836395,"user_tz":-480,"elapsed":13322,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"77bf49ed-16a6-4dc9-f015-b21dbc45b590"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import sys\n","from google.colab import drive\n","drive.mount('/content/drive')\n","sys.path.append('/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER')\n","\n","sys.argv=['']\n","del sys"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"MS5oOoYXsPkO","executionInfo":{"status":"ok","timestamp":1646046849223,"user_tz":-480,"elapsed":9819,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}}},"outputs":[],"source":["import time\n","import sys\n","import argparse\n","import random\n","import copy\n","import torch\n","import gc\n","import pickle\n","import torch.autograd as autograd\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from utils.metric import get_ner_fmeasure\n","from model.bilstmcrf import BiLSTM_CRF as SeqModel\n","from utils.data import Data\n","\n","seed_num = 100\n","random.seed(seed_num)\n","torch.manual_seed(seed_num)\n","np.random.seed(seed_num)\n","\n","\n","def data_initialization(data, gaz_file, train_file, dev_file, test_file):\n","    data.build_alphabet(train_file)\n","    data.build_alphabet(dev_file)\n","    data.build_alphabet(test_file)\n","    data.build_gaz_file(gaz_file)\n","\n","    #gaz_alphabet train,dev,test file在embedding中匹配到的词语\n","    data.build_gaz_alphabet(train_file)\n","    data.build_gaz_alphabet(dev_file)\n","    data.build_gaz_alphabet(test_file)\n","    data.fix_alphabet()\n","    return data\n","\n","\n","def predict_check(pred_variable, gold_variable, mask_variable):\n","    \"\"\"\n","        input:\n","            pred_variable (batch_size, sent_len): pred tag result, in numpy format\n","            gold_variable (batch_size, sent_len): gold  result variable\n","            mask_variable (batch_size, sent_len): mask variable\n","    \"\"\"\n","    pred = pred_variable.cpu().data.numpy()\n","    gold = gold_variable.cpu().data.numpy()\n","    mask = mask_variable.cpu().data.numpy()\n","    overlaped = (pred == gold)\n","    right_token = np.sum(overlaped * mask)\n","    total_token = mask.sum()\n","    # print(\"right: %s, total: %s\"%(right_token, total_token))\n","    return right_token, total_token\n","\n","\n","def recover_label(pred_variable, gold_variable, mask_variable, label_alphabet, word_recover):\n","    \"\"\"\n","        input:\n","            pred_variable (batch_size, sent_len): pred tag result\n","            gold_variable (batch_size, sent_len): gold result variable\n","            mask_variable (batch_size, sent_len): mask variable\n","    \"\"\"\n","    \n","    pred_variable = pred_variable[word_recover]\n","    gold_variable = gold_variable[word_recover]\n","    mask_variable = mask_variable[word_recover]\n","    batch_size = gold_variable.size(0)\n","    seq_len = gold_variable.size(1)\n","    mask = mask_variable.cpu().data.numpy()\n","    pred_tag = pred_variable.cpu().data.numpy()\n","    gold_tag = gold_variable.cpu().data.numpy()\n","    batch_size = mask.shape[0]\n","    pred_label = []\n","    gold_label = []\n","    for idx in range(batch_size):\n","        pred = [label_alphabet.get_instance(pred_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n","        gold = [label_alphabet.get_instance(gold_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n","        # print \"p:\",pred, pred_tag.tolist()\n","        # print \"g:\", gold, gold_tag.tolist()\n","        assert(len(pred)==len(gold))\n","        pred_label.append(pred)\n","        gold_label.append(gold)\n","    return pred_label, gold_label\n","\n","\n","def save_data_setting(data, save_file):\n","    \"\"\"\n","    new_data = copy.deepcopy(data)\n","    ## remove input instances\n","    new_data.train_texts = []\n","    new_data.dev_texts = []\n","    new_data.test_texts = []\n","    new_data.raw_texts = []\n","    new_data.train_Ids = []\n","    new_data.dev_Ids = []\n","    new_data.test_Ids = []\n","    new_data.raw_Ids = []\"\"\"\n","    ## save data settings\n","    with open(save_file, 'wb') as fp:\n","        pickle.dump(data, fp)\n","    print (\"Data setting saved to file: \", save_file)\n","\n","\n","def load_data_setting(save_file):\n","    with open(save_file, 'rb') as fp:\n","        data = pickle.load(fp)\n","    print (\"Data setting loaded from file: \", save_file)\n","    data.show_data_summary()\n","    return data\n","\n","def lr_decay(optimizer, epoch, decay_rate, init_lr):\n","    lr = init_lr * ((1-decay_rate)**epoch)\n","    print (\" Learning rate is setted as:\", lr)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","    return optimizer\n","\n","\n","\n","def evaluate(data, model, name):\n","    if name == \"train\":\n","        instances = data.train_Ids\n","    elif name == \"dev\":\n","        instances = data.dev_Ids\n","    elif name == 'test':\n","        instances = data.test_Ids\n","    elif name == 'raw':\n","        instances = data.raw_Ids\n","    else:\n","        print (\"Error: wrong evaluate name,\", name)\n","    pred_results = []\n","    gold_results = []\n","    ## set model in eval model\n","    model.eval()\n","    batch_size = 10\n","    start_time = time.time()\n","    train_num = len(instances)\n","    total_batch = train_num//batch_size+1\n","    for batch_id in range(total_batch):\n","        start = batch_id*batch_size\n","        end = (batch_id+1)*batch_size \n","        if end >train_num:\n","            end =  train_num\n","        instance = instances[start:end]\n","        if not instance:\n","            continue\n","        gaz_list,batch_word, batch_biword, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu, True)\n","        tag_seq = model(gaz_list,batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask)\n","        # print \"tag:\",tag_seq\n","        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet, batch_wordrecover)\n","        pred_results += pred_label\n","        gold_results += gold_label\n","    decode_time = time.time() - start_time\n","    speed = len(instances)/decode_time\n","    acc, p, r, f = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)\n","    return speed, acc, p, r, f, pred_results  \n","\n","\n","def batchify_with_label(input_batch_list, gpu, volatile_flag=False):\n","    \"\"\"\n","        input: list of words, chars and labels, various length. [[words,biwords,chars,gaz, labels],[words,biwords,chars,labels],...]\n","            words: word ids for one sentence. (batch_size, sent_len) \n","            chars: char ids for on sentences, various length. (batch_size, sent_len, each_word_length)\n","        output:\n","            zero padding for word and char, with their batch length\n","            word_seq_tensor: (batch_size, max_sent_len) Variable\n","            word_seq_lengths: (batch_size,1) Tensor\n","            char_seq_tensor: (batch_size*max_sent_len, max_word_len) Variable\n","            char_seq_lengths: (batch_size*max_sent_len,1) Tensor\n","            char_seq_recover: (batch_size*max_sent_len,1)  recover char sequence order \n","            label_seq_tensor: (batch_size, max_sent_len)\n","            mask: (batch_size, max_sent_len) \n","    \"\"\"\n","    batch_size = len(input_batch_list)\n","    words = [sent[0] for sent in input_batch_list]\n","    biwords = [sent[1] for sent in input_batch_list]\n","    chars = [sent[2] for sent in input_batch_list]\n","\n","\n","    gazs = [sent[3] for sent in input_batch_list]\n","    labels = [sent[4] for sent in input_batch_list]\n","    word_seq_lengths = torch.LongTensor(list(map(len, words)))\n","    max_seq_len = word_seq_lengths.max().item()\n","\n","    word_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile =  volatile_flag).long()\n","    biword_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile =  volatile_flag).long()\n","    label_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).long()\n","    mask = autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).byte()\n","\n","    for idx, (seq, biseq, label, seqlen) in enumerate(zip(words, biwords, labels, word_seq_lengths)):\n","        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n","        biword_seq_tensor[idx, :seqlen] = torch.LongTensor(biseq)\n","        label_seq_tensor[idx, :seqlen] = torch.LongTensor(label)\n","        mask[idx, :seqlen] = torch.Tensor([1]*seqlen.item())\n","\n","    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n","    word_seq_tensor = word_seq_tensor[word_perm_idx]\n","    biword_seq_tensor = biword_seq_tensor[word_perm_idx]\n","    label_seq_tensor = label_seq_tensor[word_perm_idx]\n","    mask = mask[word_perm_idx]\n","\n","    ### deal with char\n","    # pad_chars (batch_size, max_seq_len)\n","    pad_chars = [chars[idx] + [[0]] * (max_seq_len-len(chars[idx])) for idx in range(len(chars))]\n","    length_list = [list(map(len, pad_char)) for pad_char in pad_chars]\n","    #length_list = [len(pad_char) for pad_char in pad_chars]\n","    max_word_len = max(map(max, length_list))\n","    char_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len, max_word_len)), volatile =  volatile_flag).long()\n","    char_seq_lengths = torch.LongTensor(length_list)\n","    for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):\n","        for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):\n","            # print len(word), wordlen\n","            char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)\n","    char_seq_tensor = char_seq_tensor[word_perm_idx].view(batch_size*max_seq_len,-1)\n","    char_seq_lengths = char_seq_lengths[word_perm_idx].view(batch_size*max_seq_len,)\n","    char_seq_lengths, char_perm_idx = char_seq_lengths.sort(0, descending=True)\n","    char_seq_tensor = char_seq_tensor[char_perm_idx]\n","    _, char_seq_recover = char_perm_idx.sort(0, descending=False)\n","    _, word_seq_recover = word_perm_idx.sort(0, descending=False)\n","    \n","    ## keep the gaz_list in orignial order\n","    \n","    gaz_list = [ gazs[i] for i in word_perm_idx]\n","    gaz_list.append(volatile_flag)\n","    if gpu:\n","        word_seq_tensor = word_seq_tensor.cuda()\n","        biword_seq_tensor = biword_seq_tensor.cuda()\n","        word_seq_lengths = word_seq_lengths.cuda()\n","        word_seq_recover = word_seq_recover.cuda()\n","        label_seq_tensor = label_seq_tensor.cuda()\n","        char_seq_tensor = char_seq_tensor.cuda()\n","        char_seq_recover = char_seq_recover.cuda()\n","        mask = mask.cuda()\n","    return gaz_list, word_seq_tensor, biword_seq_tensor, word_seq_lengths, word_seq_recover, char_seq_tensor, char_seq_lengths, char_seq_recover, label_seq_tensor, mask\n","\n","\n","def train(data, save_model_dir,save_data_set, seg=True):\n","    print (\"Training model...\")\n","    data.show_data_summary()\n","    #save_data_name = save_data_set\n","    #save_data_setting(data, save_data_name)\n","\n","    save_data_name = save_data_set\n","    save_data_setting(data, save_data_name)\n","\n","    model = SeqModel(data)\n","    print (\"finished built model.\")\n","    loss_function = nn.NLLLoss()\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optimizer = optim.SGD(parameters, lr=data.HP_lr, momentum=data.HP_momentum)\n","    best_dev = -1\n","    #data.HP_iteration = 100#epoch次数\n","    data.HP_iteration = 5#epoch次数\n","    ## start training data.HP_iteration\n","    for idx in range(data.HP_iteration):\n","        epoch_start = time.time()\n","        temp_start = epoch_start\n","        print(\"Epoch: %s/%s\" %(idx,data.HP_iteration))\n","        optimizer = lr_decay(optimizer, idx, data.HP_lr_decay, data.HP_lr)\n","        instance_count = 0\n","        sample_id = 0\n","        sample_loss = 0\n","        batch_loss = 0\n","        total_loss = 0\n","        right_token = 0\n","        whole_token = 0\n","        random.shuffle(data.train_Ids)\n","        ## set model in train model\n","        model.train()\n","        model.zero_grad()\n","        batch_size = 10 ## current only support batch size = 1 to compulate and accumulate to data.HP_batch_size update weights\n","        train_num = len(data.train_Ids)\n","        total_batch = train_num//batch_size+1\n","        for batch_id in range(total_batch):\n","            start = batch_id*batch_size\n","            end = (batch_id+1)*batch_size \n","            if end >train_num:\n","                end = train_num\n","            instance = data.train_Ids[start:end]\n","            if not instance:\n","                continue\n","            gaz_list,  batch_word, batch_biword, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu)\n","            # print \"gaz_list:\",gaz_list\n","            # exit(0)\n","            instance_count += 1\n","            loss, tag_seq = model.neg_log_likelihood_loss(gaz_list, batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, batch_label, mask)\n","            right, whole = predict_check(tag_seq, batch_label, mask)\n","            right_token += right\n","            whole_token += whole\n","            #sample_loss += loss.data[0]\n","            #total_loss += loss.data[0]\n","            sample_loss += loss.data.item()\n","            total_loss += loss.data.item()\n","            batch_loss += loss\n","\n","            #if end%2000 == 0:\n","            if end%500 == 0:\n","                temp_time = time.time()\n","                temp_cost = temp_time - temp_start\n","                temp_start = temp_time\n","                print(\"Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f\"%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))\n","                sys.stdout.flush()\n","                sample_loss = 0\n","            if end%data.HP_batch_size == 0:\n","                batch_loss.backward()\n","                optimizer.step()\n","                model.zero_grad()\n","                batch_loss = 0\n","        temp_time = time.time()\n","        temp_cost = temp_time - temp_start\n","        print(\"     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f\"%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))       \n","        epoch_finish = time.time()\n","        epoch_cost = epoch_finish - epoch_start\n","        print(\"Epoch: %s training finished. Time: %.2fs, speed: %.2fst/s,  total loss: %s\"%(idx, epoch_cost, train_num/epoch_cost, total_loss))\n","        # exit(0)\n","        # continue\n","        speed, acc, p, r, f, _ = evaluate(data, model, \"dev\")\n","        dev_finish = time.time()\n","        dev_cost = dev_finish - epoch_finish\n","\n","        if seg:\n","            current_score = f\n","            print(\"Dev: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(dev_cost, speed, acc, p, r, f))\n","        else:\n","            current_score = acc\n","            print(\"Dev: time: %.2fs speed: %.2fst/s; acc: %.4f\"%(dev_cost, speed, acc))\n","\n","        if current_score > best_dev:\n","            if seg:\n","                print (\"Exceed previous best f score:\", best_dev)\n","            else:\n","                print (\"Exceed previous best acc score:\", best_dev)\n","\n","            model_name = save_model_dir\n","            torch.save(model.state_dict(), model_name)\n","            #model_name = save_model_dir +'.'+ str(idx) + \".model\"\n","            #torch.save(model.state_dict(), model_name)\n","            best_dev = current_score \n","        # ## decode test\n","        speed, acc, p, r, f, _ = evaluate(data, model, \"test\")\n","        test_finish = time.time()\n","        test_cost = test_finish - dev_finish\n","        if seg:\n","            print(\"Test: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(test_cost, speed, acc, p, r, f))\n","        else:\n","            print(\"Test: time: %.2fs, speed: %.2fst/s; acc: %.4f\"%(test_cost, speed, acc))\n","        gc.collect()\n","    return model\n","\n","\n","def load_model_decode(model_dir, data, name, gpu, seg=True):\n","    data.HP_gpu = gpu\n","    print (\"Load Model from file: \", model_dir)\n","    model = SeqModel(data)\n","    ## load model need consider if the model trained in GPU and load in CPU, or vice versa\n","    # if not gpu:\n","    #     model.load_state_dict(torch.load(model_dir), map_location=lambda storage, loc: storage)\n","    #     # model = torch.load(model_dir, map_location=lambda storage, loc: storage)\n","    # else:\n","    model.load_state_dict(torch.load(model_dir))\n","        # model = torch.load(model_dir)\n","    #model = torch.load(model_dir)\n","    \n","    print(\"Decode %s data ...\"%(name))\n","    start_time = time.time()\n","    speed, acc, p, r, f, pred_results = evaluate(data, model, name)\n","    end_time = time.time()\n","    time_cost = end_time - start_time\n","    if seg:\n","        print(\"%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(name, time_cost, speed, acc, p, r, f))\n","    else:\n","        print(\"%s: time:%.2fs, speed:%.2fst/s; acc: %.4f\"%(name, time_cost, speed, acc))\n","    return pred_results\n","\n","def load_model_decode_with_model(model, data, name, gpu, seg=True):\n","    data.HP_gpu = gpu\n","    \n","    print(\"Decode %s data ...\"%(name))\n","    start_time = time.time()\n","    speed, acc, p, r, f, pred_results = evaluate(data, model, name)\n","    end_time = time.time()\n","    time_cost = end_time - start_time\n","    if seg:\n","        print(\"%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(name, time_cost, speed, acc, p, r, f))\n","    else:\n","        print(\"%s: time:%.2fs, speed:%.2fst/s; acc: %.4f\"%(name, time_cost, speed, acc))\n","    return pred_results\n"]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='Tuning with bi-directional LSTM-CRF')\n","    parser.add_argument('--embedding',  help='Embedding for words', default='None')\n","    parser.add_argument('--status', choices=['train', 'test', 'decode'], help='update algorithm', default='train')\n","    #parser.add_argument('--savemodel', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/2-222-saved_model2.lstmcrf\")\n","    #parser.add_argument('--savedset', help='Dir of saved data setting', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/save-222.dset\")\n","    parser.add_argument('--savemodel', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/lattice-saved_model.lstmcrf\")\n","    parser.add_argument('--savedset', help='Dir of saved data setting', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/save-data.dset\")\n","    #parser.add_argument('--train', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/train.char.bmes\")\n","    #parser.add_argument('--dev', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/dev.char.bmes\" )\n","    #parser.add_argument('--test', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/test.char.bmes\")\n","\n","    parser.add_argument('--train', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/train.txt\")\n","    parser.add_argument('--dev', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/dev.txt\" )\n","    parser.add_argument('--test', default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/test.txt\")\n","\n","    #parser.add_argument('--train', default=\"test_data/fyz.train.embs\")\n","    #parser.add_argument('--dev', default=\"test_data/fyz.dev.embs\")\n","    #parser.add_argument('--test', default=\"test_data/fyz.test.embs\")\n","    parser.add_argument('--seg', default=\"True\") \n","    parser.add_argument('--extendalphabet', default=\"True\") \n","    parser.add_argument('--raw') \n","    #parser.add_argument('--loadmodel',default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/2-222-saved_model2.lstmcrf\")\n","    parser.add_argument('--loadmodel',default=\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/lattice-saved_model.lstmcrf\")\n","    parser.add_argument('--output') \n","    args = parser.parse_args()\n","   \n","    train_file = args.train\n","    dev_file = args.dev\n","    test_file = args.test\n","    raw_file = args.raw\n","    model_dir = args.loadmodel\n","    dset_dir = args.savedset\n","    output_file = args.output\n","    if args.seg.lower() == \"true\":\n","        seg = True \n","    else:\n","        seg = False\n","    status = args.status.lower()\n","\n","    save_model_dir = args.savemodel\n","    gpu = torch.cuda.is_available()\n","\n","    char_emb = \"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/gigaword_chn.all.a2b.uni.ite50.vec\"\n","    #char_emb = \"/content/drive/MyDrive/lattcie ner/TCM_NER-master/TCM_NER-master/data/sgns.sikuquanshu.vec\"\n","    bichar_emb = '/content/drive/MyDrive/lattcie ner/TCM_NER-master/TCM_NER-master/data/gigaword_chn.all.a2b.bi.ite50.vec'\n","    #bichar_emb = \"/content/drive/MyDrive/lattcie ner/TCM_NER-master/TCM_NER-master/data/sgns.sikuquanshu.bigram.vec\"\n","    gaz_file = \"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/ctb.50d.vec\"\n","    # gaz_file = None\n","    # char_emb = None\n","    #bichar_emb = None\n","\n","    print (\"CuDNN:\", torch.backends.cudnn.enabled)\n","    # gpu = False\n","    print (\"GPU available:\", gpu)\n","    print (\"Status:\", status)\n","    print (\"Seg: \", seg)\n","    print (\"Train file:\", train_file)\n","    print (\"Dev file:\", dev_file)\n","    print (\"Test file:\", test_file)\n","    print (\"Raw file:\", raw_file)\n","    print (\"Char emb:\", char_emb)\n","    print (\"Bichar emb:\", bichar_emb)\n","    print (\"Gaz file:\",gaz_file)\n","    if status == 'train':\n","        print (\"Model saved to:\", save_model_dir)\n","    sys.stdout.flush()\n","    \n","    if status == 'train':\n","        data = Data()\n","        data.HP_gpu = gpu\n","        data.HP_use_char = False\n","        data.HP_batch_size = 10\n","        data.use_bigram = False\n","        data.gaz_dropout = 0.5\n","        data.norm_gaz_emb = False\n","        data.HP_fix_gaz_emb = False\n","        data_initialization(data, gaz_file, train_file, dev_file, test_file)\n","\n","        data.generate_instance_with_gaz(train_file,'train')\n","        data.generate_instance_with_gaz(dev_file,'dev')\n","        data.generate_instance_with_gaz(test_file,'test')\n","\n","        data.build_word_pretrain_emb(char_emb)\n","        data.build_biword_pretrain_emb(bichar_emb)\n","        data.build_gaz_pretrain_emb(gaz_file)\n","        #data = load_data_setting(dset_dir)\n","        model = train(data, save_model_dir,dset_dir, seg)\n","    elif status == 'test':      \n","        data = load_data_setting(dset_dir)\n","        data.generate_instance_with_gaz(dev_file,'dev')\n","        load_model_decode(model_dir, data , 'dev', gpu, seg)\n","        data.generate_instance_with_gaz(test_file,'test')\n","        load_model_decode(model_dir, data, 'test', gpu, seg)\n","    elif status == 'decode':       \n","        data = load_data_setting(dset_dir)\n","        data.generate_instance_with_gaz(raw_file,'raw')\n","        decode_results = load_model_decode(model_dir, data, 'raw', gpu, seg)\n","        data.write_decoded_results(output_file, decode_results, 'raw')\n","    else:\n","        print (\"Invalid argument! Please use valid arguments! (train/test/decode)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n9GTu_tcPM5G","executionInfo":{"status":"ok","timestamp":1646045311896,"user_tz":-480,"elapsed":7369152,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"1444f8d6-46ca-4597-da15-343ddb54f1ec"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["CuDNN: True\n","GPU available: True\n","Status: train\n","Seg:  True\n","Train file: /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/train.txt\n","Dev file: /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/dev.txt\n","Test file: /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/test.txt\n","Raw file: None\n","Char emb: /content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/gigaword_chn.all.a2b.uni.ite50.vec\n","Bichar emb: /content/drive/MyDrive/lattcie ner/TCM_NER-master/TCM_NER-master/data/gigaword_chn.all.a2b.bi.ite50.vec\n","Gaz file: /content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/ctb.50d.vec\n","Model saved to: /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/lattice-saved_model.lstmcrf\n","Load gaz file:  /content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/data/ctb.50d.vec  total size: 704368\n","gaz alphabet size: 16084\n","gaz alphabet size: 17591\n","gaz alphabet size: 19303\n","build word pretrain emb...\n","Embedding:\n","     pretrain word:11327, prefect match:2933, case_match:0, oov:63, oov%:0.021021021021021023\n","build biword pretrain emb...\n","Embedding:\n","     pretrain word:3986686, prefect match:70380, case_match:2, oov:21365, oov%:0.232866111522867\n","build gaz pretrain emb...\n","Embedding:\n","     pretrain word:704368, prefect match:19301, case_match:0, oov:1, oov%:5.1805418846811377e-05\n","Training model...\n","DATA SUMMARY START:\n","     Tag          scheme: BIO\n","     MAX SENTENCE LENGTH: 250\n","     MAX   WORD   LENGTH: -1\n","     Number   normalized: True\n","     Use          bigram: False\n","     Word  alphabet size: 2997\n","     Biword alphabet size: 91748\n","     Char  alphabet size: 2997\n","     Gaz   alphabet size: 19303\n","     Label alphabet size: 5\n","     Word embedding size: 50\n","     Biword embedding size: 50\n","     Char embedding size: 30\n","     Gaz embedding size: 50\n","     Norm     word   emb: True\n","     Norm     biword emb: True\n","     Norm     gaz    emb: False\n","     Norm   gaz  dropout: 0.5\n","     Train instance number: 26182\n","     Dev   instance number: 5638\n","     Test  instance number: 8464\n","     Raw   instance number: 0\n","     Hyperpara  iteration: 100\n","     Hyperpara  batch size: 10\n","     Hyperpara          lr: 0.015\n","     Hyperpara    lr_decay: 0.05\n","     Hyperpara     HP_clip: 5.0\n","     Hyperpara    momentum: 0\n","     Hyperpara  hidden_dim: 200\n","     Hyperpara     dropout: 0.5\n","     Hyperpara  lstm_layer: 1\n","     Hyperpara      bilstm: True\n","     Hyperpara         GPU: True\n","     Hyperpara     use_gaz: True\n","     Hyperpara fix gaz emb: False\n","     Hyperpara    use_char: False\n","DATA SUMMARY END.\n","Data setting saved to file:  /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/save-data.dset\n","build batched lstmcrf...\n","build batched bilstm...\n","build LatticeLSTM...  forward , Fix emb: False  gaz drop: 0.5\n","load pretrain word emb... (19303, 50)\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:105: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n","  init.orthogonal(self.weight_ih.data)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:106: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n","  init.orthogonal(self.alpha_weight_ih.data)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:118: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  init.constant(self.bias.data, val=0)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:119: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  init.constant(self.alpha_bias.data, val=0)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:37: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n","  init.orthogonal(self.weight_ih.data)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:43: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  init.constant(self.bias.data, val=0)\n"]},{"output_type":"stream","name":"stdout","text":["build LatticeLSTM...  backward , Fix emb: False  gaz drop: 0.5\n","load pretrain word emb... (19303, 50)\n","build batched crf...\n","finished built model.\n","Epoch: 0/5\n"," Learning rate is setted as: 0.015\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:94: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:30.)\n","  masked_cur_partition = cur_partition.masked_select(mask_idx)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:99: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:447.)\n","  partition.masked_scatter_(mask_idx, masked_cur_partition)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:243: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:30.)\n","  tg_energy = tg_energy.masked_select(mask.transpose(1,0))\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:156: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n","  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"]},{"output_type":"stream","name":"stdout","text":["Instance: 500; Time: 24.08s; loss: 19336.7461; acc: 5557.0/10164.0=0.5467\n","Instance: 1000; Time: 20.88s; loss: 3985.9834; acc: 12626.0/19388.0=0.6512\n","Instance: 1500; Time: 26.42s; loss: 2948.6758; acc: 20960.0/29575.0=0.7087\n","Instance: 2000; Time: 23.54s; loss: 2361.9191; acc: 29333.0/39541.0=0.7418\n","Instance: 2500; Time: 22.03s; loss: 2181.5068; acc: 37478.0/49217.0=0.7615\n","Instance: 3000; Time: 23.15s; loss: 2184.5509; acc: 46065.0/59208.0=0.7780\n","Instance: 3500; Time: 24.91s; loss: 2333.4824; acc: 55140.0/69939.0=0.7884\n","Instance: 4000; Time: 22.07s; loss: 2649.3646; acc: 62970.0/79362.0=0.7935\n","Instance: 4500; Time: 21.80s; loss: 1425.0020; acc: 71499.0/88937.0=0.8039\n","Instance: 5000; Time: 25.02s; loss: 2133.5153; acc: 80793.0/99568.0=0.8114\n","Instance: 5500; Time: 22.49s; loss: 1562.7746; acc: 89581.0/109490.0=0.8182\n","Instance: 6000; Time: 24.35s; loss: 1557.4173; acc: 98897.0/119901.0=0.8248\n","Instance: 6500; Time: 21.05s; loss: 1208.1266; acc: 107176.0/129088.0=0.8303\n","Instance: 7000; Time: 24.59s; loss: 1498.0502; acc: 116424.0/139490.0=0.8346\n","Instance: 7500; Time: 22.12s; loss: 1161.1125; acc: 125032.0/148896.0=0.8397\n","Instance: 8000; Time: 22.31s; loss: 1613.0380; acc: 133714.0/158712.0=0.8425\n","Instance: 8500; Time: 21.58s; loss: 1388.6060; acc: 142218.0/168162.0=0.8457\n","Instance: 9000; Time: 22.40s; loss: 1295.6094; acc: 151028.0/177838.0=0.8492\n","Instance: 9500; Time: 24.38s; loss: 1812.2393; acc: 159783.0/187779.0=0.8509\n","Instance: 10000; Time: 26.59s; loss: 1253.4366; acc: 168656.0/197533.0=0.8538\n","Instance: 10500; Time: 21.97s; loss: 1068.9447; acc: 177335.0/206982.0=0.8568\n","Instance: 11000; Time: 24.49s; loss: 1205.0951; acc: 186823.0/217346.0=0.8596\n","Instance: 11500; Time: 22.30s; loss: 1153.6849; acc: 195780.0/227096.0=0.8621\n","Instance: 12000; Time: 21.68s; loss: 1389.2692; acc: 204322.0/236574.0=0.8637\n","Instance: 12500; Time: 23.38s; loss: 1685.7756; acc: 213655.0/246940.0=0.8652\n","Instance: 13000; Time: 22.04s; loss: 1250.8323; acc: 222349.0/256397.0=0.8672\n","Instance: 13500; Time: 23.49s; loss: 1219.0695; acc: 231713.0/266665.0=0.8689\n","Instance: 14000; Time: 24.47s; loss: 1203.1619; acc: 241510.0/277363.0=0.8707\n","Instance: 14500; Time: 22.05s; loss: 1052.8466; acc: 250293.0/286878.0=0.8725\n","Instance: 15000; Time: 22.55s; loss: 1033.8495; acc: 259391.0/296779.0=0.8740\n","Instance: 15500; Time: 23.11s; loss: 1221.3301; acc: 268629.0/306934.0=0.8752\n","Instance: 16000; Time: 22.45s; loss: 1070.2618; acc: 277650.0/316719.0=0.8766\n","Instance: 16500; Time: 22.06s; loss: 982.8934; acc: 286596.0/326388.0=0.8781\n","Instance: 17000; Time: 20.90s; loss: 901.4911; acc: 295197.0/335677.0=0.8794\n","Instance: 17500; Time: 23.34s; loss: 943.6898; acc: 304624.0/345813.0=0.8809\n","Instance: 18000; Time: 24.24s; loss: 1085.6972; acc: 314429.0/356342.0=0.8824\n","Instance: 18500; Time: 20.81s; loss: 868.3439; acc: 322882.0/365365.0=0.8837\n","Instance: 19000; Time: 22.04s; loss: 885.9303; acc: 331963.0/375055.0=0.8851\n","Instance: 19500; Time: 28.56s; loss: 1041.3491; acc: 342050.0/385920.0=0.8863\n","Instance: 20000; Time: 24.47s; loss: 1104.6609; acc: 351702.0/396340.0=0.8874\n","Instance: 20500; Time: 21.67s; loss: 962.5764; acc: 360601.0/405937.0=0.8883\n","Instance: 21000; Time: 21.15s; loss: 818.4035; acc: 369408.0/415357.0=0.8894\n","Instance: 21500; Time: 22.93s; loss: 972.7460; acc: 378591.0/425200.0=0.8904\n","Instance: 22000; Time: 20.19s; loss: 743.7866; acc: 386986.0/434143.0=0.8914\n","Instance: 22500; Time: 20.28s; loss: 760.9310; acc: 395471.0/443128.0=0.8925\n","Instance: 23000; Time: 21.83s; loss: 1223.2302; acc: 404178.0/452605.0=0.8930\n","Instance: 23500; Time: 22.11s; loss: 947.2166; acc: 413358.0/462441.0=0.8939\n","Instance: 24000; Time: 23.57s; loss: 1000.9352; acc: 423012.0/472738.0=0.8948\n","Instance: 24500; Time: 20.60s; loss: 820.9341; acc: 431489.0/481891.0=0.8954\n","Instance: 25000; Time: 21.38s; loss: 787.8388; acc: 440280.0/491333.0=0.8961\n","Instance: 25500; Time: 22.41s; loss: 881.4809; acc: 449513.0/501242.0=0.8968\n","Instance: 26000; Time: 21.18s; loss: 862.0359; acc: 458291.0/510654.0=0.8975\n","     Instance: 26182; Time: 7.03s; loss: 348.3083; acc: 461251.0/513812.0=0.8977\n","Epoch: 0 training finished. Time: 1192.53s, speed: 21.95st/s,  total loss: 89389.75743103027\n"]},{"output_type":"stream","name":"stderr","text":["__main__:184: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:185: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:186: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:187: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:207: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:263: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  word_var = autograd.Variable(torch.LongTensor(skip_input_[t][0]),volatile =  volatile_flag)\n"]},{"output_type":"stream","name":"stdout","text":["gold_num =  10552  pred_num =  10689  right_num =  8537\n","Dev: time: 103.12s, speed: 54.74st/s; acc: 0.9504, p: 0.7987, r: 0.8090, f: 0.8038\n","Exceed previous best f score: -1\n","gold_num =  15522  pred_num =  15776  right_num =  12604\n","Test: time: 151.81s, speed: 55.86st/s; acc: 0.9537, p: 0.7989, r: 0.8120, f: 0.8054\n","Epoch: 1/5\n"," Learning rate is setted as: 0.014249999999999999\n","Instance: 500; Time: 22.03s; loss: 732.0709; acc: 9117.0/9658.0=0.9440\n","Instance: 1000; Time: 22.81s; loss: 842.3843; acc: 18416.0/19628.0=0.9383\n","Instance: 1500; Time: 23.29s; loss: 710.5457; acc: 27994.0/29732.0=0.9415\n","Instance: 2000; Time: 22.92s; loss: 938.7327; acc: 37289.0/39699.0=0.9393\n","Instance: 2500; Time: 21.48s; loss: 653.0795; acc: 46260.0/49201.0=0.9402\n","Instance: 3000; Time: 23.07s; loss: 916.3172; acc: 55464.0/59078.0=0.9388\n","Instance: 3500; Time: 23.28s; loss: 796.9550; acc: 64879.0/69038.0=0.9398\n","Instance: 4000; Time: 20.64s; loss: 910.4890; acc: 73588.0/78409.0=0.9385\n","Instance: 4500; Time: 23.54s; loss: 806.2690; acc: 82893.0/88284.0=0.9389\n","Instance: 5000; Time: 25.27s; loss: 935.2462; acc: 93063.0/99106.0=0.9390\n","Instance: 5500; Time: 23.03s; loss: 715.6793; acc: 102681.0/109232.0=0.9400\n","Instance: 6000; Time: 20.84s; loss: 935.1677; acc: 111392.0/118591.0=0.9393\n","Instance: 6500; Time: 23.18s; loss: 662.5273; acc: 121145.0/128801.0=0.9406\n","Instance: 7000; Time: 20.89s; loss: 814.8818; acc: 129944.0/138199.0=0.9403\n","Instance: 7500; Time: 25.04s; loss: 773.5884; acc: 140295.0/149096.0=0.9410\n","Instance: 8000; Time: 22.54s; loss: 731.6365; acc: 149777.0/159096.0=0.9414\n","Instance: 8500; Time: 23.05s; loss: 765.9630; acc: 159053.0/168947.0=0.9414\n","Instance: 9000; Time: 25.37s; loss: 686.9575; acc: 168308.0/178716.0=0.9418\n","Instance: 9500; Time: 21.36s; loss: 736.6747; acc: 177326.0/188275.0=0.9418\n","Instance: 10000; Time: 19.19s; loss: 671.6655; acc: 185508.0/196942.0=0.9419\n","Instance: 10500; Time: 22.29s; loss: 726.6157; acc: 194870.0/206885.0=0.9419\n","Instance: 11000; Time: 22.33s; loss: 684.3274; acc: 204173.0/216702.0=0.9422\n","Instance: 11500; Time: 22.97s; loss: 760.3212; acc: 213600.0/226592.0=0.9427\n","Instance: 12000; Time: 21.36s; loss: 772.3624; acc: 222764.0/236279.0=0.9428\n","Instance: 12500; Time: 24.25s; loss: 761.1774; acc: 232791.0/246877.0=0.9429\n","Instance: 13000; Time: 20.57s; loss: 748.0462; acc: 241458.0/256066.0=0.9430\n","Instance: 13500; Time: 19.16s; loss: 652.8577; acc: 249706.0/264793.0=0.9430\n","Instance: 14000; Time: 26.14s; loss: 777.7628; acc: 260204.0/275868.0=0.9432\n","Instance: 14500; Time: 22.47s; loss: 650.7357; acc: 269451.0/285593.0=0.9435\n","Instance: 15000; Time: 23.69s; loss: 1098.0652; acc: 278785.0/295639.0=0.9430\n","Instance: 15500; Time: 21.56s; loss: 673.2825; acc: 287863.0/305224.0=0.9431\n","Instance: 16000; Time: 20.61s; loss: 561.4535; acc: 296423.0/314224.0=0.9433\n","Instance: 16500; Time: 21.88s; loss: 620.4769; acc: 305562.0/323822.0=0.9436\n","Instance: 17000; Time: 24.36s; loss: 818.0040; acc: 315396.0/334172.0=0.9438\n","Instance: 17500; Time: 20.64s; loss: 712.6893; acc: 324048.0/343325.0=0.9439\n","Instance: 18000; Time: 26.00s; loss: 678.9089; acc: 333399.0/353174.0=0.9440\n","Instance: 18500; Time: 21.67s; loss: 761.9736; acc: 342317.0/362621.0=0.9440\n","Instance: 19000; Time: 21.80s; loss: 743.2213; acc: 351430.0/372261.0=0.9440\n","Instance: 19500; Time: 20.05s; loss: 672.3777; acc: 359880.0/381184.0=0.9441\n","Instance: 20000; Time: 24.65s; loss: 748.7549; acc: 370267.0/392086.0=0.9444\n","Instance: 20500; Time: 23.57s; loss: 834.5791; acc: 379763.0/402165.0=0.9443\n","Instance: 21000; Time: 22.90s; loss: 780.1888; acc: 389291.0/412258.0=0.9443\n","Instance: 21500; Time: 23.55s; loss: 770.3799; acc: 398919.0/422456.0=0.9443\n","Instance: 22000; Time: 22.80s; loss: 861.3232; acc: 408230.0/432353.0=0.9442\n","Instance: 22500; Time: 22.68s; loss: 667.4100; acc: 417612.0/442274.0=0.9442\n","Instance: 23000; Time: 20.45s; loss: 698.8339; acc: 426243.0/451407.0=0.9443\n","Instance: 23500; Time: 21.38s; loss: 706.7344; acc: 435063.0/460743.0=0.9443\n","Instance: 24000; Time: 22.39s; loss: 690.3665; acc: 444320.0/470509.0=0.9443\n","Instance: 24500; Time: 22.84s; loss: 648.0793; acc: 453605.0/480294.0=0.9444\n","Instance: 25000; Time: 23.97s; loss: 956.0894; acc: 463213.0/490635.0=0.9441\n","Instance: 25500; Time: 21.24s; loss: 604.5923; acc: 472162.0/500000.0=0.9443\n","Instance: 26000; Time: 23.44s; loss: 707.1766; acc: 481458.0/509856.0=0.9443\n","     Instance: 26182; Time: 9.28s; loss: 227.7548; acc: 485257.0/513812.0=0.9444\n","Epoch: 1 training finished. Time: 1181.78s, speed: 22.15st/s,  total loss: 39513.75360107422\n","gold_num =  10552  pred_num =  9883  right_num =  8523\n","Dev: time: 103.86s, speed: 54.35st/s; acc: 0.9572, p: 0.8624, r: 0.8077, f: 0.8342\n","Exceed previous best f score: 0.8038227955369333\n","gold_num =  15522  pred_num =  14627  right_num =  12664\n","Test: time: 151.72s, speed: 55.89st/s; acc: 0.9602, p: 0.8658, r: 0.8159, f: 0.8401\n","Epoch: 2/5\n"," Learning rate is setted as: 0.0135375\n","Instance: 500; Time: 20.40s; loss: 536.7110; acc: 8753.0/9143.0=0.9573\n","Instance: 1000; Time: 20.86s; loss: 589.6511; acc: 17416.0/18300.0=0.9517\n","Instance: 1500; Time: 23.95s; loss: 610.2016; acc: 27466.0/28810.0=0.9533\n","Instance: 2000; Time: 23.30s; loss: 731.4610; acc: 37134.0/39008.0=0.9520\n","Instance: 2500; Time: 22.05s; loss: 608.3384; acc: 46312.0/48617.0=0.9526\n","Instance: 3000; Time: 22.11s; loss: 654.9143; acc: 55584.0/58346.0=0.9527\n","Instance: 3500; Time: 22.18s; loss: 801.1943; acc: 64893.0/68231.0=0.9511\n","Instance: 4000; Time: 25.32s; loss: 725.3213; acc: 75454.0/79334.0=0.9511\n","Instance: 4500; Time: 22.95s; loss: 586.3957; acc: 85111.0/89393.0=0.9521\n","Instance: 5000; Time: 23.16s; loss: 624.3540; acc: 94866.0/99644.0=0.9520\n","Instance: 5500; Time: 19.37s; loss: 549.0053; acc: 102924.0/108123.0=0.9519\n","Instance: 6000; Time: 22.46s; loss: 674.9854; acc: 112146.0/117853.0=0.9516\n","Instance: 6500; Time: 20.80s; loss: 672.4825; acc: 120918.0/127058.0=0.9517\n","Instance: 7000; Time: 21.66s; loss: 610.8835; acc: 129982.0/136536.0=0.9520\n","Instance: 7500; Time: 22.23s; loss: 677.6337; acc: 139318.0/146347.0=0.9520\n","Instance: 8000; Time: 24.12s; loss: 585.6700; acc: 149440.0/156921.0=0.9523\n","Instance: 8500; Time: 20.61s; loss: 636.5739; acc: 158212.0/166142.0=0.9523\n","Instance: 9000; Time: 22.94s; loss: 690.4977; acc: 167630.0/176093.0=0.9519\n","Instance: 9500; Time: 24.28s; loss: 622.1055; acc: 176738.0/185635.0=0.9521\n","Instance: 10000; Time: 24.49s; loss: 764.7646; acc: 186895.0/196351.0=0.9518\n","Instance: 10500; Time: 22.37s; loss: 597.9241; acc: 196160.0/206034.0=0.9521\n","Instance: 11000; Time: 19.58s; loss: 524.3712; acc: 204700.0/214970.0=0.9522\n","Instance: 11500; Time: 22.97s; loss: 821.6545; acc: 214241.0/225127.0=0.9516\n","Instance: 12000; Time: 23.47s; loss: 639.4562; acc: 224071.0/235440.0=0.9517\n","Instance: 12500; Time: 21.70s; loss: 536.9854; acc: 233243.0/244989.0=0.9521\n","Instance: 13000; Time: 22.10s; loss: 565.7352; acc: 242722.0/254872.0=0.9523\n","Instance: 13500; Time: 20.97s; loss: 545.5223; acc: 251680.0/264205.0=0.9526\n","Instance: 14000; Time: 21.88s; loss: 531.2503; acc: 260988.0/273884.0=0.9529\n","Instance: 14500; Time: 24.05s; loss: 648.9761; acc: 271165.0/284514.0=0.9531\n","Instance: 15000; Time: 20.40s; loss: 577.8868; acc: 279690.0/293488.0=0.9530\n","Instance: 15500; Time: 24.07s; loss: 680.7773; acc: 289497.0/303872.0=0.9527\n","Instance: 16000; Time: 24.14s; loss: 666.9368; acc: 299352.0/314216.0=0.9527\n","Instance: 16500; Time: 21.62s; loss: 701.0284; acc: 308495.0/323886.0=0.9525\n","Instance: 17000; Time: 22.31s; loss: 534.7020; acc: 317794.0/333573.0=0.9527\n","Instance: 17500; Time: 21.68s; loss: 576.6046; acc: 326805.0/343019.0=0.9527\n","Instance: 18000; Time: 22.82s; loss: 554.3448; acc: 336190.0/352833.0=0.9528\n","Instance: 18500; Time: 26.62s; loss: 571.5067; acc: 345910.0/362936.0=0.9531\n","Instance: 19000; Time: 20.46s; loss: 597.0006; acc: 354714.0/372156.0=0.9531\n","Instance: 19500; Time: 21.85s; loss: 640.4609; acc: 363775.0/381655.0=0.9532\n","Instance: 20000; Time: 23.86s; loss: 766.5417; acc: 373547.0/391943.0=0.9531\n","Instance: 20500; Time: 25.83s; loss: 769.8881; acc: 384075.0/403050.0=0.9529\n","Instance: 21000; Time: 21.80s; loss: 587.1096; acc: 393396.0/412808.0=0.9530\n","Instance: 21500; Time: 21.37s; loss: 587.4048; acc: 402576.0/422423.0=0.9530\n","Instance: 22000; Time: 21.00s; loss: 471.0228; acc: 411482.0/431669.0=0.9532\n","Instance: 22500; Time: 24.13s; loss: 650.3146; acc: 421408.0/442013.0=0.9534\n","Instance: 23000; Time: 24.70s; loss: 703.7010; acc: 431535.0/452642.0=0.9534\n","Instance: 23500; Time: 21.07s; loss: 593.7968; acc: 440443.0/461991.0=0.9534\n","Instance: 24000; Time: 21.30s; loss: 591.6038; acc: 449396.0/471381.0=0.9534\n","Instance: 24500; Time: 21.64s; loss: 707.2083; acc: 458340.0/480877.0=0.9531\n","Instance: 25000; Time: 23.65s; loss: 599.4626; acc: 468125.0/491112.0=0.9532\n","Instance: 25500; Time: 20.47s; loss: 619.6437; acc: 476956.0/500375.0=0.9532\n","Instance: 26000; Time: 23.34s; loss: 752.6755; acc: 486600.0/510525.0=0.9531\n","     Instance: 26182; Time: 7.72s; loss: 238.0806; acc: 489701.0/513812.0=0.9531\n","Epoch: 2 training finished. Time: 1176.18s, speed: 22.26st/s,  total loss: 33104.72283935547\n","gold_num =  10552  pred_num =  10238  right_num =  8900\n","Dev: time: 102.39s, speed: 55.14st/s; acc: 0.9647, p: 0.8693, r: 0.8434, f: 0.8562\n","Exceed previous best f score: 0.8341570834352826\n","gold_num =  15522  pred_num =  15175  right_num =  13159\n","Test: time: 150.17s, speed: 56.47st/s; acc: 0.9648, p: 0.8671, r: 0.8478, f: 0.8573\n","Epoch: 3/5\n"," Learning rate is setted as: 0.012860624999999997\n","Instance: 500; Time: 24.15s; loss: 602.2979; acc: 10224.0/10634.0=0.9614\n","Instance: 1000; Time: 20.42s; loss: 557.6384; acc: 19083.0/19879.0=0.9600\n","Instance: 1500; Time: 23.86s; loss: 679.4026; acc: 28986.0/30275.0=0.9574\n","Instance: 2000; Time: 21.07s; loss: 464.1230; acc: 37947.0/39559.0=0.9593\n","Instance: 2500; Time: 21.31s; loss: 438.4531; acc: 46826.0/48773.0=0.9601\n","Instance: 3000; Time: 19.47s; loss: 469.0389; acc: 55151.0/57394.0=0.9609\n","Instance: 3500; Time: 20.87s; loss: 573.7031; acc: 63953.0/66599.0=0.9603\n","Instance: 4000; Time: 20.45s; loss: 647.3276; acc: 72782.0/75865.0=0.9594\n","Instance: 4500; Time: 25.75s; loss: 688.4424; acc: 83254.0/86853.0=0.9586\n","Instance: 5000; Time: 21.66s; loss: 555.7060; acc: 92196.0/96215.0=0.9582\n","Instance: 5500; Time: 20.98s; loss: 543.5529; acc: 101124.0/105532.0=0.9582\n","Instance: 6000; Time: 23.73s; loss: 558.5493; acc: 111066.0/115881.0=0.9584\n","Instance: 6500; Time: 23.60s; loss: 640.4091; acc: 120807.0/126127.0=0.9578\n","Instance: 7000; Time: 20.93s; loss: 582.4302; acc: 129844.0/135540.0=0.9580\n","Instance: 7500; Time: 22.87s; loss: 636.5276; acc: 139101.0/145325.0=0.9572\n","Instance: 8000; Time: 23.39s; loss: 625.2935; acc: 148789.0/155483.0=0.9569\n","Instance: 8500; Time: 25.49s; loss: 661.3878; acc: 159166.0/166364.0=0.9567\n","Instance: 9000; Time: 26.44s; loss: 522.4391; acc: 168870.0/176443.0=0.9571\n","Instance: 9500; Time: 25.13s; loss: 588.8833; acc: 179299.0/187247.0=0.9576\n","Instance: 10000; Time: 20.08s; loss: 503.6473; acc: 187976.0/196277.0=0.9577\n","Instance: 10500; Time: 23.11s; loss: 593.0042; acc: 197535.0/206255.0=0.9577\n","Instance: 11000; Time: 20.42s; loss: 612.2129; acc: 206281.0/215438.0=0.9575\n","Instance: 11500; Time: 23.88s; loss: 637.2604; acc: 216253.0/225869.0=0.9574\n","Instance: 12000; Time: 20.55s; loss: 536.1977; acc: 225028.0/235042.0=0.9574\n","Instance: 12500; Time: 23.94s; loss: 596.2206; acc: 235010.0/245434.0=0.9575\n","Instance: 13000; Time: 22.74s; loss: 494.9243; acc: 244557.0/255352.0=0.9577\n","Instance: 13500; Time: 21.32s; loss: 604.0441; acc: 253602.0/264804.0=0.9577\n","Instance: 14000; Time: 25.06s; loss: 538.4828; acc: 263814.0/275441.0=0.9578\n","Instance: 14500; Time: 21.86s; loss: 618.6227; acc: 272777.0/284864.0=0.9576\n","Instance: 15000; Time: 21.06s; loss: 590.8762; acc: 281740.0/294274.0=0.9574\n","Instance: 15500; Time: 21.59s; loss: 644.3759; acc: 290643.0/303672.0=0.9571\n","Instance: 16000; Time: 23.37s; loss: 516.9382; acc: 300278.0/313701.0=0.9572\n","Instance: 16500; Time: 22.48s; loss: 581.7199; acc: 309715.0/323559.0=0.9572\n","Instance: 17000; Time: 21.91s; loss: 453.3794; acc: 318967.0/333145.0=0.9574\n","Instance: 17500; Time: 22.34s; loss: 574.2426; acc: 327977.0/342556.0=0.9574\n","Instance: 18000; Time: 22.44s; loss: 592.7681; acc: 337554.0/352550.0=0.9575\n","Instance: 18500; Time: 27.53s; loss: 555.2668; acc: 347480.0/362949.0=0.9574\n","Instance: 19000; Time: 21.62s; loss: 507.5866; acc: 356693.0/372534.0=0.9575\n","Instance: 19500; Time: 19.41s; loss: 460.4724; acc: 365148.0/381330.0=0.9576\n","Instance: 20000; Time: 19.54s; loss: 533.4570; acc: 373623.0/390178.0=0.9576\n","Instance: 20500; Time: 24.70s; loss: 682.7279; acc: 383946.0/401029.0=0.9574\n","Instance: 21000; Time: 23.66s; loss: 550.8733; acc: 393722.0/411235.0=0.9574\n","Instance: 21500; Time: 21.95s; loss: 449.4612; acc: 402883.0/420739.0=0.9576\n","Instance: 22000; Time: 23.35s; loss: 427.0403; acc: 412731.0/430898.0=0.9578\n","Instance: 22500; Time: 21.35s; loss: 517.2490; acc: 421717.0/440270.0=0.9579\n","Instance: 23000; Time: 20.66s; loss: 555.1246; acc: 430574.0/449540.0=0.9578\n","Instance: 23500; Time: 24.14s; loss: 524.0795; acc: 440474.0/459815.0=0.9579\n","Instance: 24000; Time: 22.12s; loss: 614.9189; acc: 449838.0/469624.0=0.9579\n","Instance: 24500; Time: 20.60s; loss: 580.7578; acc: 458605.0/478817.0=0.9578\n","Instance: 25000; Time: 23.83s; loss: 821.3586; acc: 468375.0/489217.0=0.9574\n","Instance: 25500; Time: 25.55s; loss: 680.1976; acc: 478981.0/500295.0=0.9574\n","Instance: 26000; Time: 22.26s; loss: 557.2106; acc: 488435.0/510165.0=0.9574\n","     Instance: 26182; Time: 8.41s; loss: 236.9893; acc: 491910.0/513812.0=0.9574\n","Epoch: 3 training finished. Time: 1180.37s, speed: 22.18st/s,  total loss: 29979.29476928711\n","gold_num =  10552  pred_num =  10246  right_num =  8950\n","Dev: time: 101.87s, speed: 55.42st/s; acc: 0.9660, p: 0.8735, r: 0.8482, f: 0.8607\n","Exceed previous best f score: 0.8561808561808562\n","gold_num =  15522  pred_num =  15153  right_num =  13308\n","Test: time: 150.36s, speed: 56.40st/s; acc: 0.9679, p: 0.8782, r: 0.8574, f: 0.8677\n","Epoch: 4/5\n"," Learning rate is setted as: 0.012217593749999998\n","Instance: 500; Time: 23.27s; loss: 581.7950; acc: 9991.0/10456.0=0.9555\n","Instance: 1000; Time: 22.57s; loss: 543.7815; acc: 19405.0/20291.0=0.9563\n","Instance: 1500; Time: 20.23s; loss: 437.5908; acc: 28181.0/29396.0=0.9587\n","Instance: 2000; Time: 22.10s; loss: 626.7693; acc: 37495.0/39183.0=0.9569\n","Instance: 2500; Time: 23.78s; loss: 560.5063; acc: 47147.0/49295.0=0.9564\n","Instance: 3000; Time: 21.22s; loss: 478.0698; acc: 56313.0/58769.0=0.9582\n","Instance: 3500; Time: 22.03s; loss: 480.8660; acc: 65435.0/68231.0=0.9590\n","Instance: 4000; Time: 20.00s; loss: 484.8936; acc: 74045.0/77201.0=0.9591\n","Instance: 4500; Time: 25.43s; loss: 557.9564; acc: 84459.0/88026.0=0.9595\n","Instance: 5000; Time: 21.27s; loss: 531.7117; acc: 93350.0/97292.0=0.9595\n","Instance: 5500; Time: 21.79s; loss: 521.9878; acc: 102461.0/106797.0=0.9594\n","Instance: 6000; Time: 21.62s; loss: 490.6523; acc: 111751.0/116449.0=0.9597\n","Instance: 6500; Time: 23.36s; loss: 551.2448; acc: 121445.0/126556.0=0.9596\n","Instance: 7000; Time: 19.98s; loss: 451.9746; acc: 130105.0/135584.0=0.9596\n","Instance: 7500; Time: 21.45s; loss: 368.4019; acc: 139329.0/145078.0=0.9604\n","Instance: 8000; Time: 24.86s; loss: 572.5289; acc: 149664.0/155828.0=0.9604\n","Instance: 8500; Time: 23.00s; loss: 512.0823; acc: 159259.0/165768.0=0.9607\n","Instance: 9000; Time: 21.80s; loss: 513.1075; acc: 168445.0/175313.0=0.9608\n","Instance: 9500; Time: 24.32s; loss: 444.4902; acc: 177658.0/184851.0=0.9611\n","Instance: 10000; Time: 20.36s; loss: 427.9721; acc: 186430.0/193923.0=0.9614\n","Instance: 10500; Time: 23.30s; loss: 497.2791; acc: 196236.0/204122.0=0.9614\n","Instance: 11000; Time: 21.30s; loss: 528.8248; acc: 205304.0/213576.0=0.9613\n","Instance: 11500; Time: 23.61s; loss: 467.0580; acc: 215124.0/223760.0=0.9614\n","Instance: 12000; Time: 23.02s; loss: 634.2006; acc: 224538.0/233676.0=0.9609\n","Instance: 12500; Time: 22.35s; loss: 575.4929; acc: 234045.0/243571.0=0.9609\n","Instance: 13000; Time: 20.95s; loss: 568.1694; acc: 242983.0/252905.0=0.9608\n","Instance: 13500; Time: 23.67s; loss: 664.1357; acc: 253052.0/263479.0=0.9604\n","Instance: 14000; Time: 24.34s; loss: 579.5005; acc: 262996.0/273858.0=0.9603\n","Instance: 14500; Time: 23.11s; loss: 608.1356; acc: 272362.0/283696.0=0.9600\n","Instance: 15000; Time: 25.42s; loss: 651.0396; acc: 282727.0/294540.0=0.9599\n","Instance: 15500; Time: 23.77s; loss: 471.2257; acc: 292409.0/304624.0=0.9599\n","Instance: 16000; Time: 20.98s; loss: 422.1351; acc: 301331.0/313844.0=0.9601\n","Instance: 16500; Time: 21.06s; loss: 472.8011; acc: 310200.0/323111.0=0.9600\n","Instance: 17000; Time: 23.43s; loss: 468.9648; acc: 319949.0/333168.0=0.9603\n","Instance: 17500; Time: 21.95s; loss: 493.4711; acc: 329260.0/342830.0=0.9604\n","Instance: 18000; Time: 26.07s; loss: 567.0148; acc: 338781.0/352746.0=0.9604\n","Instance: 18500; Time: 20.85s; loss: 500.8960; acc: 347629.0/361966.0=0.9604\n","Instance: 19000; Time: 21.02s; loss: 478.8661; acc: 356536.0/371222.0=0.9604\n","Instance: 19500; Time: 23.50s; loss: 633.4971; acc: 366191.0/381362.0=0.9602\n","Instance: 20000; Time: 25.22s; loss: 584.6187; acc: 376453.0/392003.0=0.9603\n","Instance: 20500; Time: 21.28s; loss: 499.8733; acc: 385361.0/401318.0=0.9602\n","Instance: 21000; Time: 20.88s; loss: 468.2416; acc: 394288.0/410593.0=0.9603\n","Instance: 21500; Time: 22.39s; loss: 622.7063; acc: 403238.0/420025.0=0.9600\n","Instance: 22000; Time: 21.94s; loss: 472.8765; acc: 412448.0/429611.0=0.9600\n","Instance: 22500; Time: 22.42s; loss: 541.6245; acc: 421840.0/439400.0=0.9600\n","Instance: 23000; Time: 22.83s; loss: 607.7380; acc: 431251.0/449216.0=0.9600\n","Instance: 23500; Time: 22.46s; loss: 522.5474; acc: 440415.0/458782.0=0.9600\n","Instance: 24000; Time: 21.91s; loss: 555.0611; acc: 449807.0/468582.0=0.9599\n","Instance: 24500; Time: 26.04s; loss: 617.7845; acc: 460657.0/479897.0=0.9599\n","Instance: 25000; Time: 20.26s; loss: 444.4366; acc: 468980.0/488563.0=0.9599\n","Instance: 25500; Time: 25.98s; loss: 685.0767; acc: 479681.0/499744.0=0.9599\n","Instance: 26000; Time: 24.13s; loss: 544.6059; acc: 489506.0/509984.0=0.9598\n","     Instance: 26182; Time: 9.18s; loss: 272.8558; acc: 493167.0/513812.0=0.9598\n","Epoch: 4 training finished. Time: 1185.07s, speed: 22.09st/s,  total loss: 27861.13751220703\n","gold_num =  10552  pred_num =  10854  right_num =  9260\n","Dev: time: 104.56s, speed: 53.99st/s; acc: 0.9630, p: 0.8531, r: 0.8776, f: 0.8652\n","Exceed previous best f score: 0.8606596788152706\n","gold_num =  15522  pred_num =  16001  right_num =  13627\n","Test: time: 154.18s, speed: 55.00st/s; acc: 0.9641, p: 0.8516, r: 0.8779, f: 0.8646\n"]}]},{"cell_type":"code","source":["torch.save(model,\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/86%-228-test.model\")"],"metadata":{"id":"HSV6HkI1Adag","executionInfo":{"status":"ok","timestamp":1646046393322,"user_tz":-480,"elapsed":548,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import torch\n","model_test=(torch.load(\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/86%-228-test.model\"))\n","data = load_data_setting(\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/save-data.dset\")\n","data.generate_instance_with_gaz('/content/drive/MyDrive/TT.txt','test')\n","gpu = torch.cuda.is_available()\n","load_model_decode_with_model(model_test, data, 'test', gpu, True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KNY9vapOBPoc","executionInfo":{"status":"ok","timestamp":1646046990529,"user_tz":-480,"elapsed":20057,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"1a114ab7-a2b1-4d71-b251-1a1b2582ee00"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Data setting loaded from file:  /content/drive/MyDrive/lattcie ner/fyz-lattcie/cyx/save-data.dset\n","DATA SUMMARY START:\n","     Tag          scheme: BIO\n","     MAX SENTENCE LENGTH: 250\n","     MAX   WORD   LENGTH: -1\n","     Number   normalized: True\n","     Use          bigram: False\n","     Word  alphabet size: 2997\n","     Biword alphabet size: 91748\n","     Char  alphabet size: 2997\n","     Gaz   alphabet size: 19303\n","     Label alphabet size: 5\n","     Word embedding size: 50\n","     Biword embedding size: 50\n","     Char embedding size: 30\n","     Gaz embedding size: 50\n","     Norm     word   emb: True\n","     Norm     biword emb: True\n","     Norm     gaz    emb: False\n","     Norm   gaz  dropout: 0.5\n","     Train instance number: 26182\n","     Dev   instance number: 5638\n","     Test  instance number: 8464\n","     Raw   instance number: 0\n","     Hyperpara  iteration: 100\n","     Hyperpara  batch size: 10\n","     Hyperpara          lr: 0.015\n","     Hyperpara    lr_decay: 0.05\n","     Hyperpara     HP_clip: 5.0\n","     Hyperpara    momentum: 0\n","     Hyperpara  hidden_dim: 200\n","     Hyperpara     dropout: 0.5\n","     Hyperpara  lstm_layer: 1\n","     Hyperpara      bilstm: True\n","     Hyperpara         GPU: True\n","     Hyperpara     use_gaz: True\n","     Hyperpara fix gaz emb: False\n","     Hyperpara    use_char: False\n","DATA SUMMARY END.\n","Decode test data ...\n"]},{"output_type":"stream","name":"stderr","text":["__main__:184: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:185: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:186: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:187: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:207: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:263: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  word_var = autograd.Variable(torch.LongTensor(skip_input_[t][0]),volatile =  volatile_flag)\n"]},{"output_type":"stream","name":"stdout","text":["gold_num =  0  pred_num =  3  right_num =  0\n","test: time:2.26s, speed:1.33st/s; acc: 0.6897, p: 0.0000, r: -1.0000, f: -1.0000\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:156: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n","  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"]},{"output_type":"execute_result","data":{"text/plain":["[['O',\n","  'B-SYM',\n","  'E-SYM',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O'],\n"," ['B-SYM', 'I-SYM', 'I-SYM', 'E-SYM', 'O', 'O', 'O'],\n"," ['B-SYM', 'I-SYM', 'E-SYM', 'O', 'O', 'O', 'O']]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["data.generate_instance_with_gaz('/content/drive/MyDrive/TT.txt','test')"],"metadata":{"id":"PMWy4gzLDcLu","executionInfo":{"status":"ok","timestamp":1646047121665,"user_tz":-480,"elapsed":1486,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["load_model_decode_with_model(model_test, data, 'test', gpu, True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1qGd5rclDY8T","executionInfo":{"status":"ok","timestamp":1646047125562,"user_tz":-480,"elapsed":676,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"856d82be-dffc-4969-ab36-2263c08af689"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Decode test data ...\n","gold_num =  0  pred_num =  6  right_num =  0\n","test: time:0.08s, speed:51.80st/s; acc: 0.6087, p: 0.0000, r: -1.0000, f: -1.0000\n"]},{"output_type":"stream","name":"stderr","text":["__main__:184: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:185: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:186: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:187: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:207: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:263: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  word_var = autograd.Variable(torch.LongTensor(skip_input_[t][0]),volatile =  volatile_flag)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:156: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n","  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"]},{"output_type":"execute_result","data":{"text/plain":["[['O',\n","  'B-SYM',\n","  'E-SYM',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O'],\n"," ['B-SYM', 'I-SYM', 'I-SYM', 'E-SYM', 'O', 'O', 'O'],\n"," ['B-SYM', 'I-SYM', 'E-SYM', 'O', 'O', 'O', 'O'],\n"," ['O',\n","  'O',\n","  'O',\n","  'B-SYM',\n","  'E-SYM',\n","  'B-SYM',\n","  'E-SYM',\n","  'O',\n","  'B-SYM',\n","  'E-SYM',\n","  'E-SYM',\n","  'E-SYM',\n","  'E-SYM',\n","  'O',\n","  'O',\n","  'O',\n","  'O']]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["data = load_data_setting(\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/save-222.dset\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fOS4JpTmV8RY","executionInfo":{"status":"ok","timestamp":1645931514815,"user_tz":-480,"elapsed":14880,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"9c4220e6-8765-4768-dcf6-bf322947dd61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data setting loaded from file:  /content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/save-222.dset\n","DATA SUMMARY START:\n","     Tag          scheme: BIO\n","     MAX SENTENCE LENGTH: 250\n","     MAX   WORD   LENGTH: -1\n","     Number   normalized: True\n","     Use          bigram: False\n","     Word  alphabet size: 3153\n","     Biword alphabet size: 54022\n","     Char  alphabet size: 3153\n","     Gaz   alphabet size: 12449\n","     Label alphabet size: 15\n","     Word embedding size: 50\n","     Biword embedding size: 50\n","     Char embedding size: 30\n","     Gaz embedding size: 50\n","     Norm     word   emb: True\n","     Norm     biword emb: True\n","     Norm     gaz    emb: False\n","     Norm   gaz  dropout: 0.5\n","     Train instance number: 72016\n","     Dev   instance number: 24006\n","     Test  instance number: 24006\n","     Raw   instance number: 0\n","     Hyperpara  iteration: 100\n","     Hyperpara  batch size: 10\n","     Hyperpara          lr: 0.015\n","     Hyperpara    lr_decay: 0.05\n","     Hyperpara     HP_clip: 5.0\n","     Hyperpara    momentum: 0\n","     Hyperpara  hidden_dim: 200\n","     Hyperpara     dropout: 0.5\n","     Hyperpara  lstm_layer: 1\n","     Hyperpara      bilstm: True\n","     Hyperpara         GPU: True\n","     Hyperpara     use_gaz: True\n","     Hyperpara fix gaz emb: False\n","     Hyperpara    use_char: False\n","DATA SUMMARY END.\n"]}]},{"cell_type":"code","source":["model111 = torch.load(\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/2-222-saved_model2.lstmcrf\")\n","model_test.load_state_dict(model111)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tz7xkiiFTnNF","executionInfo":{"status":"ok","timestamp":1645540475017,"user_tz":-480,"elapsed":480,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"5657193d-92e8-4fd0-806c-25f8c52ac6e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["type(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R062BOQM4XOA","executionInfo":{"status":"ok","timestamp":1645591235165,"user_tz":-480,"elapsed":410,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"af474691-bd3a-4101-e531-07237c08f448"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["model.bilstmcrf.BiLSTM_CRF"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["torch.save(model,\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/223_test.model\")"],"metadata":{"id":"X1e73Isq4pxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_test=(torch.load(\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/223_test.model\"))"],"metadata":{"id":"Tc2UzxHE4w2I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_test==model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JO09lqtJ4-MT","executionInfo":{"status":"ok","timestamp":1645591408704,"user_tz":-480,"elapsed":363,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"83ff3ba2-8a01-4f89-94e7-e8573b7c1968"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["type(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mv4NYLW25DrL","executionInfo":{"status":"ok","timestamp":1645591420720,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"418fd4b5-53cd-49b5-b381-f34040432224"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["model.bilstmcrf.BiLSTM_CRF"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["type(model_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZH8QN57O5Gaz","executionInfo":{"status":"ok","timestamp":1645591429201,"user_tz":-480,"elapsed":5,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"7c97294a-f5d6-4420-9336-f421d7882aab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["model.bilstmcrf.BiLSTM_CRF"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["pred_results = load_model_decode_with_model(model, data, 'test', gpu, seg)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HazXXWUS12Gf","executionInfo":{"status":"ok","timestamp":1645590882622,"user_tz":-480,"elapsed":288853,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"2eeb14ce-fb6f-4739-e895-f37262d7e559"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Decode test data ...\n"]},{"output_type":"stream","name":"stderr","text":["__main__:184: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:185: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:186: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:187: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:207: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:263: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  word_var = autograd.Variable(torch.LongTensor(skip_input_[t][0]),volatile =  volatile_flag)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:156: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n","  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"]},{"output_type":"stream","name":"stdout","text":["gold_num =  27377  pred_num =  27388  right_num =  26986\n","test: time:288.80s, speed:83.24st/s; acc: 0.9949, p: 0.9853, r: 0.9857, f: 0.9855\n"]}]},{"cell_type":"code","source":["f=open('pred_result.txt','w',encoding='utf-8')\n","for i in pred_results:\n","    for word in i:\n","        f.write(word+'\\n')\n","    f.write('\\n')\n","f.close()"],"metadata":{"id":"q0Ko_uP93fma"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_results2 = load_model_decode_with_model(model_test, data, 'test', gpu, seg)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x1PzhQYQ6Qs7","executionInfo":{"status":"ok","timestamp":1645592031774,"user_tz":-480,"elapsed":285859,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"acd4f8ac-97bd-4263-afca-0176d8e186a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Decode test data ...\n"]},{"output_type":"stream","name":"stderr","text":["__main__:184: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:185: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:186: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:187: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:207: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:263: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  word_var = autograd.Variable(torch.LongTensor(skip_input_[t][0]),volatile =  volatile_flag)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:156: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n","  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"]},{"output_type":"stream","name":"stdout","text":["gold_num =  27377  pred_num =  27388  right_num =  26986\n","test: time:285.32s, speed:84.25st/s; acc: 0.9949, p: 0.9853, r: 0.9857, f: 0.9855\n"]}]},{"cell_type":"code","source":["pred_results2==pred_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tD7ICH2r7_pg","executionInfo":{"status":"ok","timestamp":1645592196325,"user_tz":-480,"elapsed":549,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"ff22e5f0-b46a-495b-c128-fbae5bc0eb8d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["f=open('pred_result2.txt','w',encoding='utf-8')\n","for i in pred_results2:\n","    for word in i:\n","        f.write(word+'\\n')\n","    f.write('\\n')\n","f.close()"],"metadata":{"id":"FA3vo1aaOEkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ebnzL7f08EwN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = load_data_setting(\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/save-222.dset\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q1pJLghjKiZj","executionInfo":{"status":"ok","timestamp":1645943640227,"user_tz":-480,"elapsed":14958,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"471a36d4-49fe-4e8f-b6af-8e5b146c4532"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data setting loaded from file:  /content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/save-222.dset\n","DATA SUMMARY START:\n","     Tag          scheme: BIO\n","     MAX SENTENCE LENGTH: 250\n","     MAX   WORD   LENGTH: -1\n","     Number   normalized: True\n","     Use          bigram: False\n","     Word  alphabet size: 3153\n","     Biword alphabet size: 54022\n","     Char  alphabet size: 3153\n","     Gaz   alphabet size: 12449\n","     Label alphabet size: 15\n","     Word embedding size: 50\n","     Biword embedding size: 50\n","     Char embedding size: 30\n","     Gaz embedding size: 50\n","     Norm     word   emb: True\n","     Norm     biword emb: True\n","     Norm     gaz    emb: False\n","     Norm   gaz  dropout: 0.5\n","     Train instance number: 72016\n","     Dev   instance number: 24006\n","     Test  instance number: 24006\n","     Raw   instance number: 0\n","     Hyperpara  iteration: 100\n","     Hyperpara  batch size: 10\n","     Hyperpara          lr: 0.015\n","     Hyperpara    lr_decay: 0.05\n","     Hyperpara     HP_clip: 5.0\n","     Hyperpara    momentum: 0\n","     Hyperpara  hidden_dim: 200\n","     Hyperpara     dropout: 0.5\n","     Hyperpara  lstm_layer: 1\n","     Hyperpara      bilstm: True\n","     Hyperpara         GPU: True\n","     Hyperpara     use_gaz: True\n","     Hyperpara fix gaz emb: False\n","     Hyperpara    use_char: False\n","DATA SUMMARY END.\n"]}]},{"cell_type":"code","source":["model=(torch.load(\"/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/223_test.model\"))"],"metadata":{"id":"EJedNH4aLCp5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpu = torch.cuda.is_available()"],"metadata":{"id":"gI71UXSmMMrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser(description='Tuning with bi-directional LSTM-CRF')\n","parser.add_argument('--seg', default=\"True\") \n","args = parser.parse_args()\n","if args.seg.lower() == \"true\":\n","  seg = True \n","else:\n","  seg = False"],"metadata":{"id":"cnTXLbSKMWzs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(data, model, name):\n","    if name == \"train\":\n","        instances = data.train_Ids\n","    elif name == \"dev\":\n","        instances = data.dev_Ids\n","    elif name == 'test':\n","        instances = data.test_Ids\n","    elif name == 'raw':\n","        instances = data.raw_Ids\n","    else:\n","        print (\"Error: wrong evaluate name,\", name)\n","    pred_results = []\n","    gold_results = []\n","    ## set model in eval model\n","    model.eval()\n","    batch_size = 10\n","    start_time = time.time()\n","    train_num = len(instances)\n","    total_batch = train_num//batch_size+1\n","    for batch_id in range(total_batch):\n","        start = batch_id*batch_size\n","        end = (batch_id+1)*batch_size \n","        if end >train_num:\n","            end =  train_num\n","        instance = instances[start:end]\n","        if not instance:\n","            continue\n","        gaz_list,batch_word, batch_biword, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu, True)\n","        tag_seq = model(gaz_list,batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask)\n","        # print \"tag:\",tag_seq\n","        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet, batch_wordrecover)\n","        pred_results += pred_label\n","        gold_results += gold_label\n","    #decode_time = time.time() - start_time\n","    #speed = len(instances)/decode_time\n","    #acc, p, r, f = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)\n","    return pred_results"],"metadata":{"id":"Pg5GCD62M-35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_model_decode_with_model_predict(model, data, name, gpu, seg=True):\n","    data.HP_gpu = gpu\n","    \n","    print(\"Decode %s data ...\"%(name))\n","    start_time = time.time()\n","    pred_results = predict(data, model, name)\n","    end_time = time.time()\n","    time_cost = end_time - start_time\n","    return pred_results"],"metadata":{"id":"6Fj4wxQTNLL0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.generate_instance_with_gaz('/content/drive/MyDrive/TT.txt','test')\n","a=load_model_decode_with_model_predict(model, data, 'test', gpu, seg)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hvjpUzCXMBWN","executionInfo":{"status":"ok","timestamp":1645944659442,"user_tz":-480,"elapsed":924,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"767adb62-ba6a-4c46-feac-21c07b634356"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Decode test data ...\n"]},{"output_type":"stream","name":"stderr","text":["__main__:184: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:185: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:186: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:187: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:207: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:263: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  word_var = autograd.Variable(torch.LongTensor(skip_input_[t][0]),volatile =  volatile_flag)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:156: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n","  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"]}]},{"cell_type":"code","source":["a"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bDK4KD_H6YVo","executionInfo":{"status":"ok","timestamp":1645944661481,"user_tz":-480,"elapsed":366,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"a53bc329-8745-4ce6-ceba-85ff29f0920a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['O',\n","  'B-sym',\n","  'E-sym',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'B-sym',\n","  'I-sym',\n","  'E-sym'],\n"," ['B-sym', 'I-sym', 'I-sym', 'E-sym', 'O', 'O', 'O'],\n"," ['B-dru', 'I-dru', 'E-dru', 'O', 'O', 'O', 'O']]"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["data.test_Ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8vkdOk-PNT5V","executionInfo":{"status":"ok","timestamp":1645932330557,"user_tz":-480,"elapsed":438,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"f1fb93ff-3097-40fc-d1e7-78aee12400e4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["data.generate_instance_with_gaz('/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/ResumeNER/my_dev_set.txt','dev')\n","load_model_decode_with_model(model, data , 'dev', gpu, seg)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ae24_YlONVB2","executionInfo":{"status":"ok","timestamp":1645932880934,"user_tz":-480,"elapsed":288764,"user":{"displayName":"Yuxin Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzaqERT_6RUUmK7h_xnZaNIIg55B_IXGRYWSTU=s64","userId":"03834150235925799896"}},"outputId":"58fed25e-b4eb-441d-d4f5-b1c0aeeb5442"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Decode dev data ...\n"]},{"output_type":"stream","name":"stderr","text":["__main__:184: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:185: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:186: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:187: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","__main__:207: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/latticelstm.py:263: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  word_var = autograd.Variable(torch.LongTensor(skip_input_[t][0]),volatile =  volatile_flag)\n","/content/drive/MyDrive/lattcie ner/fyz-lattcie/fyz_lattice_NER/model/crf.py:156: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n","  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"]},{"output_type":"stream","name":"stdout","text":["gold_num =  27528  pred_num =  27538  right_num =  27159\n","dev: time:281.01s, speed:85.55st/s; acc: 0.9954, p: 0.9862, r: 0.9866, f: 0.9864\n","Decode test data ...\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"wRCQcknTN-vq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NGuAghc2e4tb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NwG5OKkle8GK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Ff2sCyBEe70n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ECt6JLig4kQf"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"Lattice LSTM.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1zPVN41wYEswJfzG8Todpl5T1yjFJGpCr","authorship_tag":"ABX9TyNtx9jb0Ux3XfHNkASV84Cq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}